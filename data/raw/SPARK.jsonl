{"key":"SPARK-52598","summary":"Reorganize docs for Spark Connect","description":"","status":"Open","priority":"Minor","reporter":"Nicholas Chammas","labels":["pull-request-available"],"project":"SPARK","created":"2025-06-27T15:55:07.000+0000","updated":"2025-10-25T00:26:56.000+0000","comments":[]}
{"key":"SPARK-53335","summary":"Support `spark.kubernetes.driver.annotateExitException`","description":"For jobs which run on kubernetes there is no native concept of diagnostics (like there is in YARN), which means that for debugging and triaging errors users _must_ go to logs. For many jobs which run on YARN this is often not necessary, since the diagnostics contains the root cause reason for failure. Additionally, for platforms which provide automation of failure insights, or make decisions based on failures, there must be a custom solution or deciding why the application failed (e.g. log and stack trace parsing). We should provide a way for yarn-equivalent diagnostics for spark jobs on kubernetes.","status":"Resolved","priority":"Major","reporter":"Victor Sunderland","assignee":"Victor Sunderland","labels":["pull-request-available"],"project":"SPARK","created":"2025-08-20T03:49:56.000+0000","updated":"2025-10-25T04:52:12.000+0000","comments":[{"author":"Victor Sunderland","body":"https://github.com/apache/spark/pull/52068"},{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52068 [https://github.com/apache/spark/pull/52068]"},{"author":"Dongjoon Hyun","body":"To improve the visibility of this feature, I collected this as a subtask of SPARK-54016."}]}
{"key":"SPARK-53880","summary":"Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants","description":"This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.","status":"Resolved","priority":"Major","reporter":"L. C. Hsieh","assignee":"L. C. Hsieh","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-12T18:22:52.000+0000","updated":"2025-10-25T04:54:32.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52578 [https://github.com/apache/spark/pull/52578]"},{"author":"L. C. Hsieh","body":"[~dongjoon] Thank you for updating this ticket."},{"author":"Dongjoon Hyun","body":"Thank YOU for the fix. :)"}]}
{"key":"SPARK-52509","summary":"Fallback storage accumulates removed shuffle data","description":"The fallback storage removes migrated shuffle data on Spark context shutdown. Ideally, it should remove individual shuffles once they get removed from the Spark context. Otherwise, all shuffle data ever migrated to the fallback storage accumulate on the remote storage until the Spark context shuts down. For long running jobs with a lot of decommissioning, this can be orders of magnitude more data than what is actively been used / usable / referenced.","status":"Resolved","priority":"Major","reporter":"Enrico Minack","assignee":"Enrico Minack","labels":["pull-request-available"],"project":"SPARK","created":"2025-06-17T08:55:17.000+0000","updated":"2025-10-25T05:08:43.000+0000","comments":[{"author":"Subham Singhal","body":"[~enricomi] Do we have to create a custom Spark event(UnregisterShuffle) and add a listener which will remove shuffleId from fallback storage? I am willing to contribte but need guidance."},{"author":"Enrico Minack","body":"This is fixed in [https://github.com/apache/spark/pull/51199.] Looks like the PR did not get linked to the issue. Spark already exists the infrastructure to notify the fallback storage to remove shuffles, the message {{RemoveShuffle}} was just not handled."},{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 51199 [https://github.com/apache/spark/pull/51199]"}]}
{"key":"SPARK-53942","summary":"Support changing stateless shuffle partitions upon restart of streaming query","description":"We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this. The main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.","status":"Resolved","priority":"Major","reporter":"Jungtaek Lim","assignee":"Jungtaek Lim","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-17T07:00:53.000+0000","updated":"2025-10-25T08:57:53.000+0000","comments":[{"author":"Jungtaek Lim","body":"Issue resolved by pull request 52645 [https://github.com/apache/spark/pull/52645]"}]}
{"key":"SPARK-28098","summary":"Native ORC reader doesn't support subdirectories with Hive tables","description":"The Hive ORC reader supports recursive directory reads from S3. Spark's native ORC reader supports recursive directory reads, but not when used with Hive. {code:java} val testData = List(1,2,3,4,5) val dataFrame = testData.toDF() dataFrame .coalesce(1) .write .mode(SaveMode.Overwrite) .format(\"orc\") .option(\"compression\", \"zlib\") .save(\"s3://ddrinka.sparkbug/dirTest/dir1/dir2/\") spark.sql(\"DROP TABLE IF EXISTS ddrinka_sparkbug.dirTest\") spark.sql(\"CREATE EXTERNAL TABLE ddrinka_sparkbug.dirTest (val INT) STORED AS ORC LOCATION 's3://ddrinka.sparkbug/dirTest/'\") spark.conf.set(\"hive.mapred.supports.subdirectories\",\"true\") spark.conf.set(\"mapred.input.dir.recursive\",\"true\") spark.conf.set(\"mapreduce.input.fileinputformat.input.dir.recursive\",\"true\") spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"true\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //0 spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //5{code}","status":"In Progress","priority":"Major","reporter":"Douglas Drinka","labels":["pull-request-available"],"project":"SPARK","created":"2019-06-18T20:13:13.000+0000","updated":"2025-10-25T09:27:53.000+0000","comments":[{"author":"Hyukjin Kwon","body":"[~ddrinka], do you mind if I ask to check similar stuff as said in https://issues.apache.org/jira/browse/SPARK-28099?focusedCommentId=16868249&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16868249?"},{"author":"lithiumlee-_-","body":"Also not support subdirectories which generated by tez union all."},{"author":"Apache Spark","body":"User 'FatalLin' has created a pull request for this issue: https://github.com/apache/spark/pull/32202"},{"author":"Apache Spark","body":"User 'FatalLin' has created a pull request for this issue: https://github.com/apache/spark/pull/32202"},{"author":"Yu-Tang Lin","body":"Hi [~ddrinka], I already made a PR to resolve the issue, maybe we could take a look on it?"},{"author":"Apache Spark","body":"User 'chong0929' has created a pull request for this issue: https://github.com/apache/spark/pull/32679"},{"author":"Zhen Wang","body":"> set spark.sql.hive.convertMetastoreParquet=false With this parameter, it works fine, but I don't think it's a good way."}]}
{"key":"SPARK-54015","summary":"Relex Py4J requirement to 0.10.9.7+","description":"JVM are compatible with 0.10.9.7+ and above versions have some correctness fixes","status":"Resolved","priority":"Major","reporter":"Hyukjin Kwon","assignee":"Hyukjin Kwon","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-24T12:54:21.000+0000","updated":"2025-10-25T23:56:27.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52721 [https://github.com/apache/spark/pull/52721]"}]}
{"key":"SPARK-53002","summary":"RocksDB Bounded Memory Fix","description":"Currently, RocksDB metrics show 0 bytes used if bounded memory is enabled. This PR will provide an approximation of memory used by fetching the memory used by RocksDB per executor, then dividing it by the open RocksDB instances per executor.","status":"Closed","priority":"Major","reporter":"Eric Marnadi","assignee":"Eric Marnadi","labels":["pull-request-available"],"project":"SPARK","created":"2025-07-29T18:36:40.000+0000","updated":"2025-10-26T01:02:43.000+0000","comments":[{"author":"Anish Shrigondekar","body":"PR merged here - https://github.com/apache/spark/pull/51709"}]}
{"key":"SPARK-53406","summary":"Support Shuffle Spec in Direct Partition ID Pass Through","description":"","status":"Resolved","priority":"Major","reporter":"Shujing Yang","assignee":"Shujing Yang","labels":["pull-request-available"],"project":"SPARK","created":"2025-08-28T00:02:15.000+0000","updated":"2025-10-26T14:19:24.000+0000","comments":[{"author":"Wenchen Fan","body":"Issue resolved by pull request 52443 [https://github.com/apache/spark/pull/52443]"}]}
{"key":"SPARK-54012","summary":"Improve Netty usage patterns","description":"","status":"Resolved","priority":"Critical","reporter":"Dongjoon Hyun","assignee":"Kent Yao","labels":["releasenotes"],"project":"SPARK","created":"2025-10-24T05:16:15.000+0000","updated":"2025-10-26T20:10:40.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Hi, [~yao]. As a recognition of your contribution, I made this umbrella JIRA issue with `releasenotes` label. Although I have more ideas for this topic and will add some, I believe we are able to mark this as a *resolved* item for Apache Spark 4.1.0 by containing only what we did and what we want to add until the feature freeze. WDYT?"},{"author":"Kent Yao","body":"Thank you for making this umbrella, [~dongjoon]. Making it resolved sounds good to me"},{"author":"Dongjoon Hyun","body":"Thank you, [~yao]."}]}
{"key":"SPARK-54018","summary":"Upgrade Volcano to 1.13.0","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-24T16:06:39.000+0000","updated":"2025-10-26T23:08:19.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52722 [https://github.com/apache/spark/pull/52722]"}]}
{"key":"SPARK-52790","summary":"Introduce new grid testing method which provides better naming","description":"Currently, gridTest accepts test name prefix and sequence of parameters. Final test name is made like this `testNamePrefix + s\" ($param)\"`. Which is not good since developers often don't know how final test name would look like and pass here sequence of map, or sequence of booleans, which results in unintuitive test case names. E.g. {code:java} gridTest(\"Select with limit\")(Seq(true, false)) { pushdownEnabled => ... } {code} Will result in registering of next test cases: * Select with limit (true) * Select with limit (false) Instead of that, developers should provide descriptive name suffix: {code:java} gridTest(\"Select with limit\")(Seq( GridTestCase(params = true, suffix = \"pushdown enabled\"), GridTestCase(params = false, suffix = \"pushdown disabled\"), )) { pushdownEnabled => ... } {code} Instead of relying on developers to look for base implementation and make some case class for parameters with overriden `toString` implementation, we should enforce engineers to provide suffix. (Even with proper `toString` implementation, intent of test case may be unknown).","status":"Open","priority":"Major","reporter":"Uros Stankovic","labels":["pull-request-available"],"project":"SPARK","created":"2025-07-14T12:20:51.000+0000","updated":"2025-10-27T00:30:29.000+0000","comments":[]}
{"key":"SPARK-54035","summary":"Construct FileStatus from the executor side directly","description":"https://github.com/apache/spark/pull/50765#discussion_r2357607758","status":"Open","priority":"Major","reporter":"Cheng Pan","labels":[],"project":"SPARK","created":"2025-10-27T02:33:18.000+0000","updated":"2025-10-27T02:33:18.000+0000","comments":[]}
{"key":"SPARK-52011","summary":"Reduce HDFS NameNode RPC on vectorized Parquet reader","description":"","status":"Open","priority":"Major","reporter":"Cheng Pan","labels":["pull-request-available"],"project":"SPARK","created":"2025-05-06T02:20:11.000+0000","updated":"2025-10-27T02:38:08.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Hi, [~chengpan]. Do you have more subtasks for this umbrella JIRA issue?"},{"author":"Cheng Pan","body":"[~dongjoon] thanks for the reminder, I created another subtask."},{"author":"Dongjoon Hyun","body":"Thank you!"}]}
{"key":"SPARK-54032","summary":"Prefer to use native Netty transports by default","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-26T20:07:43.000+0000","updated":"2025-10-27T02:42:36.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52736 [https://github.com/apache/spark/pull/52736]"}]}
{"key":"SPARK-54023","summary":"Support `AUTO` Netty IO Mode","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-24T18:42:53.000+0000","updated":"2025-10-27T02:43:42.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52724 [https://github.com/apache/spark/pull/52724]"}]}
{"key":"SPARK-53980","summary":"Add `SparkConf.getAllWithPrefix(String, String => K)` API","description":"We need to set some config related to S3 for our inner Spark. The implementation of the function show below. {code:java} private def setS3Configs(conf: SparkConf): Unit = { val S3A_PREFIX = \"spark.fs.s3a\" val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\" val s3aConf = conf.getAllWithPrefix(S3A_PREFIX) s3aConf .foreach( confPair => { val keyWithoutPrefix = confPair._1 val oldKey = S3A_PREFIX + keyWithoutPrefix val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix val value = confPair._2 (newKey, value) }) } {code} These code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.","status":"Resolved","priority":"Major","reporter":"Jiaan Geng","assignee":"Jiaan Geng","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-22T07:36:01.000+0000","updated":"2025-10-27T02:48:35.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52693 [https://github.com/apache/spark/pull/52693]"}]}
{"key":"SPARK-54036","summary":"Add `build_python_3.11_macos26.yml` GitHub Action job","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-27T03:00:13.000+0000","updated":"2025-10-27T03:19:34.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52740 [https://github.com/apache/spark/pull/52740]"}]}
{"key":"SPARK-53659","summary":"Infer Variant shredding schema in parquet writer","description":"","status":"Resolved","priority":"Major","reporter":"David Cashman","assignee":"David Cashman","labels":["pull-request-available"],"project":"SPARK","created":"2025-09-22T13:39:59.000+0000","updated":"2025-10-27T03:52:57.000+0000","comments":[{"author":"Wenchen Fan","body":"Issue resolved by pull request 52406 [https://github.com/apache/spark/pull/52406]"}]}
{"key":"SPARK-53962","summary":"Upgrade ASM to 9.9","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-20T21:29:00.000+0000","updated":"2025-10-27T04:18:49.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52672 [https://github.com/apache/spark/pull/52672]"}]}
{"key":"SPARK-54037","summary":"Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0","description":"My team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database. I have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only. In case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5. I have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines \"Running task x in stage y\" and \"Finished task x in stage y\". Is this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ? (I'll also mention that we are using checkpointing (in case it might be important here))","status":"Open","priority":"Major","reporter":"Adrian Pusty","labels":[],"project":"SPARK","created":"2025-10-27T08:21:21.000+0000","updated":"2025-10-27T08:21:59.000+0000","comments":[]}
{"key":"SPARK-54034","summary":"Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly","description":"","status":"Resolved","priority":"Critical","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-26T22:27:03.000+0000","updated":"2025-10-27T15:29:41.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52738 [https://github.com/apache/spark/pull/52738]"}]}
{"key":"SPARK-54040","summary":"Remove unused commons-collections 3.x","description":"","status":"Resolved","priority":"Major","reporter":"Cheng Pan","assignee":"Cheng Pan","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-27T14:47:43.000+0000","updated":"2025-10-27T18:21:49.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 52743 [https://github.com/apache/spark/pull/52743]"}]}
{"key":"SPARK-54006","summary":"WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above","description":"Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with {code} WithAggregationKinesisBackedBlockRDDSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56) at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893) at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45) at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60) at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23) at scala.collection.immutable.Range.foreach(Range.scala:158) at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57) at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134) ... Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56) at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893) at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45) at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60) at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23) ... {code}","status":"Open","priority":"Minor","reporter":"Vlad Rozov","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-23T18:45:57.000+0000","updated":"2025-10-27T21:06:38.000+0000","comments":[]}
{"key":"SPARK-54043","summary":"Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-27T20:05:42.000+0000","updated":"2025-10-27T21:16:43.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 252 [https://github.com/apache/spark-connect-swift/pull/252]"}]}
{"key":"SPARK-54042","summary":"Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-27T16:11:43.000+0000","updated":"2025-10-27T21:46:39.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 251 [https://github.com/apache/spark-connect-swift/pull/251]"}]}
{"key":"SPARK-54044","summary":"Upgrade `gRPC Swift NIO Transport` to 2.2.0","description":"","status":"Resolved","priority":"Major","reporter":"Dongjoon Hyun","assignee":"Dongjoon Hyun","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-27T21:57:06.000+0000","updated":"2025-10-27T22:20:09.000+0000","comments":[{"author":"Dongjoon Hyun","body":"Issue resolved by pull request 253 [https://github.com/apache/spark-connect-swift/pull/253]"}]}
{"key":"SPARK-51127","summary":"Kill the Python worker on idle timeout.","description":"","status":"Resolved","priority":"Major","reporter":"Takuya Ueshin","assignee":"Takuya Ueshin","labels":["pull-request-available"],"project":"SPARK","created":"2025-02-07T02:10:21.000+0000","updated":"2025-10-27T23:20:55.000+0000","comments":[{"author":"Takuya Ueshin","body":"Issue resolved by pull request 49843 [https://github.com/apache/spark/pull/49843]"},{"author":"Dongjoon Hyun","body":"This is reverted from branch-4.0 via [https://github.com/apache/spark/commit/a94faa738fcb602df6d641a79bf2a168224a52f5]"}]}
{"key":"SPARK-54024","summary":"add sbt-dependency-graph to SBT plugins","description":"The plugin adds few useful commands to browser dependencies tree in SBT.","status":"Open","priority":"Minor","reporter":"Vlad Rozov","labels":["pull-request-available"],"project":"SPARK","created":"2025-10-24T19:52:00.000+0000","updated":"2025-10-27T23:23:55.000+0000","comments":[]}
{"key":"SPARK-53732","summary":"Add TimeTravelSpec to DataSourceV2Relation","description":"","status":"Resolved","priority":"Major","reporter":"Anton Okolnychyi","assignee":"Anton Okolnychyi","labels":["pull-request-available"],"project":"SPARK","created":"2025-09-26T08:08:11.000+0000","updated":"2025-10-27T23:41:38.000+0000","comments":[{"author":"Wenchen Fan","body":"Issue resolved by pull request 52599 [https://github.com/apache/spark/pull/52599]"}]}
