{"key":"HADOOP-18637","summary":"S3A to support upload of files greater than 2 GB using DiskBlocks","description":"Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. cc: [~mthakur] [~stevel@apache.org] [~mehakmeet]","status":"Resolved","priority":"Major","reporter":"Harshit Gupta","assignee":"Harshit Gupta","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-21T09:05:42.000+0000","updated":"2025-10-25T00:22:13.000+0000","comments":[{"author":"Harshit Gupta","body":"S3ABlockOutputStream creates three different types of DataBlock depending upon the {{fs.s3a.fast.upload.buffer}} which defaults to disk, we can create an empty file for the same size and limit the Buffer size to {{Integer.MAX_VALUE}} . *For other buffer types should we deny uploads larger than 2 Gigs or should we add the support there as well?* like for {{ByteArrayBlock}} which writes directly to the {{S3AByteArrayOutputStream}} which will be again initialized with {{Integer.MAX_Value}} .The same goes for {{ByteBufferBlock}} as well. One thing to make sure of here is that it's never gonna write something larger than {{Integer.MAX_VALUE}} as the calling function to write has the signature {{public synchronized void write(byte[] source, int offset, int len)}} (S3ABlockOutputStream). *This is just for compatibility with non-AWS s3 stores.*"},{"author":"Mukund Thakur","body":"As discussed offline, following changes will be required. * Introduce a new config to disable multipart upload everywhere and enable just a large file upload. * Error in public S3AFS.createMultipartUploader based on above config. * Error in staging committer based on above config. * Error in magic committer based on above config. * Error in write operations helper based on above config. * Add hasCapability(isMultiPartAllowed, path) use config. * If multipart upload is disabled we only upload via Disk. Add check for this."},{"author":"ASF GitHub Bot","body":"HarshitGupta11 opened a new pull request, #5481: URL: https://github.com/apache/hadoop/pull/5481 ### Description of PR Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. ### How was this patch tested? The patch was tested against us-west-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1469829840 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 33s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 96m 36s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5e32eadfab76 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2d25f629c809071990645855774528e52103cfe | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/2/testReport/ | | Max. process+thread count | 727 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1469831351 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 37s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 47s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 27s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 32s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 98m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 035b98662ea9 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a2d25f629c809071990645855774528e52103cfe | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/1/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1146257205 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String ALLOW_MULTIPART_UPLOADS = \"fs.s3a.allow.multipart.uploads\"; + + public static final boolean IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT = true; Review Comment: change to MULTIPART_UPLOAD_ENABLED_DEFAULT; ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -564,11 +564,12 @@ class ByteBufferBlock extends DataBlock { * @param statistics statistics to update */ ByteBufferBlock(long index, - int bufferSize, + long bufferSize, BlockOutputStreamStatistics statistics) { super(index, statistics); - this.bufferSize = bufferSize; - blockBuffer = requestBuffer(bufferSize); + this.bufferSize = bufferSize > Integer.MAX_VALUE ? + Integer.MAX_VALUE : (int) bufferSize; + blockBuffer = requestBuffer((int) bufferSize); Review Comment: use this.bufferSize rather than casting again. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -169,6 +169,9 @@ class S3ABlockOutputStream extends OutputStream implements /** Thread level IOStatistics Aggregator. */ private final IOStatisticsAggregator threadIOStatisticsAggregator; + /**Is multipart upload allowed? */ + private final boolean isMultipartAllowed; Review Comment: isMultipartEnabled ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: throw Exception. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String ALLOW_MULTIPART_UPLOADS = \"fs.s3a.allow.multipart.uploads\"; Review Comment: Change to MULTIPART_UPLOADS_ENABLED = \"fs.s3a.multipart.uploads.enabled\"; ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -516,6 +516,7 @@ public void initialize(URI name, Configuration originalConf) maxKeys = intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1); partSize = getMultipartSizeProperty(conf, MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE); + LOG.warn(\"Patcchhhh: The part size is : {}\", partSize); Review Comment: delete ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: just add a method validateOutputStreamConfiguration() and throw exception in the implementation only. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ + boolean isAllowedMultipart = conf.getBoolean(ALLOW_MULTIPART_UPLOADS, + IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT); + if (isAllowedMultipart) { Review Comment: this is wrong here I guess. if isAllowedMultipart is enabled then FAST_UPLOAD_BUFFER must be disk else we throw an error right? ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java: ########## @@ -269,8 +269,8 @@ public PutObjectRequest createPutObjectRequest( String dest, File sourceFile, final PutObjectOptions options) { - Preconditions.checkState(sourceFile.length() Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int) limit; + buffer = new S3AByteArrayOutputStream((int) limit); Review Comment: use this.limit. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -595,7 +596,7 @@ public void initialize(URI name, Configuration originalConf) } blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER); - partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize); + //partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize); Review Comment: cut"},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1481310374 @steveloughran could you review this please. thanks."},{"author":"ASF GitHub Bot","body":"HarshitGupta11 commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1149003025 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: Why do we need to throw exception here, since this just meant to skip the initialisation? cc: @mehakmeet @steveloughran"},{"author":"ASF GitHub Bot","body":"HarshitGupta11 commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1149011685 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ + boolean isAllowedMultipart = conf.getBoolean(ALLOW_MULTIPART_UPLOADS, + IS_ALLOWED_MULTIPART_UPLOADS_DEFAULT); + if (isAllowedMultipart) { Review Comment: If multipart is disabled and the FAST_UPLOAD_BUFFER is not disk then we throw an error."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1485290836 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 1s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 40s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 21s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 31s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 98m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux cdba540f5bf0 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f381b88a9260facd94616e0cf62c426b44575f45 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/3/testReport/ | | Max. process+thread count | 721 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1150546662 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -558,19 +565,21 @@ public String toString() { } /** - * Upload the current block as a single PUT request; if the buffer - * is empty a 0-byte PUT will be invoked, as it is needed to create an - * entry at the far end. - * @throws IOException any problem. - * @return number of bytes uploaded. If thread was interrupted while - * waiting for upload to complete, returns zero with interrupted flag set - * on this thread. + * Upload the current block as a single PUT request; if the buffer is empty a + * 0-byte PUT will be invoked, as it is needed to create an entry at the far Review Comment: no need to reformat the entire javadoc, you don't want your IDE set to do this as it only makes cherrypicking harder ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -169,6 +169,9 @@ class S3ABlockOutputStream extends OutputStream implements /** Thread level IOStatistics Aggregator. */ private final IOStatisticsAggregator threadIOStatisticsAggregator; + /**Is multipart upload allowed? */ Review Comment: nit, add a space after the ** ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1126,6 +1135,11 @@ public static final class BlockOutputStreamBuilder { */ private IOStatisticsAggregator ioStatisticsAggregator; + /** + * Is Multipart Uploads enabled for the given upload Review Comment: add a . to keep javadoc happy ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1126,6 +1135,11 @@ public static final class BlockOutputStreamBuilder { */ private IOStatisticsAggregator ioStatisticsAggregator; + /** + * Is Multipart Uploads enabled for the given upload + */ + private boolean isMultipartAllowed; Review Comment: rename isMultipartEnabled ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -1276,5 +1290,11 @@ public BlockOutputStreamBuilder withIOStatisticsAggregator( ioStatisticsAggregator = value; return this; } + + public BlockOutputStreamBuilder withMultipartAllowed( Review Comment: again, rename ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; Review Comment: and add java.* block above the others. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); + configuration.setInt(KEY_TEST_TIMEOUT, 36000); + configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); + configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); Review Comment: use the relevant constant ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,19 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + public static boolean checkDiskBuffer(Configuration conf){ Review Comment: 1. javadocs 2. add a space after the ) and { ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); + configuration.setInt(KEY_TEST_TIMEOUT, 36000); + configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); + configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); + return configuration; + } + + @Test + public void uploadFileSinglePut() throws IOException { + LOG.info(\"Creating file with size : {}\", fileSize); + ContractTestUtils.createAndVerifyFile(getFileSystem(), Review Comment: after the upload, use the iostatistics of the fs to verify that only one PUT operation took place, and therefore that the operation worked ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; Review Comment: is this going to be configurable? as i might want to make the size smaller so my tests don't time out as much (in fact: I absolutely WILL because I run with -Dscale most of the time) look at AbstractSTestS3AHugeFiles.setup() to see how it picks up the size. use the same configuration option to control the size in this test case. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1830,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ + throw new IOException(\"The filesystem conf is not \" + Review Comment: that's not a particularly useful error message. better to say which options are inconsistent ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1255,4 +1255,8 @@ private Constants() { */ public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + + public static final String MULTIPART_UPLOADS_ENABLED = \"fs.s3a.multipart.uploads.enabled\"; Review Comment: add javadocs for these constants with `{@value}` in them, so IDEs will show what they mean and the generated docs show the strings also add a mention in the \"How S3A writes data to S3\" in hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; Review Comment: split import blocks, into apache and non apache, put the non-apache one first ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUpload.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override Review Comment: nit: newline between these two ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUpload.java: ########## @@ -0,0 +1,55 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; +import org.junit.Test; +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.IOException; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; + +public class ITestS3AHugeFileUpload extends S3AScaleTestBase{ Review Comment: * add a javadoc to see what it is doing. * give it a name to indicate it is doing a single put, e.g `ITestS3AHugeFileUploadSinglePut` * add a space before { ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,9 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + if (!isMultipartAllowed){ + return; Review Comment: good q. we should throw it because if something does get as far as calling it, something has gone very wrong. I would just use ``` Preconditions.checkState(!isMultipartEnabled, \"multipart upload is disabled\") ```"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1157069838 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,13 +1031,22 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } - public static boolean checkDiskBuffer(Configuration conf){ + /** + * Check whether the configuration for S3ABlockOutputStream is + * consistent or not. Multipart uploads allow all kinds of fast buffers to + * be supported. When the option is disabled only disk buffers are allowed to + * be used as the file size might be bigger than the buffer size that can be + * allocated. + * @param conf + * @return Review Comment: nit: document conf argument and retrun value ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java: ########## @@ -1256,7 +1256,16 @@ private Constants() { public static final String PREFETCH_BLOCK_COUNT_KEY = \"fs.s3a.prefetch.block.count\"; public static final int PREFETCH_BLOCK_DEFAULT_COUNT = 8; + /** + * Option to enable or disable the multipart uploads. Review Comment: nit: still needs an {@value} element ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUploadSinglePut.java: ########## @@ -42,14 +51,23 @@ protected Configuration createScaleConfiguration() { configuration.setLong(MULTIPART_SIZE, 53687091200L); configuration.setInt(KEY_TEST_TIMEOUT, 36000); configuration.setInt(IO_CHUNK_BUFFER_SIZE, 655360); - configuration.set(\"fs.s3a.connection.request.timeout\", \"1h\"); + configuration.set(REQUEST_TIMEOUT, \"1h\"); + fileSize = getTestPropertyBytes(configuration, KEY_HUGE_FILESIZE, + DEFAULT_HUGE_FILESIZE); return configuration; } @Test public void uploadFileSinglePut() throws IOException { LOG.info(\"Creating file with size : {}\", fileSize); - ContractTestUtils.createAndVerifyFile(getFileSystem(), - getTestPath(), fileSize ); + S3AFileSystem fs = getFileSystem(); + ContractTestUtils.createAndVerifyFile(fs, + getTestPath(), fileSize); + //No more than three put requests should be made during the upload of the file + //First one being the creation of test/ directory marker + //Second being the creation of the file with tests3ascale/ + //Third being the creation of directory marker tests3ascale/ on the file delete + assertEquals(3L, Review Comment: use `IOStatisticAssertions` here; it generates AssertJ assertion chains from lookups with automatic generation of error text. ```java assertThatStatisticCounter(fs.getIOStatistics(), OBJECT_PUT_REQUESTS.getSymbol()) .isEqualTo(3); ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1859,7 +1859,7 @@ private FSDataOutputStream innerCreateFile( .withPutOptions(putOptions) .withIOStatisticsAggregator( IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()) - .withMultipartAllowed(getConf().getBoolean( + .withMultipartEnabled(getConf().getBoolean( Review Comment: i think the multipart enabled flag should be made a field and stored during initialize(), so we can save on scanning the conf map every time a file is created."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1495990200 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 13s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 25m 59s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 55s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 19s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 32s | | the patch passed | | +1 :green_heart: | compile | 22m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 25s | | the patch passed | | +1 :green_heart: | compile | 20m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 31s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 30s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/artifact/out/results-checkstyle-root.txt) | root: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 2m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 47s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | spotbugs | 4m 11s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 22s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 47s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 230m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 728c0caced16 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ea0007fa8ac7aba823faab608539cf133cb27323 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/testReport/ | | Max. process+thread count | 1259 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1496209366 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 54s | | trunk passed | | +1 :green_heart: | compile | 23m 9s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 42s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 54s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 17s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 22m 31s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 31s | | the patch passed | | +1 :green_heart: | compile | 22m 38s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 22m 38s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/5/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 11s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/5/artifact/out/results-checkstyle-root.txt) | root: The patch generated 2 new + 9 unchanged - 0 fixed = 11 total (was 9) | | +1 :green_heart: | mvnsite | 2m 33s | | the patch passed | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 14s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 20s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 43s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 229m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 0016822b3a9d 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1f56e2a27ba37e1896ccf9b86e346278d149f617 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/5/testReport/ | | Max. process+thread count | 3159 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1498651485 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 46m 46s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 12s | | trunk passed | | +1 :green_heart: | compile | 23m 1s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 40s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 38s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 4s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 0s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 24s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 33s | | the patch passed | | +1 :green_heart: | compile | 22m 32s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 32s | | the patch passed | | +1 :green_heart: | compile | 20m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 34s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/artifact/out/results-checkstyle-root.txt) | root: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 44s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 47s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 31s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 25s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 258m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 4db319f228cd 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4e922b4a81e4c6abc85d43267e74eb3b9827d197 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/testReport/ | | Max. process+thread count | 2152 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1499070067 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 4s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 45s | | trunk passed | | +1 :green_heart: | compile | 23m 10s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 57s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 42s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 34s | | trunk passed | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 23s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 1s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 49s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 12s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 30s | | the patch passed | | +1 :green_heart: | compile | 22m 28s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 28s | | the patch passed | | +1 :green_heart: | compile | 20m 36s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 36s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 31s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/artifact/out/results-checkstyle-root.txt) | root: The patch generated 4 new + 9 unchanged - 0 fixed = 13 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 45s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 46s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 30s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 19s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 226m 22s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 2cbac9757e23 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 13fc2d5d4f4ae17b7701a897969bdb8e0643f8bd | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/testReport/ | | Max. process+thread count | 1291 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1159979341 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -767,6 +781,18 @@ public RequestFactoryBuilder withRequestPreparer( this.requestPreparer = value; return this; } + + /** + * Multipart enabled Review Comment: always add a trailing \".\" on javadocs. it'll save review iterations ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFileUploadSinglePut.java: ########## @@ -0,0 +1,74 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.scale; + +import java.io.IOException; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; +import org.junit.Test; + +import org.apache.hadoop.fs.s3a.S3AFileSystem; +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.contract.ContractTestUtils; +import org.apache.hadoop.fs.s3a.Constants; + +import static org.apache.hadoop.fs.contract.ContractTestUtils.IO_CHUNK_BUFFER_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_SIZE; +import static org.apache.hadoop.fs.s3a.Constants.REQUEST_TIMEOUT; +import static org.apache.hadoop.fs.s3a.S3ATestUtils.getTestPropertyBytes; +import static org.apache.hadoop.fs.s3a.Statistic.OBJECT_PUT_REQUESTS; +import static org.apache.hadoop.fs.statistics.IOStatisticAssertions.assertThatStatisticCounter; + +/** + * Test a file upload using a single PUT operation. Multipart uploads will + * be disabled in the test. + */ +public class ITestS3AHugeFileUploadSinglePut extends S3AScaleTestBase{ + final private Logger LOG = LoggerFactory.getLogger( + ITestS3AHugeFileUploadSinglePut.class.getName()); + + private long fileSize = Integer.MAX_VALUE * 2L; + @Override + protected Configuration createScaleConfiguration() { + Configuration configuration = super.createScaleConfiguration(); + configuration.setBoolean(Constants.MULTIPART_UPLOADS_ENABLED, false); + configuration.setLong(MULTIPART_SIZE, 53687091200L); Review Comment: is this some special value? if so: make a constant, explain what it is. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestRequestFactory.java: ########## @@ -173,7 +174,11 @@ private void createFactoryObjects(RequestFactory factory) { a(factory.newListObjectsV1Request(path, \"/\", 1)); a(factory.newListNextBatchOfObjectsRequest(new ObjectListing())); a(factory.newListObjectsV2Request(path, \"/\", 1)); - a(factory.newMultipartUploadRequest(path, null)); + try { Review Comment: just change have the method throw IOE and cut the try/catch ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java: ########## @@ -99,6 +99,11 @@ public interface StreamCapabilities { */ String IOSTATISTICS_CONTEXT = \"fs.capability.iocontext.supported\"; + /** Review Comment: as this is s3a only, put it in org.apache.hadoop.fs.s3a.Constants with the MULTIPART_UPLOADS_ENABLED definition and using the same prefix `fs.s3a.capability.` as `STORE_CAPABILITY_DIRECTORY_MARKER*` probes ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -460,7 +466,10 @@ public AbortMultipartUploadRequest newAbortMultipartUploadRequest( @Override public InitiateMultipartUploadRequest newMultipartUploadRequest( final String destKey, - @Nullable final PutObjectOptions options) { + @Nullable final PutObjectOptions options) throws IOException { + if(!isMultipartEnabled){ Review Comment: nit: spacing ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: prefer LambdaTestUtils.intercept. why so? * the move to jupiter API is a PITA for backporting etc * intercept() will include the toString() value of whatever was returned (here: the committer) in the exception raised. hence: automatic diagnostics"},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1160262354 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: . in the end for javadoc. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocolFailure.java: ########## @@ -0,0 +1,58 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.staging.integration; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.InternalCommitterConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestStagingCommitProtocolFailure extends AbstractS3ATestBase { + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, InternalCommitterConstants.COMMITTER_NAME_STAGING); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: same intercept. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: Other config names clearly mentions it is multipart upload only. Why are we not using isMultipartUploadEnabled here as well? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); + assertThrows(PathCommitException.class, Review Comment: and it is beautifully integrated with other parts of tests well. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -124,6 +124,11 @@ public class RequestFactoryImpl implements RequestFactory { */ private final StorageClass storageClass; + /** + * Is Multipart Enabled Review Comment: m small, is multipart upload enabled. As this is only for uploads not downloads as well. ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,57 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); + conf.set(S3A_COMMITTER_FACTORY_KEY, CommitConstants.S3A_COMMITTER_FACTORY); + conf.set(FS_S3A_COMMITTER_NAME, CommitConstants.COMMITTER_NAME_MAGIC); + return conf; + } + + @Test + public void testCreateCommitter() { + TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), + new TaskAttemptID()); + Path commitPath = getFileSystem().makeQualified( + new Path(getContract().getTestPath(), \"/testpath\")); + LOG.debug(\"{}\", commitPath); Review Comment: log what are you printing. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: This is still pending. I don't really mind leaving it as it is but I think my suggestion is consistent with other parts of the code and is more readable. CC @steveloughran ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -767,6 +781,18 @@ public RequestFactoryBuilder withRequestPreparer( this.requestPreparer = value; return this; } + + /** + * Multipart enabled Review Comment: +1"},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1160269091 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1831,6 +1832,11 @@ private FSDataOutputStream innerCreateFile( final PutObjectOptions putOptions = new PutObjectOptions(keep, null, options.getHeaders()); + if(!checkDiskBuffer(getConf())){ Review Comment: > just add a method validateOutputStreamConfiguration() and throw exception in the implementation only. This is still pending. I don't really mind leaving it as it is but I think my suggestion is consistent with other parts of the code and is more readable. CC @steveloughran"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1501718145 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 26m 43s | | trunk passed | | +1 :green_heart: | compile | 23m 6s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 41s | | trunk passed | | +1 :green_heart: | javadoc | 1m 53s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 58s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 22s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 31s | | the patch passed | | +1 :green_heart: | compile | 22m 21s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 21s | | the patch passed | | +1 :green_heart: | compile | 20m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 33s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 38s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/artifact/out/results-checkstyle-root.txt) | root: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | +1 :green_heart: | javadoc | 1m 45s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 48s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 4m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 21s | | hadoop-common in the patch passed. | | -1 :x: | unit | 2m 22s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 227m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 665b3a783820 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1476424088e2d20e5cf139dfb7e075d5000218d2 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/testReport/ | | Max. process+thread count | 1259 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1501730716 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 4 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 45s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/artifact/out/blanks-eol.txt) | The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 3 new + 9 unchanged - 0 fixed = 12 total (was 9) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javadoc | 0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 5s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 93m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 97ba7bf3ee91 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / f18c0cb9c0ca8baa6eb911f9c1c753124a8a9785 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/testReport/ | | Max. process+thread count | 758 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/9/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162165006 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,8 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + Preconditions.checkState(!isMultipartUploadEnabled, Review Comment: this is wrong."},{"author":"ASF GitHub Bot","body":"mehakmeet commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162432674 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java: ########## @@ -217,6 +217,10 @@ protected AbstractS3ACommitter( LOG.debug(\"{} instantiated for job \\\"{}\\\" ID {} with destination {}\", role, jobName(context), jobIdString(context), outputPath); S3AFileSystem fs = getDestS3AFS(); + if (!fs.isMultipartUploadEnabled()) { Review Comment: So we want to fail for any s3a committer initialization if the multipart is disabled? iirc magic committer does require multipart but should we be failing for others as well? CC @steveloughran also seems like alot of tests are failing when I run the suite on default props(by default this should be true and not fail here) could be due to UTs using \"MockS3AFileSystem\" which doesn't actually initialize and set the variable. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java: ########## @@ -369,6 +373,8 @@ private synchronized void uploadCurrentBlock(boolean isLast) */ @Retries.RetryTranslated private void initMultipartUpload() throws IOException { + Preconditions.checkState(!isMultipartUploadEnabled, Review Comment: +1"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162568369 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java: ########## @@ -217,6 +217,10 @@ protected AbstractS3ACommitter( LOG.debug(\"{} instantiated for job \\\"{}\\\" ID {} with destination {}\", role, jobName(context), jobIdString(context), outputPath); S3AFileSystem fs = getDestS3AFS(); + if (!fs.isMultipartUploadEnabled()) { Review Comment: they all use multiparts as that is how they write-but-don't-commit the data. this is something harshit and I worked on"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5481: URL: https://github.com/apache/hadoop/pull/5481#discussion_r1162598087 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -414,6 +414,11 @@ public class S3AFileSystem extends FileSystem implements StreamCapabilities, */ private ArnResource accessPoint; + /** + * Is this S3A FS instance has multipart uploads enabled? Review Comment: grammar nit \"is multipart upload enabled?\" ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java: ########## @@ -1031,6 +1031,40 @@ public static long getMultipartSizeProperty(Configuration conf, return partSize; } + /** + * Validates the output stream configuration + * @param conf : configuration object for the given context + * @throws IOException : throws an IOException on config mismatch + */ + public static void validateOutputStreamConfiguration(Configuration conf) throws IOException { + if(!checkDiskBuffer(conf)){ + throw new IOException(\"Unable to create OutputStream with the given\" + + \" multipart upload and buffer configuration.\"); + } + } + + /** + * Check whether the configuration for S3ABlockOutputStream is + * consistent or not. Multipart uploads allow all kinds of fast buffers to + * be supported. When the option is disabled only disk buffers are allowed to + * be used as the file size might be bigger than the buffer size that can be + * allocated. + * @param conf : configuration object for the given context + * @return true if the disk buffer and the multipart settings are supported + */ + public static boolean checkDiskBuffer(Configuration conf) { + boolean isMultipartUploadEnabled = conf.getBoolean(MULTIPART_UPLOADS_ENABLED, + MULTIPART_UPLOAD_ENABLED_DEFAULT); + if (isMultipartUploadEnabled) { + return true; + } else if (!isMultipartUploadEnabled && conf.get(FAST_UPLOAD_BUFFER) Review Comment: can be simplified to ``` return isMultipartUploadEnabled || FAST_UPLOAD_BUFFER_DISK.equals(conf.get(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER)); ``` that default in conf.get is critical to prevent NPEs if the option is unset, moving the constant first even more rigorous ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/api/RequestFactory.java: ########## @@ -199,7 +200,7 @@ AbortMultipartUploadRequest newAbortMultipartUploadRequest( */ Review Comment: document the @throws IOException, stating when it is raised. ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java: ########## @@ -1854,7 +1863,8 @@ private FSDataOutputStream innerCreateFile( .withCSEEnabled(isCSEEnabled) .withPutOptions(putOptions) .withIOStatisticsAggregator( - IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()); + IOStatisticsContext.getCurrentIOStatisticsContext().getAggregator()) + .withMultipartEnabled(isMultipartUploadEnabled); Review Comment: nit, indentation. should be aligned with the `.with` above ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocolFailure.java: ########## @@ -0,0 +1,59 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.staging.integration; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.InternalCommitterConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestStagingCommitProtocolFailure extends AbstractS3ATestBase { + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); Review Comment: again clear any overrides anyone has set. ``` removeBucketOverrides(getTestBucketName(conf), conf, S3A_COMMITTER_FACTORY_KEY, FS_S3A_COMMITTER_NAME, MULTIPART_UPLOADS_ENABLED); } ``` ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java: ########## @@ -460,7 +466,10 @@ public AbortMultipartUploadRequest newAbortMultipartUploadRequest( @Override public InitiateMultipartUploadRequest newMultipartUploadRequest( final String destKey, - @Nullable final PutObjectOptions options) { + @Nullable final PutObjectOptions options) throws IOException { + if (!isMultipartUploadEnabled) { + throw new IOException(\"Multipart uploads are disabled on the given filesystem.\"); Review Comment: make a PathIOException and include destkey. This gives a bit more detail. ``` throw new PathIOException(destKey, \"Multipart uploads are disabled\"); ``` ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocolFailure.java: ########## @@ -0,0 +1,58 @@ +/* + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.s3a.commit.magic; + +import org.junit.Test; + +import org.apache.hadoop.conf.Configuration; +import org.apache.hadoop.fs.Path; +import org.apache.hadoop.fs.s3a.AbstractS3ATestBase; +import org.apache.hadoop.fs.s3a.commit.CommitConstants; +import org.apache.hadoop.fs.s3a.commit.PathCommitException; +import org.apache.hadoop.mapreduce.TaskAttemptContext; +import org.apache.hadoop.mapreduce.TaskAttemptID; +import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl; + +import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_UPLOADS_ENABLED; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.FS_S3A_COMMITTER_NAME; +import static org.apache.hadoop.fs.s3a.commit.CommitConstants.S3A_COMMITTER_FACTORY_KEY; +import static org.apache.hadoop.test.LambdaTestUtils.intercept; + +public class ITestMagicCommitProtocolFailure extends AbstractS3ATestBase { + + @Override + protected Configuration createConfiguration() { + Configuration conf = super.createConfiguration(); + conf.setBoolean(MULTIPART_UPLOADS_ENABLED, false); Review Comment: tests need to make sure that they've removed any per-bucket settings as they can trigger false failures. There's a method in S3ATestUtils to do this - `AbstractCommitITest` shows its use ``` removeBucketOverrides(getTestBucketName(conf), conf, MAGIC_COMMITTER_ENABLED, S3A_COMMITTER_FACTORY_KEY, FS_S3A_COMMITTER_NAME, MULTIPART_UPLOADS_ENABLED); } ```"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1503339214 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 42s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 20s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 35s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 53s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 6s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 5s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/10/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 93m 39s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.s3a.commit.staging.TestStagingDirectoryOutputCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing | | | hadoop.fs.s3a.commit.staging.TestStagingCommitter | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedJobCommit | | | hadoop.fs.s3a.commit.staging.TestStagingPartitionedTaskCommit | | | hadoop.fs.s3a.commit.staging.TestDirectoryCommitterScale | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5481 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint | | uname | Linux 412427820e6b 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7207fddd59f637e3bc05ae2603f9855457c049d8 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/10/testReport/ | | Max. process+thread count | 626 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5481/10/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1504044502 added a change in #5543 to pull in here. now, what do we do for large file renames. currently the transfer manager using that part size to trigger use of MPUs in renames; it doesn't use our request factory so it won't surface. we could add a modified auditor which would trigger an exception on any MPU initialisation POST, then make sure the huge file renames don't trigger it..."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-1504045199 or just bypass the xfer manager entirely in this world and do a single copy request?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504053513 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 20s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 43s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 2 new + 9 unchanged - 0 fixed = 11 total (was 9) | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 31s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 47s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux dbd69a77967d 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d990781d2b5e08d81afeaf22f96e9af8f06850e6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/1/testReport/ | | Max. process+thread count | 737 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163358833 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: remainingCapacity is long so shouldn't it be long.MAX_VALUE"},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163383275 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: Although I see we are always casting to int. So should fine. I am assuming it is like that as we are writing the big file in disk in loop."},{"author":"ASF GitHub Bot","body":"mukund-thakur commented on code in PR #5543: URL: https://github.com/apache/hadoop/pull/5543#discussion_r1163383275 ########## hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java: ########## @@ -849,14 +855,29 @@ long dataSize() { return bytesWritten; } + /** + * Does this block have unlimited space? + * @return true if a block with no size limit was created. + */ + private boolean unlimited() { + return limit < 0; + } + @Override boolean hasCapacity(long bytes) { - return dataSize() + bytes <= limit; + return unlimited() || dataSize() + bytes <= limit; } + /** + * {@inheritDoc}. + * If there is no limit to capacity, return MAX_VALUE. + * @return capacity in the block. + */ @Override long remainingCapacity() { - return limit - bytesWritten; + return unlimited() Review Comment: Although I see we are always casting to int. So should fine. I think it is like that as we are writing the big file in disk in loop."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504240307 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 21s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 39s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 30s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 94m 57s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 5a06c41cf424 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9f07eba3bddc23d8f08aa87731f7c0e8e0a27b38 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/2/testReport/ | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1504241884 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 48s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 23s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 34s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 53s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 35s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 95m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5543 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 5ccf2de415b0 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9f07eba3bddc23d8f08aa87731f7c0e8e0a27b38 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/3/testReport/ | | Max. process+thread count | 700 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5543/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"mukund-thakur merged PR #5543: URL: https://github.com/apache/hadoop/pull/5543"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1505272396 I have a followup for this feature, primarily to reject multipart copy requests when disabled, test to verify that for a large enough threshold, calls don't get rejected."},{"author":"Steve Loughran","body":"[~harshit.gupta] can you do a PR of the trunk commit against branch-3.3 while I do a followup?"},{"author":"Harshit Gupta","body":"Sure, I will get on it."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5543: URL: https://github.com/apache/hadoop/pull/5543#issuecomment-1516269415 @HarshitGupta11 create a new PR with your change for yetus to review, then we can merge through the github ui. No need code reviews, unless related to the backport itself"},{"author":"Steve Loughran","body":"resolved in 3.4.0; harshit -if you want to backport, submit a followup pr"},{"author":"ASF GitHub Bot","body":"HarshitGupta11 opened a new pull request, #5630: URL: https://github.com/apache/hadoop/pull/5630 ### Description of PR Backport of HADOOP-18637 ### How was this patch tested? The patch was tested against uswest-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5630: URL: https://github.com/apache/hadoop/pull/5630#issuecomment-1541917118 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.3 Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 12s | | branch-3.3 passed | | +1 :green_heart: | compile | 0m 34s | | branch-3.3 passed | | +1 :green_heart: | checkstyle | 0m 30s | | branch-3.3 passed | | +1 :green_heart: | mvnsite | 0m 42s | | branch-3.3 passed | | +1 :green_heart: | javadoc | 0m 36s | | branch-3.3 passed | | +1 :green_heart: | spotbugs | 1m 18s | | branch-3.3 passed | | +1 :green_heart: | shadedclient | 25m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 33s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 103m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5630/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5630 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux d88a42d9ca82 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / b8d6365b9f60a1978f8fca0a1f4b245a9d20fe06 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~18.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5630/1/testReport/ | | Max. process+thread count | 641 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5630/1/console | | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran merged PR #5630: URL: https://github.com/apache/hadoop/pull/5630"},{"author":"ASF GitHub Bot","body":"HarshitGupta11 opened a new pull request, #5641: URL: https://github.com/apache/hadoop/pull/5641 ### Description of PR Backport of HADOOP-18637 ### How was this patch tested? The patch is tested against uswest-2 ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1543338764 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 2m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 5 new or modified test files. | |||| _ branch-3.3 Compile Tests _ | | +1 :green_heart: | mvninstall | 36m 33s | | branch-3.3 passed | | +1 :green_heart: | compile | 0m 39s | | branch-3.3 passed | | +1 :green_heart: | checkstyle | 0m 35s | | branch-3.3 passed | | +1 :green_heart: | mvnsite | 0m 46s | | branch-3.3 passed | | +1 :green_heart: | javadoc | 0m 41s | | branch-3.3 passed | | +1 :green_heart: | spotbugs | 1m 21s | | branch-3.3 passed | | +1 :green_heart: | shadedclient | 24m 5s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed | | +1 :green_heart: | spotbugs | 1m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 47s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 34s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 99m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5641/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5641 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint | | uname | Linux 026421debab8 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / 3361f432b5fe7678305460b0a37140bf2dfcf512 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~18.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5641/1/testReport/ | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5641/1/console | | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1543792390 1. what was missing from the previous cherrypick? 2. what were the full commands passed in to maven?"},{"author":"ASF GitHub Bot","body":"HarshitGupta11 commented on PR #5641: URL: https://github.com/apache/hadoop/pull/5641#issuecomment-1545412081 Hi @steveloughran, 1. Due to the merge resolution, the S3ABlockOutputStreamBuilder was not being properly initialised, also the threadpool was not being initialised with the prefetching threads. 2. The commands that were used : `mvn -Dscale -Dparallel-tests -DtestsThreadCount=8 -fae clean verify`"},{"author":"Shilun Fan","body":"-3.3.6 release has been fixed, fix version removed 3.4.0-"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5481: URL: https://github.com/apache/hadoop/pull/5481#issuecomment-3440029664 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5481: HADOOP-18637:S3A to support upload of files greater than 2 GB using DiskBlocks URL: https://github.com/apache/hadoop/pull/5481"}]}
{"key":"HADOOP-18657","summary":"Tune ABFS create() retry logic","description":"Based on experience trying to debug this happening # add debug statements when create() fails # generated exception text to reference string shared with tests, path and error code # generated exception to include inner exception for full stack trace Currently the retry logic is # create(overwrite=false) # if HTTP_CONFLICT/409 raised; call HEAD # use etag in create(path, overwrite=true, etag) # special handling of error HTTP_PRECON_FAILED = 412 There's a race condition here, which is if between 1 and 2 the file which exists is deleted. The retry should succeed, but currently a 404 from the head is escalated to a failure proposed changes # if HEAD is 404, leave etag == null and continue # special handling of 412 also to handle 409","status":"Open","priority":"Major","reporter":"Steve Loughran","assignee":"Steve Loughran","labels":["pull-request-available"],"project":"HADOOP","created":"2023-03-08T14:23:29.000+0000","updated":"2025-10-25T00:22:17.000+0000","comments":[{"author":"ASF GitHub Bot","body":"steveloughran opened a new pull request, #5462: URL: https://github.com/apache/hadoop/pull/5462 ### Description of PR Tunes how abfs handles a failure during create which may be due to concurrency *or* load-related retries happening in the store. * better logging * happy with the confict being resolved by the file being deleted * more diagnostics in failure raised ### How was this patch tested? lease test run already; doing full hadoop-azure test run ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1460326545 fyi @saxenapranav @mehakmeet as well as improving diagnostics, this patch also changes the recovery code by handling a deletion of the target file between the first failure and the retry."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1460491719 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 57s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 12s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 93m 52s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5462 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 0304206b7a96 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a4122e276ad2264c6303eecc3584b63f865dd353 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/1/testReport/ | | Max. process+thread count | 627 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1130438266 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict + // has implicitly been resolved. + LOG.debug(\"File at {} has been deleted; creation can continue\", relativePath); } else { throw ex; } } - String eTag = op.getResult() - .getResponseHeader(HttpHeaderConfigurations.ETAG); + String eTag = op != null + ? op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG) + : null; + LOG.debug(\"Attempting to create file {} with etag of {}\", relativePath, eTag); try { - // overwrite only if eTag matches with the file properties fetched befpre - op = client.createPath(relativePath, true, true, permission, umask, + // overwrite only if eTag matches with the file properties fetched or the file + // was deleted and there is no etag. + // if the etag was not retrieved, overwrite is still false, so will fail + // if another process has just created the file + op = client.createPath(relativePath, true, eTag != null, permission, umask, isAppendBlob, eTag, tracingContext); } catch (AbfsRestOperationException ex) { - if (ex.getStatusCode() == HttpURLConnection.HTTP_PRECON_FAILED) { + final int sc = ex.getStatusCode(); + LOG.debug(\"Failed to create file {} with etag {}; status code={}\", + relativePath, eTag, sc, ex); + if (sc == HttpURLConnection.HTTP_PRECON_FAILED + || sc == HttpURLConnection.HTTP_CONFLICT) { Review Comment: good that we have taken care of 409 which can come when due to `etag!=null` -> overwrite argument to `client.createPath` = false. would be awesome if we can put it in comments, and also have log according to it. log1: about some file is there whose eTag is with our process. When we went back to createPath with the same eTag, some other process had replaced that file which would lead to 412, which is present in the added code: ``` final ConcurrentWriteOperationDetectedException ex2 = new ConcurrentWriteOperationDetectedException( AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED + \" Path =\\\"\" + relativePath+ \"\\\"\" + \"; Status code =\" + sc + \"; etag = \\\"\" + eTag + \"\\\"\" + \"; error =\" + ex.getErrorMessage()); ``` suggestion to add log2: where in when we searched for etag, there was no file, now when we will try to createPath with overWrite = false, if it will give 409 in case some other process created a file on same path. Also, in case of 409, it is similar to the case we started with in this method. Should we get into 409 control as in https://github.com/apache/hadoop/blob/7f9ca101e2ae057a42829883596085732f8d5fa6/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java#L624 for a number of times. Like if we keep threshold as 2. If it happens that it gets 409 at this line, we will try once again to handle 409, post that we fail. @snvijaya @anmolanmol1234 @sreeb-msft, what you feel."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1130757159 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict Review Comment: There is a race condition in the job, and developer should be informed about the same. @snvijaya @anmolanmol1234 @sreeb-msft , what you feel."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1137732758 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { // Is a parallel access case, as file which was found to be // present went missing by this request. - throw new ConcurrentWriteOperationDetectedException( - \"Parallel access to the create path detected. Failing request \" - + \"to honor single writer semantics\"); + // this means the other thread deleted it and the conflict Review Comment: will do; text will indicate this may be due to a lease on the parent dir too."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1470924983 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 50m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 105m 15s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5462 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 24e1da3b49dc 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1853a46ecbb41baf82035664e30cf03584b77a64 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/2/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5462/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1508743152 any updates on this? big issue is whether to retry on 409 or not?"},{"author":"ASF GitHub Bot","body":"snvijaya commented on code in PR #5462: URL: https://github.com/apache/hadoop/pull/5462#discussion_r1131060581 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -621,37 +622,57 @@ private AbfsRestOperation conditionalCreateOverwriteFile(final String relativePa isAppendBlob, null, tracingContext); } catch (AbfsRestOperationException e) { + LOG.debug(\"Failed to create {}\", relativePath, e); if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) { // File pre-exists, fetch eTag + LOG.debug(\"Fetching etag of {}\", relativePath); try { op = client.getPathStatus(relativePath, false, tracingContext); } catch (AbfsRestOperationException ex) { + LOG.debug(\"Failed to to getPathStatus {}\", relativePath, ex); if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) { Review Comment: Hi @steveloughran, Given Hadoop is single writer semantic, would it be correct to expect that as part of job parallelization only one worker process should try to create a file ? As this check for FileNotFound is post an attempt to create the file with overwrite=false, which inturn failed with conflict indicating file was just present, concurrent operation on the file is indeed confirmed. Its quite possible that if we let this create proceed, some other operation such as delete can kick in later on as well. Below code that throws exception at the first indication of parallel activity would be the right thing to do ? As the workload pattern is not honoring the single writer semantic I feel we should retain the logic to throw ConcurrentWriteOperationDetectedException."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-1545445339 reviewing this; too many other things have got in my way. I agree, with create overwrite=false, we must fail with a concurrency error what we don't want to do is overreact if we are doing overwrite=true and something does happen partway. I'll look at this in more detail, maybe focus purely on being meaningful on errors, in particular making sure that if the file is deleted before the error is raised, keep raising that concurrency error."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5462: URL: https://github.com/apache/hadoop/pull/5462#issuecomment-3440030100 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5462: HADOOP-18657. Tune ABFS create() retry logic URL: https://github.com/apache/hadoop/pull/5462"}]}
{"key":"HADOOP-18450","summary":"JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail","description":"Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: {noformat} renameOrFail(Path src, Path dest) throws IOException {noformat} ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: {noformat} $ hadoop key create sample java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691'{noformat} Update the implementation to check for the file, throwing a FileNotFoundException.","status":"Open","priority":"Critical","reporter":"Steve Vaughan","assignee":"Steve Vaughan","labels":["pull-request-available"],"project":"HADOOP","created":"2022-09-12T01:43:46.000+0000","updated":"2025-10-25T00:22:18.000+0000","comments":[{"author":"ASF GitHub Bot","body":"snmvaughan opened a new pull request, #5452: URL: https://github.com/apache/hadoop/pull/5452  in renameOrFail ### Description of PR Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: ``` renameOrFail(Path src, Path dest) throws IOException ``` ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: ``` $ hadoop key create sample java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691' ``` Update the implementation to check for the file, throwing a FileNotFoundException. ### How was this patch tested? Running in an Hadoop development environment docker image. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5452: URL: https://github.com/apache/hadoop/pull/5452#issuecomment-1453873533 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 46s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 28s | | trunk passed | | +1 :green_heart: | compile | 23m 17s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 1m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 47s | | trunk passed | | +1 :green_heart: | javadoc | 1m 17s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 22m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 23s | | the patch passed | | +1 :green_heart: | compile | 20m 36s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 6s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 41s | | the patch passed | | +1 :green_heart: | javadoc | 1m 8s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 24s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 209m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5452 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 523383cac356 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9027fa0f097af13bcee9c54033e400d6994059a5 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/testReport/ | | Max. process+thread count | 1418 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5452/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"virajjasani commented on code in PR #5452: URL: https://github.com/apache/hadoop/pull/5452#discussion_r1132896769 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java: ########## @@ -640,6 +640,10 @@ private void revertFromOld(Path oldPath, boolean fileExisted) private void renameOrFail(Path src, Path dest) throws IOException { if (!fs.rename(src, dest)) { + if (!fs.exists(src)) { + throw new FileNotFoundException(src.toUri().toString()); + } Review Comment: How about keeping this check just before `fs#rename`? Could save efforts of doing rename in the first place?"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5452: URL: https://github.com/apache/hadoop/pull/5452#issuecomment-3440030224 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5452: HADOOP-18450. JavaKeyStoreProvider should throw FileNotFoundException URL: https://github.com/apache/hadoop/pull/5452"}]}
{"key":"HADOOP-17725","summary":"Improve error message for token providers in ABFS","description":"It would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider. For example, when running the following code: {code:java} import org.apache.hadoop.conf._ import org.apache.hadoop.fs._ val conf = new Configuration() conf.set(\"fs.azure.account.auth.type\", \"OAuth\") conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") conf.set(\"fs.azure.account.oauth2.client.id\", \"my-client-id\") // conf.set(\"fs.azure.account.oauth2.client.secret.my-account.dfs.core.windows.net\", \"my-secret\") conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"my-endpoint\") val path = new Path(\"abfss://container@my-account.dfs.core.windows.net/\") val fs = path.getFileSystem(conf) fs.getFileStatus(path){code} The following exception is thrown: {code:java} TokenAccessProviderException: Unable to load OAuth token provider class. ... Caused by: UncheckedExecutionException: java.lang.NullPointerException: clientSecret ... Caused by: NullPointerException: clientSecret {code} which does not tell what configuration key was not loaded. IMHO, it would be good if the exception was something like this: {code:java} TokenAccessProviderException: Unable to load OAuth token provider class. ... Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found. {code}","status":"Resolved","priority":"Major","reporter":"Ivan Sadikov","assignee":"Viraj Jasani","labels":["pull-request-available"],"project":"HADOOP","created":"2021-05-21T11:07:38.000+0000","updated":"2025-10-25T00:25:36.000+0000","comments":[{"author":"Viraj Jasani","body":"[~stevel@apache.org] Would you like to take a look at https://github.com/apache/hadoop/pull/3041? Thanks"},{"author":"Steve Loughran","body":"This seems to be triggering some needless failures. We should look hard and see if it is asking for properties which do not always need to be set"},{"author":"Viraj Jasani","body":"The only parameters that were not covered by Preconditions.checkNotNull are clientId and tenantGuid used by MsiTokenProvider: {code:java} } else if (tokenProviderClass == MsiTokenProvider.class) { String authEndpoint = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); String tenantGuid = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); String clientId = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); String authority = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); authority = appendSlashIfNeeded(authority); tokenProvider = new MsiTokenProvider(authEndpoint, tenantGuid, clientId, authority); LOG.trace(\"MsiTokenProvider initialized\"); } {code} If we replace them back to getPasswordString() and not encounter for precise error message for missing configs, perhaps the failures you are now seeing would be gone. But for any other params that are covered by getMandatoryPasswordString(), they are all also covered by Preconditions.checkNotNull meaning, they would fail with NPE anyways. Hence, are the needless failures relevant to MsiTokenProvider? If yes, this patch would revert the behaviour for them: {code:java} diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java index 9719da7dc16..d43f1d99a77 100644 --- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java +++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java @@ -812,9 +812,9 @@ public AccessTokenProvider getTokenProvider() throws TokenAccessProviderExceptio FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); String tenantGuid = - getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); + getPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT); String clientId = - getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); + getPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID); String authority = getTrimmedPasswordString( FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); {code}"},{"author":"Viraj Jasani","body":"[~ivan.sadikov] Would you like to verify above info? If you still have your account setup, could you also create PR and run tests against the accessible zone? {quote}We should look hard and see if it is asking for properties which do not always need to be set {quote} I have confirmed that the above mentioned props (fs.azure.account.oauth2.msi.tenant and fs.azure.account.oauth2.client.id) are the ones that are not always required to be set."},{"author":"Mehakmeet Singh","body":"Re-opening as it caused some unexpected failures in our environment. Since, MSI tenant ID and client ID are optional, as stated in: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html#OAuth_2.0_Client_Credentials and also discussed above by Viraj and steve, I think we should not mandate them to be set."},{"author":"Viraj Jasani","body":"[~mehakmeetSingh] I can create addendum PR if you would like but I don't have setup to run against an endpoint as of now. Would you like to run tests against any endpoint you have access to? Edit: Managed to get an endpoint to run tests against, but OAuth is not yet setup."},{"author":"Mehakmeet Singh","body":"I can run the tests against an endpoint(on sharedKey auth setup), but I think for this to verify we need OAuth creds, which unfortunately I don't have set up as of now. If [~ivan.sadikov]already have that setup, that's good. We did have some problems setting OAuth tests on our local machines the last time we tried. You can open up a PR in the meantime, and we can start reviewing it."},{"author":"Viraj Jasani","body":"Opened the addendum [PR|https://github.com/apache/hadoop/pull/3788]. Unfortunately, without this addendum, 3.3.2 RC0 might need to sink. However, let [~stevel@apache.org] comment for the final decision. FYI [~csun] [~mehakmeetSingh] [~ivan.sadikov]"},{"author":"Viraj Jasani","body":"{quote}We did have some problems setting OAuth tests on our local machines the last time we tried. {quote} I am also facing issues with tests using OAuth providers as mentioned on the PR: [https://github.com/apache/hadoop/pull/3788]"},{"author":"Steven Swor","body":"[~vjasani] [~stevel@apache.org] Can this be bumped to a later release so that 3.3.2 can go out? This seems to be the only remaining open issue marked for that release."},{"author":"Viraj Jasani","body":"[~stevel@apache.org] Since the first commit already landed on 3.3.2, I believe we can't remove 3.3.2 from fixVersion. Could you please confirm?"},{"author":"Steve Loughran","body":"3.3.2 has this patch in, so closing as fixed in that version."},{"author":"Carl","body":"The changes related to this ticket force to set optional fields and prevent from relying on Azure's metadata service to set them automatically. Both [~vjasani] 's PR ([https://github.com/apache/hadoop/pull/3788)] and mine fix it ([https://github.com/apache/hadoop/pull/4262|https://github.com/apache/hadoop/pull/4262)]). Can we get one of these merged please ?"},{"author":"ASF GitHub Bot","body":"CLevasseur commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1327466165 Hi @pranavsaxena-microsoft, I've tried to follow[ this section ](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/site/markdown/testing_azure.md#generating-test-run-configurations-and-test-triggers-over-various-config-combinations)of the documentation that you mentioned, but it's outdated: - The folder `./src/test/resources/accountSettings/` doesn't exist, nor the template file that I should use to create my account settings file - `dev-support/testrun-scripts/runtests.sh` should prompt a menu, but in my case it runs `AppendBlob-HNS-OAuth` and gives me no choice I followed the rest of the documentation by copying `./src/test/resources/azure-auth-keys.xml.template` to `./src/test/resources/azure-auth-keys.xml` and replacing those variables by the right values in the xml file: - `{ABFS_ACCOUNT_NAME}` - `{ACCOUNT_ACCESS_KEY}` - `{TENANTID}` - `{WASB_ACCOUNT_NAME}` - `{WASB_FILESYSTEM}` - `{CONTAINER_NAME}` - `{ACCOUNT_NAME}` Then I ran ``` dev-support/testrun-scripts/runtests.sh -c \"NonHNS-SharedKey\" [...] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 11.030 s [INFO] Finished at: 2022-11-25T13:08:18Z [INFO] ------------------------------------------------------------------------ Running the combination: NonHNS-SharedKey... # Then it terminates without running anything ``` So it doesn't run the tests, same for `HNS-OAuth` and `HNS-SharedKey`. Do you know what I am missing for those tests to run ?"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328485053 Hi @CLevasseur , the testing section requires you to have latest trunk commits into your branch (as the testing section has been updated for the trunk). Hence requesting you to kindly back-merge apache:trunk into your branch. Thanks."},{"author":"ASF GitHub Bot","body":"CLevasseur commented on PR #4262: URL: https://github.com/apache/hadoop/pull/4262#issuecomment-1328860096 I have pulled the latest changes. It now fails when running the tests. Note that it also fails when I run the tests from the trunk branch of the `apache/hadoop` repository. I haven't setup OAuth2, is that alright if we just run those tests using the Shared Key ? Also, it looks like those tests create a lot of containers in the storage account, is there an easy way to clean those ? **Tests Output (Both apache/hadoop:trunk and CLevasseur/hadoop:trunk give the same errors)** **NonHNS-SharedKey**: ``` Choose action: [Note - SET_ACTIVE_TEST_CONFIG will help activate the config for IDE/single test class runs] 1) SET_ACTIVE_TEST_CONFIG 2) RUN_TEST 3) CLEAN_UP_OLD_TEST_CONTAINERS 4) SET_OR_CHANGE_TEST_ACCOUNT 5) PRINT_LOG4J_LOG_PATHS_FROM_LAST_RUN #? 2 Enter parallel test run process count [default - 8]: Set the active test combination to run the action: 1) HNS-OAuth 2) HNS-SharedKey 3) nonHNS-SharedKey 4) AppendBlob-HNS-OAuth 5) AllCombinationsTestRun 6) Quit #? 3 Combination specific property setting: [ key=fs.azure.account.auth.type , value=SharedKey ] Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: dataenginfraus3prod for combination NonHNS-SharedKey Running test for combination NonHNS-SharedKey on account dataenginfraus3prod [ProcessCount=8] Test run report can be seen in dev-support/testlogs/2022-11-28_10-12-22/Test-Logs-NonHNS-SharedKey.txt pcregrep: pcre_exec() gave error -27 while matching text that starts: [ERROR] ITestAzureBlobFileSystemMainOperation>FSMainOperationsBaseTest.testGlobStatusFilterWithEmptyPathResults:492 ? AbfsRestOperation [ERROR] ITestAzureBlobF pcregrep: Error -8, -21 or -27 means that a resource limit was exceeded. pcregrep: Check your regex for nested unlimited loops. ---"},{"author":"ASF GitHub Bot","body":"virajjasani closed pull request #3788: HADOOP-17725. Keep MSI tenant ID and client ID optional (ADDENDUM) URL: https://github.com/apache/hadoop/pull/3788"}]}
{"key":"HADOOP-18033","summary":"Upgrade fasterxml Jackson to 2.13.0","description":"Spark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper. h2. this has been reverted. we had to revert this as it broke tez.","status":"Resolved","priority":"Major","reporter":"Akira Ajisaka","assignee":"Viraj Jasani","labels":["pull-request-available"],"project":"HADOOP","created":"2021-12-03T14:54:23.000+0000","updated":"2025-10-25T00:26:09.000+0000","comments":[{"author":"Viraj Jasani","body":"Let's upgrade to 2.13.0? It already seems to have good number of usages [here|https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core/2.13.0/usages]."},{"author":"Viraj Jasani","body":"Build fails with 2.13.0 {code:java} [WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: META-INF/versions/11/module-info.class [ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1] [ERROR] {code} Will get back to this after some time."},{"author":"Viraj Jasani","body":"With this upgrade, we will also need to explicitly add new dependency {_}javax.ws.rs:javax.ws.rs-api{_}. We also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies. For the record, let me provide duplicate class details: {code:java} Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: org/apache/hadoop/shaded/javax/ws/rs/POST.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class org/apache/hadoop/shaded/javax/ws/rs/PATCH.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericEntity.class org/apache/hadoop/shaded/javax/ws/rs/core/PathSegment.class org/apache/hadoop/shaded/javax/ws/rs/BadRequestException.class org/apache/hadoop/shaded/javax/ws/rs/ext/ExceptionMapper.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientBuilder.class org/apache/hadoop/shaded/javax/ws/rs/Priorities.class org/apache/hadoop/shaded/javax/ws/rs/HeaderParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Context.class org/apache/hadoop/shaded/javax/ws/rs/container/ResourceContext.class org/apache/hadoop/shaded/javax/ws/rs/ConstrainedTo.class org/apache/hadoop/shaded/javax/ws/rs/Encoded.class org/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class org/apache/hadoop/shaded/javax/ws/rs/client/Entity.class org/apache/hadoop/shaded/javax/ws/rs/client/SyncInvoker.class org/apache/hadoop/shaded/javax/ws/rs/NameBinding.class org/apache/hadoop/shaded/javax/ws/rs/client/Invocation$Builder.class org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyReader.class org/apache/hadoop/shaded/javax/ws/rs/client/ResponseProcessingException.class org/apache/hadoop/shaded/javax/ws/rs/sse/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/client/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestFilter.class org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate$HeaderDelegate.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status$Family.class org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptor.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseContext.class org/apache/hadoop/shaded/javax/ws/rs/ApplicationPath.class org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/PUT.class org/apache/hadoop/shaded/javax/ws/rs/container/ResourceInfo.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$ResponseBuilder.class org/apache/hadoop/shaded/javax/ws/rs/ext/MessageBodyWriter.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource.class org/apache/hadoop/shaded/javax/ws/rs/FormParam.class org/apache/hadoop/shaded/javax/ws/rs/PathParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Application.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$Builder.class org/apache/hadoop/shaded/javax/ws/rs/NotAcceptableException.class org/apache/hadoop/shaded/javax/ws/rs/NotAllowedException.class org/apache/hadoop/shaded/javax/ws/rs/ext/InterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/container/ConnectionCallback.class org/apache/hadoop/shaded/javax/ws/rs/container/TimeoutHandler.class org/apache/hadoop/shaded/javax/ws/rs/core/Request.class org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException.class org/apache/hadoop/shaded/javax/ws/rs/ext/WriterInterceptor.class org/apache/hadoop/shaded/javax/ws/rs/RedirectionException.class org/apache/hadoop/shaded/javax/ws/rs/ext/RuntimeDelegate.class org/apache/hadoop/shaded/javax/ws/rs/CookieParam.class org/apache/hadoop/shaded/javax/ws/rs/container/CompletionCallback.class org/apache/hadoop/shaded/javax/ws/rs/Path.class org/apache/hadoop/shaded/javax/ws/rs/client/Invocation.class org/apache/hadoop/shaded/javax/ws/rs/core/EntityTag.class org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilder.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSource$Builder.class org/apache/hadoop/shaded/javax/ws/rs/DefaultValue.class org/apache/hadoop/shaded/javax/ws/rs/client/Client.class org/apache/hadoop/shaded/javax/ws/rs/ext/FactoryFinder.class org/apache/hadoop/shaded/javax/ws/rs/NotSupportedException.class org/apache/hadoop/shaded/javax/ws/rs/HEAD.class org/apache/hadoop/shaded/javax/ws/rs/core/Link.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter$Lazy.class org/apache/hadoop/shaded/javax/ws/rs/QueryParam.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$StatusType.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseFilter.class org/apache/hadoop/shaded/javax/ws/rs/client/RxInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedHashMap.class org/apache/hadoop/shaded/javax/ws/rs/core/UriBuilderException.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestFilter.class org/apache/hadoop/shaded/javax/ws/rs/client/RxInvokerProvider.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEvent.class org/apache/hadoop/shaded/javax/ws/rs/DELETE.class org/apache/hadoop/shaded/javax/ws/rs/Produces.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType.class org/apache/hadoop/shaded/javax/ws/rs/core/NoContentException.class org/apache/hadoop/shaded/javax/ws/rs/OPTIONS.class org/apache/hadoop/shaded/javax/ws/rs/ext/Provider.class org/apache/hadoop/shaded/javax/ws/rs/BeanParam.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/Feature.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverter.class org/apache/hadoop/shaded/javax/ws/rs/core/Form.class org/apache/hadoop/shaded/javax/ws/rs/Consumes.class org/apache/hadoop/shaded/javax/ws/rs/ClientErrorException.class org/apache/hadoop/shaded/javax/ws/rs/client/CompletionStageRxInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/MultivaluedMap.class org/apache/hadoop/shaded/javax/ws/rs/ext/ReaderInterceptorContext.class org/apache/hadoop/shaded/javax/ws/rs/sse/InboundSseEvent.class org/apache/hadoop/shaded/javax/ws/rs/core/NewCookie.class org/apache/hadoop/shaded/javax/ws/rs/core/Variant$VariantListBuilder.class org/apache/hadoop/shaded/javax/ws/rs/client/WebTarget.class org/apache/hadoop/shaded/javax/ws/rs/core/Configuration.class org/apache/hadoop/shaded/javax/ws/rs/ForbiddenException.class org/apache/hadoop/shaded/javax/ws/rs/RuntimeType.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$1.class org/apache/hadoop/shaded/javax/ws/rs/MatrixParam.class org/apache/hadoop/shaded/javax/ws/rs/client/InvocationCallback.class org/apache/hadoop/shaded/javax/ws/rs/container/Suspended.class org/apache/hadoop/shaded/javax/ws/rs/ext/Providers.class org/apache/hadoop/shaded/javax/ws/rs/InternalServerErrorException.class org/apache/hadoop/shaded/javax/ws/rs/container/DynamicFeature.class org/apache/hadoop/shaded/javax/ws/rs/ext/ContextResolver.class org/apache/hadoop/shaded/javax/ws/rs/core/Cookie.class org/apache/hadoop/shaded/javax/ws/rs/HttpMethod.class org/apache/hadoop/shaded/javax/ws/rs/ServiceUnavailableException.class org/apache/hadoop/shaded/javax/ws/rs/GET.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseEventSink.class org/apache/hadoop/shaded/javax/ws/rs/sse/Sse.class org/apache/hadoop/shaded/javax/ws/rs/container/AsyncResponse.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerResponseFilter.class org/apache/hadoop/shaded/javax/ws/rs/core/Configurable.class org/apache/hadoop/shaded/javax/ws/rs/core/Response.class org/apache/hadoop/shaded/javax/ws/rs/ServerErrorException.class org/apache/hadoop/shaded/javax/ws/rs/core/Form$1.class org/apache/hadoop/shaded/javax/ws/rs/ProcessingException.class org/apache/hadoop/shaded/javax/ws/rs/client/ClientResponseContext.class org/apache/hadoop/shaded/javax/ws/rs/core/Response$Status.class org/apache/hadoop/shaded/javax/ws/rs/NotAuthorizedException.class org/apache/hadoop/shaded/javax/ws/rs/core/Variant.class org/apache/hadoop/shaded/javax/ws/rs/core/CacheControl.class org/apache/hadoop/shaded/javax/ws/rs/core/UriInfo.class org/apache/hadoop/shaded/javax/ws/rs/client/AsyncInvoker.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbAdapter.class org/apache/hadoop/shaded/javax/ws/rs/ext/ParamConverterProvider.class org/apache/hadoop/shaded/javax/ws/rs/core/SecurityContext.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent.class {code}"},{"author":"Viraj Jasani","body":"With a quick search, I realized we already have Jira HADOOP-16908 to remove all org.codehaus.jackson usages from the codebase. Let me link it with this Jira. I can take up that work once this is resolved. Thanks"},{"author":"Akira Ajisaka","body":"Merged the PR into trunk. Hi [~vjasani], would you create a PR for branch-3.3?"},{"author":"Viraj Jasani","body":"Sure [~aajisaka], I am on it."},{"author":"Akira Ajisaka","body":"Committed to branch-3.3. Thank you [~vjasani]"},{"author":"Ayush Saxena","body":"guess, this broke the way for tez to upgrade post 3.3.1 and there after for hive as well. Have been discussing the upgrade stuff for Tez internally and I think this is the one, any way we have decided to settle for at 3.3.1. and that works.... -> {quote}With this upgrade, we will also need to explicitly add new dependency javax.ws.rs:javax.ws.rs-api. We also need to exclude it from shading, else we will get multiple duplicate class clash with existing javax.ws.rs dependencies. {quote} Most probably this is the reason, I will create a Jira and try to see what can be done or if something in trunk already sorted this, I see a couple of Jiras linked to this ticket"},{"author":"Steve Loughran","body":"what is the problem here?"},{"author":"Ayush Saxena","body":"[~stevel@apache.org] It added javax.ws.rs:javax.ws.rs-api and that isn't shaded also and is conflicting with jsr311-api in tez. Can see the error here as well: [https://ci-hadoop.apache.org/job/tez-multibranch/job/PR-213/4/testReport/org.apache.tez.dag.history.ats.acls/TestATSHistoryWithACLs/testDagLoggingDisabled/] and someone quoted some problem here as well(I didn't check what is that though): https://github.com/apache/hadoop/pull/3764#issuecomment-1158641569"},{"author":"Viraj Jasani","body":"[~ayushtkn] Thanks for posting your findings, just had a high level glance at the above failure stacktrace in Tez. {quote}at org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager.createTimelineDomain(ATSHistoryACLPolicyManager.java:127) {quote} For this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api? I understand the pain with minor release upgrade when it has to deal with such issues. FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line. Let me also check if something can be done in the meantime. (As you already noticed, the problem here is that with shading, we get multiple duplicate class clashes for javax.ws.rs dependencies and hence we have no choice but to remove it from shading)"},{"author":"Viraj Jasani","body":"Ah I see, HADOOP-18178 has also bumped Jackson to 2.13.2 in light of fixing CVE-2020-36518, and it has made it's way to 3.3.2 release but I guess the pain related to javax.ws.rs-api remains the same."},{"author":"Ayush Saxena","body":"{quote}For this one, is it be convenient to include javax.ws.rs-api in [https://github.com/apache/tez/blob/master/tez-plugins/tez-yarn-timeline-history-with-acls/pom.xml] ? Or does that also conflict with jsr311-api? {quote} The actual error is : java.lang.AbstractMethodError: javax.ws.rs.core.UriBuilder.uri(Ljava/lang/String;)Ljavax/ws/rs/core/UriBuilder; and this is due to conflict with jsr311-api, javax.ws.rs-api already got included as transitive dependency, if I exclude javax.ws.rs-api in Tez it can make the test pass, but we don't want to play with exclusions as we aren't sure what runtime issues it can create"},{"author":"Ayush Saxena","body":"Spent some time checking if we have any quick solution or not and see how things are: Both {{jsr311-api}} and {{javax.ws.rs-api}} have couple of similar classes and different implementations, That is why this duplicate classes issue started surfacing, I guess Jackson 2 requires implementation classes from {{javax.ws.rs-api}} at runtime or so. In ideal situation we should either have {{javax.ws.rs-api}} or {{js311-api}} in our code, when adding {{javax.ws.rs-api}} if we could have got rid of {{js311-api}} then everything would have been sorted for the shading part. But I guess we have some dependencies on {{{}js311-api{}}}, and it is coming from some other thirdparty libs as well, so may be we have to explore and upgrade them to a version, where they ditch {{js311-api}} for {{{}javax.ws.rs-api{}}}. Then our shading jar should get sorted. How tough is that we don't know, a normal exclude of {{js311-api}} as a transitive dependency isn't a solution because {{javax.ws.rs-api}} has different implementation of methods. The duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude... Now coming for Tez, Tez still has {{js311-api}} as a dependency, if we some how ditch that and move to {{javax.ws.rs-api}} in hadoop, I am not very sure if Tez too have to adapt to our Jackson version and do the same to get things working.. {quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line {quote} Revert isn't an option now, HADOOP-18178 got its way clear only because of this, else it would have been facing this same issue and would have crashed. Now we have a CVE fixed in 3.3.2 & 3.3.3, we can't get it back in 3.3.4, We won't fix a thirdparty CVE we could have said, but after fixing and claiming we have fixed one, we can't get it back AFAIK, this issue only somehow we have to fix. BTW. I am not sure what Spark and Kyubi issues are exactly, that also seems class conflicts may be.. [~pan3793] can you share some more information about that here"},{"author":"Cheng Pan","body":"> BTW. I am not sure what Spark and Kyuubi issues are exactly, that also seems class conflicts may be Yes, I think it's because Jackson requires some classes which only exist in javax.ws.rs-api, which are not bundled into the shaded client. Have a brief look, js311-api is only required by jersey 1.x? If yes, I think upgrading the jersey to 2.x which depends on javax.ws.rs-api and dropping js311-api may be the right direction. And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice."},{"author":"Viraj Jasani","body":"{quote}The duplicate class exception that we saw here was actually an alarm here that these two dependencies can't stay in peace together, but we got away with that by an exclude... {quote} I don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime. I have already tried this before, it doesn't work AFAIK. At least, one of them would have to keep the exclusion on. HADOOP-15983 has upgraded all com.sun.jersey dependencies (jersey-core, jersey-servlet etc) to the latest version in 1.x line and the latest version of jersey-core pulls-in jsr311-api with it: {code:java} [INFO] | +- com.sun.jersey:jersey-core:jar:1.19.4:compile [INFO] | | \\- javax.ws.rs:jsr311-api:jar:1.1.1:compile {code} I don't think without exclusion (and maybe some additional code change, if JAX-RS 1.x and 2.x incompatibilities are in use), we might be able to get rid of jsr311-api. I would expect the same for Tez and other dependencies as well. Tez and other dependencies also can explore the similar path of excluding jsr311-api completely and only rely on JAX-RS 2.x based javax.ws.rs-api (specifically if already using jersey 2.x release versions). jsr311-api is the official spec jar for the JAX-RS 1.x line and the latest central release available is from Nov, 2009 (too old) [https://mvnrepository.com/artifact/javax.ws.rs/jsr311-api] whereas javax.ws.rs-api is jar for JAX-RS 2.x line [https://mvnrepository.com/artifact/javax.ws.rs/javax.ws.rs-api] (latest version from 2018), hence we can expect more upgraded thirdparty libraries (just like Jackson2) having dependency on javax.ws.rs-api and less on jsr311-api. So all downstreamers (Tez, Hadoop, Spark) should try to get rid of jsr311-api anyways, totally agree here. One dependency doesn't necessarily have to wait for another to remove it, for instance, Tez can go ahead with exclusion of jsr311-api even before upgrading to Hadoop 3.3.4 because if not Hadoop, some of it's other dependencies (like jersey-core latest version, as mentioned above) would likely anyways pull it in transitively. On the other hand, let Hadoop also get rid of jsr311-api. But I am pretty sure, removing it won't solve shading issue completely. Will come up with patch because I do recall I have already tried this as part of this Jira only. We can also run full build QA (all modules) and I can manually verify HDFS, MapReduce and ATSv2 working on pseudo-distributed mode."},{"author":"Cheng Pan","body":"The mvnrepository[1] tips that com.sun.jersey:jersey-core was moved to [org.glassfish.jersey.core|https://mvnrepository.com/artifact/org.glassfish.jersey.core] Upgrading(or migrating) to glassfish jersey 2.x should help. [1] [https://mvnrepository.com/artifact/com.sun.jersey/jersey-core]"},{"author":"Viraj Jasani","body":"[~pan3793] Thanks for your comments. {quote}And I also see that Hadoop 3.3.2 mixed use jersey 1.x and 2.x in module hadoop-yarn-applications-catalog-webapp, not sure if it's a good practice. {quote} Do you mean jersey-json or jersey-media-json-jackson? HADOOP-15983 has been a recent work and jersey-media-json-jackson is a test dependency."},{"author":"Viraj Jasani","body":"[~pan3793] HADOOP-15984 has migration to Jersey 2.x related efforts going on."},{"author":"Viraj Jasani","body":"Remove jsr311-api dependency: [https://github.com/apache/hadoop/pull/4460] (to see how QA results go for now)"},{"author":"Ayush Saxena","body":"Removing jsr311-api from hadoop, will not cause duplicate file exception in shading, because when I tried it didn't... But whether we can do that? because jsr311-api & javax.ws.rs-api aren't compatible with each other. That is one thing I am sure, because that only caused Tez to give that AbstractMethod Error... The build will pass I think, If test fails it is good, at least we will come to know what is broken and what needs to be fixed and we can some how figure out how. It might come green as well, because couple of tests which I tried were passing.(Running whole Hadoop test suite isn't easy to run locally) Else if jersey is using jsr311-api, that can create runtime issues, earlier we had jsr311-api in our client jar, now we won't be having that, what impact to downstream projects, that will stay a mystery... Atleast we have to try we don't need have any transitive dependency of jsr311-api I think downstream projects have to get rid of jsr311-api and upgrade Jackson to adapt to this even if we sort this FWIW. Tez is already going ahead with 3.3.1 for the current release: [https://lists.apache.org/thread/7sw84rcc729fgw31g0w9h9y9r61tok9d]"},{"author":"Viraj Jasani","body":"Thanks [~ayushtkn], yes we should hopefully get the full build QA results in ~24 hrs. On the shading side, I meant: {quote}I don't think it was as simple as \"completely removing jsr311-api from Hadoop\" would allow us to exclude shading javax.ws.rs-api from both hadoop-client-minicluster and hadoop-client-runtime {quote} I tried this again and the build fails with the same error that I faced earlier: {code:java} [INFO] --------------- [INFO] Building Apache Hadoop Client Packaging Invariants for Test 3.4.0-SNAPSHOT [105/112] [INFO] --------------------------------[ pom ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-client-check-test-invariants --- [INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target [INFO] Deleting /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants (includes = [dependency-reduced-pom.xml], excludes = []) [INFO] [INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-client-check-test-invariants --- [INFO] Executing tasks main: [mkdir] Created dir: /Users/vjasani/Documents/src/hadoop-trunk/hadoop/hadoop-client-modules/hadoop-client-check-test-invariants/target/test-dir [INFO] Executed tasks [INFO] [INFO] --- maven-enforcer-plugin:3.0.0:enforce (enforce-banned-dependencies) @ hadoop-client-check-test-invariants --- [INFO] Adding ignore: module-info [INFO] Adding ignore: META-INF/versions/*/module-info [INFO] Adding ignorable dependency: org.apache.hadoop:hadoop-annotations:null [INFO] Adding ignore: * [WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message: Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile Duplicate classes: org/apache/hadoop/shaded/javax/ws/rs/POST.class org/apache/hadoop/shaded/javax/ws/rs/core/Link$JaxbLink.class org/apache/hadoop/shaded/javax/ws/rs/NotFoundException.class org/apache/hadoop/shaded/javax/ws/rs/container/PreMatching.class org/apache/hadoop/shaded/javax/ws/rs/container/ContainerRequestContext.class org/apache/hadoop/shaded/javax/ws/rs/core/FeatureContext.class org/apache/hadoop/shaded/javax/ws/rs/core/HttpHeaders.class org/apache/hadoop/shaded/javax/ws/rs/PATCH.class org/apache/hadoop/shaded/javax/ws/rs/sse/OutboundSseEvent$Builder.class org/apache/hadoop/shaded/javax/ws/rs/core/GenericType.class org/apache/hadoop/shaded/javax/ws/rs/sse/SseBroadcaster.class org/apache/hadoop/shaded/javax/ws/rs/core/MediaType$2.class org/apache/hadoop/shaded/javax/ws/rs/core/StreamingOutput.class ... ... ... {code} Hence, with the above PR, I have removed exclusion only from hadoop-client-runtime shade. Now we can confirm that these classes are present in hadoop-client-runtime but not on hadoop-client-minicluster jar: {code:java} $ jar tf hadoop-client-modules/hadoop-client-runtime/target/hadoop-client-runtime-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\" org/apache/hadoop/shaded/javax/ws/rs/core/AbstractMultivaluedMap.class $ jar tf hadoop-client-modules/hadoop-client-minicluster/target/hadoop-client-minicluster-3.4.0-SNAPSHOT.jar | grep \"AbstractMultivaluedMap\" (no-output){code} One question for you: how do we determine which client module to shade the new dependency in? Is it always hadoop-client-runtime (for downstreamers)?"},{"author":"Viraj Jasani","body":"Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this). I just applied this patch, and the build is successful: {code:java} diff --git a/hadoop-client-modules/hadoop-client-runtime/pom.xml b/hadoop-client-modules/hadoop-client-runtime/pom.xml index 35fbd7665fb..0879ce1e3bc 100644 --- a/hadoop-client-modules/hadoop-client-runtime/pom.xml +++ b/hadoop-client-modules/hadoop-client-runtime/pom.xml @@ -163,7 +163,6 @@ org.bouncycastle:* org.xerial.snappy:* - javax.ws.rs:javax.ws.rs-api {code} Created PR [https://github.com/apache/hadoop/pull/4461] for full build results. We can compare QA results for both PRs tomorrow."},{"author":"Ayush Saxena","body":"{quote}Actually, javax.ws.rs-api is not even clashing with jsr311-api (weird, didn't expect this). {quote} The duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster :( Looks like yes: {noformat} Duplicate classes found: Found in: org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile{noformat} AFAIK Whatever gets shaded in hadoop-client-runtime we anyway have to exclude from hadoop-minicluster, we can't shade it in both, there are bunch of lines like. : {{exclude everything that comes in via the shaded runtime and api}} {{exclude things that came in via transitive in shaded runtime and api}} the problem that [~pan3793] quoted for spark & kyubi should get sorted atleast by this"},{"author":"Viraj Jasani","body":"Yeah that problem should be at least resolved by #4461 even if #4460 turns out to be complicated. If #4460 is good, maybe we can pursue some testing in ATSv2 and Yarn to verify the basic functionalities with REST APIs. {quote}The duplicate error that you were quoting was conflict in between hadoop-client-runtime vs hadoop-minicluster {quote} Unfortunately, yes i also realized this now :("},{"author":"Viraj Jasani","body":"Once QA results are available, will create new Jira to link both PRs #4460 and #4461."},{"author":"Cheng Pan","body":"Looks reasonable, thanks [~ayushtkn] and [~vjasani] for investigating this issue. cc [~csun]"},{"author":"Ayush Saxena","body":"Shading issue we might get rid of, because the issue wasn't in the same jar, so that isn't a problem & that might solve problem for some. Tez doesn't pull in hadoop-client dependency AFAIK, Now after this it is pulling in both javax.ws.rs-api and jsr311-api which is creating runtime issues like URIBuilder Class is there in both the packages and so. Excluding/Removing javax.ws.rs-api will make Jackson cry. I tried this as well a lot of test fails. Excluding/Removing jsr311-api makes Jersey cry. Like TestHttpServer#testJersey fails if we exclude it. I tried this only but some more can fail as well. This gives the same error what Tez is getting: {noformat} 2022-06-19 17:53:24,623 WARN server.HttpChannel (HttpChannel.java:handleException(689)) - /jersey/foo java.lang.AbstractMethodError: javax.ws.rs.core.UriBuilder.uri(Ljava/lang/String;)Ljavax/ws/rs/core/UriBuilder; {noformat}"},{"author":"Viraj Jasani","body":"{quote}Excluding/Removing javax.ws.rs-api will make Jackson cry. {quote} {quote}Excluding/Removing jsr311-api makes Jersey cry. {quote} That is so true. Unfortunately we have not received a single QA result on PR#4460 so far. Although Jenkins is getting shut down it seems, at least we have bunch of test results available on PR#4461, hence perhaps some test failures are making builds difficult on 4460. Anyways, we might have to spend another day to see the full results. But we are sure we will have test failures (likely more than expected, as Ayush already mentioned about TestHttpServer#testJersey). Overall, it seems we are back to HADOOP-15984 (upgrading Jersey to 2.x) because so long as we have \"com.sun.jersey:jersey-core\", things will stay complicated."},{"author":"Akira Ajisaka","body":"Thank you [~ayushtkn] [~pan3793] [~vjasani] for your discussion. I'm really sorry for not caching up with. In my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes. Therefore I think we should revert the patch and update to 2.12.x latest to avoid the above issue for (at least) Hadoop 3.3.x. What do you think? Jackson 2.12.x would work because I think the change in https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 caused the issue and it is only in Jackson 2.13.0 and upper."},{"author":"Ayush Saxena","body":"Thanx [~aajisaka] for checking, I am +1 on revert"},{"author":"Cheng Pan","body":"Reverting Jackson in the 3.3 branch looks reasonable to me, since Kyuubi and Spark use Hadoop shaded client, downgrading Jackson from 2.13 to 2.12 should not cause another dependency issue."},{"author":"Viraj Jasani","body":"{quote}In my past experience, Jersey 2.x upgrade takes a lot of time and I think it will cause some incompatible changes. {quote} I agree that 3.3 subsequent releases should not wait for Jersey 2 because of the sheer volume of changes and incompatibility with Jersey 1. From my previous comment: {quote}FWIW, although Hadoop 3.3 could revert this for 3.3.4 release but from security viewpoint, staying up with latest Jackson2 is also in good favour of 3.3 release line, given that 3.3 is the latest release line. {quote} we might have to call out on the Jackson CVE that we claimed to have fixed with 3.3.2 and 3.3.3 and now 3.3.4 would get it exposed with the revert. IIRC, Jersey 1.19 is not flagged by security for active CVEs but Jackson versions <= 2.12 are? But I can understand that since it is breaking downstreamers, it might be worth reverting this and HADOOP-18178 at the expense of known CVE exposure."},{"author":"Akira Ajisaka","body":"bq. we might have to call out on the Jackson CVE The CVE is fixed in 2.12.6.1 or upper (https://github.com/FasterXML/jackson-databind/issues/2816), therefore we should change the version to 2.12.7 (the latest 2.12.x as of now). That way the vulnerability will be still fixed."},{"author":"Akira Ajisaka","body":"Note: When reverting this issue, I recommend to use a separate JIRA because the change was released in 3.3.2 and 3.3.3. That way we can easily track what change is in the specific release."},{"author":"Viraj Jasani","body":"[~aajisaka] do you recommend downgrading to 2.12.7 only for branch-3.3 or for trunk as well? Can trunk continue using Jackson 2.13.2 because Jersey upgrade work is in progress? For 3.4.0 release, HADOOP-15984 can be treated as blocker that way?"},{"author":"Akira Ajisaka","body":"[~vjasani] Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat HADOOP-15984 as a blocker for 3.4.0. (maybe HADOOP-15984 is still a blocker for 3.4.0 regardless of this issue as it blocks compile Hadoop with Java 11)"},{"author":"Ayush Saxena","body":"Well, Jersey upgrade is good to have, for Java-11, kind of new feature support. If we have it in a good and safe manor, But yes I too believe it will have a bunch of incompatible changes, need to see, how to handle that... Reverting for now in both trunk & branch-3.3, makes sense, it will allow us time and won't be blocking any of the release lines. I reverted locally these two commits and pushed a PR. Hopefully it shouldn't break any tests. https://github.com/apache/hadoop/pull/4544"},{"author":"Steve Loughran","body":"ok. someone do the 3.3.x revert and i will get it into the 3.3.4 release i will kick off once it is in"},{"author":"PJ Fanning","body":"[~ayushtkn] would https://issues.apache.org/jira/browse/HADOOP-18332 be worth trying first before looking to undo jackson/rs-api changes? I've been doing a build locally and so far, at least, things look ok (that jsr311-api dependency can be removed)."},{"author":"Ayush Saxena","body":"[~pj.fanning] I think that was tried: https://github.com/apache/hadoop/pull/4460 Try TestHttpServer#testJersey if it passes with your code change"},{"author":"PJ Fanning","body":"So the Tez issue seems (possibly) to be caused by https://github.com/FasterXML/jackson-jaxrs-providers/issues/134 - is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change"},{"author":"Ayush Saxena","body":"{quote}is it ok to downgrade jackson to 2.12.7? - has latest CVE fixes but not this change {quote} Sounds good to me , if we get rid of javax.ws.rs-api dependency without compromising on the CVE, I think there isn't anything better which we can think of. [~aajisaka] too pointed that we can explore moving to 2.12.7. Initially this Jira too was raised to move Jackson to 2.12.x latest. I think if the build doesn't complain post removing javax.ws.rs-api and moving to 2.12.7, then we are sorted"},{"author":"Viraj Jasani","body":"{quote}Currently I recommend downgrading to 2.12.7 in both trunk and branch-3.3. That way we don't need to treat HADOOP-15984 as a blocker for 3.4.0. {quote} I understand that if we are doing the revert with a new Jira, the new Jira should ideally land on trunk before making it's way to active release branches, but Jackson downgrade to 2.12.7 and removal of javax.ws.rs-api would also likely need to be reverted as part of HADOOP-15984, so for HADOOP-15984 it will be too much work staying upto date with trunk (it's already struggling to do so btw with whatever progress is made), and now it will have to reintroduce javax.ws.rs-api and remove jsr311-api. So far I have jsr311-api removed from the current local patch, but if trunk removes javax.ws.rs-api as part of revert of HADOOP-18033 on trunk, there will be rework (basically, revert of revert of HADOOP-18033 for HADOOP-15984 to make progress) that would make the overall progress for HADOOP-15984 even more complicated. Hence, I am requesting if we could only restrict the revert of HADOOP-18033 for branch-3.3 to unblock 3.3.4 release. IIUC, we are anyways not ready for 3.4.0 release anytime soon?"},{"author":"Steve Loughran","body":"i want to kick off a 3.3 3+ cve-only release this week, with the real \"branch-3.3\" coming later. what do we do here? as the longer we think about this the more PRs to update even more jars will surface"},{"author":"PJ Fanning","body":"[~stevel@apache.org] I've had to make a change to https://github.com/apache/hadoop/pull/4547 - there is also https://github.com/apache/hadoop/pull/4544 (which builds ok). The difference is that 4544 uses an older version of Jackson - but both PRs involve downgrading Jackson. Is there a branch you would prefer us to target for your 3.3.3+ cve-only release?"},{"author":"Ayush Saxena","body":"[~stevel@apache.org] I have a PR which reverts the two commits here: [https://github.com/apache/hadoop/pull/4544] So, initial thought was to revert those commits and unblock the releases. Then HADOOP-18332 came up with revert 2 + move to Jackson 2.12.7, so we don't expose the CVE as well and remove the new jar which is creating problems. (Let me know if need separate commits, like 2 different revert commits & one upgrade, will do some CLI stuff with HADOOP-18332) Both revert PR & the new PR have green builds, unfortunately I have a review comment on the new one but that is no big stuff and to me that is the final solution, unless other people come and block us. The plan was to try the Tez stuff as well with that change & ask the other folks who flagged Spark issues to try that as well, but considering the timelines, lets not spend too much time there... {*}So, in best case should unblock the release by day after{*}, considering the build will take some 24 hours, if updated tomorrow. Regarding trunk vs only branch-3.3, in favour of keeping all the branches in sync for now, otherwise if some change comes in trunk which uses this new jar, then we would be doing this revert exercise again and with new set of problems. Moreover no point in keeping the trunk also in broken state. [~vjasani] regarding the effort due to this revert activity and so. The best offer I have is \"I can help or worst get some help\", may be with some rebase effort, so this revert activity doesn't become an overhead for you."},{"author":"Ayush Saxena","body":"I have approved the PR at HADOOP-18332, Tried the two Tez tests which failed with 3.3.3. They pass locally with those changes. Haven't run all the tests though... It is kind of revert of these changes, So, once folks involved here are convinced with the new changes. It can be merged. Nothing blocking from my side now."},{"author":"Viraj Jasani","body":"Thanks [~ayushtkn], HADOOP-18332 PRs (trunk and 3.3) seem good enough to unblock 3.3.4."},{"author":"Steve Loughran","body":"transient CVE issues (snakeyaml) are generating motivation for upgrading hadoop jackson. anyone got an idea about how to do this in a way which could work downstream?"},{"author":"PJ Fanning","body":"We're stuck on Jackson 2.12 because of jersey v1. Jackson 2.13 has a change that drops support for jersey v1. Options include: * forking the jackson module for jaxrs to undo the change that drops jersey v1 support * or removing the dependence on that jackson module by doing https://issues.apache.org/jira/browse/HADOOP-18619 * or completing the move to jersey 2 (https://issues.apache.org/jira/browse/HADOOP-15984)"},{"author":"Ayush Saxena","body":"I have almost lost track of this and honestly didn't return back once Tez-Hive upgrade got sorted. :( Jersey upgrade is the best thing to do, but that is stuck, we need that anyway for JDK-11 compile time support as well. but if thats not working, HADOOP-18619 could be a way out, forking would be a trouble during next upgrades and all."},{"author":"Steve Loughran","body":"java11, please please please"},{"author":"ASF GitHub Bot","body":"virajjasani closed pull request #4460: HADOOP-18033. [WIP] Remove jsr311-api dependency URL: https://github.com/apache/hadoop/pull/4460"}]}
{"key":"HADOOP-18142","summary":"Increase precommit job timeout from 24 hr to 30 hr","description":"As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.","status":"Patch Available","priority":"Minor","reporter":"Viraj Jasani","labels":["pull-request-available"],"project":"HADOOP","created":"2022-02-24T07:14:20.000+0000","updated":"2025-10-25T00:26:24.000+0000","comments":[{"author":"Zoltan Haindrich","body":"[~vjasani] I've noticed this ticket by chance and would like to mention a few things I've done in Hive to avoid kinda like the same issues: * [use rateLimit|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L21] to avoid building the same PR multiple time a day ; this naturally adds a 6 hour wait before the next would start * use a global lock to [limit the number|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L150] of concurrently running * [disable concurrent builds|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L23] as there is no point running the tests for someone who pushed new changes while it was still running => the contributor most likely will push more commits anyway which could launch even more builds...not starting a new build means it could pick up multiple trigger events while the one executing is still running * auto-kill the build in case the PR was updated while it waiting/running ; by calling [this method|https://github.com/apache/hive/blob/master/Jenkinsfile#L30-L45] at a few key points in the build"},{"author":"Viraj Jasani","body":"Thanks [~kgyrtkirk] for the nice suggestions! If you would like to take up this work, please feel free to go ahead with creating PR."},{"author":"Viraj Jasani","body":"[~kgyrtkirk] I went through the changes you have mentioned above, nice work indeed! Based on my recent observation with [PR#4000|https://github.com/apache/hadoop/pull/4000], Hadoop full build definitely exceeds current timeout of 24 hr (regardless of whether we rate limit i.e. run only 1 build or run multiple builds concurrently) hence increasing timeout to 30 hr is a certain requirement for the entire hadoop build to be finished. For the improvements that you have mentioned above (specifically disabling concurrent builds and auto-kill for updated PR), will create a new follow-up Jira."},{"author":"Takanobu Asanuma","body":"It seems the recent QBTs finished within 20 hours. Maybe we should wait and see for a while. https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/"},{"author":"Viraj Jasani","body":"[~tasanuma] It seems QBTs run only single JDK version builds, whereas PR builds run full QA with both Java 8 and 11 (for trunk PRs), hence 24 hr is not sufficient for many runs."},{"author":"Viraj Jasani","body":"One recent example of build timeout after 24 hr https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4267/17/"},{"author":"Shilun Fan","body":"Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0."},{"author":"ASF GitHub Bot","body":"virajjasani closed pull request #4056: HADOOP-18142. Increase precommit job timeout from 24 hr to 30 hr URL: https://github.com/apache/hadoop/pull/4056"}]}
{"key":"HADOOP-19654","summary":"Upgrade AWS SDK to 2.35.4","description":"Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.","status":"In Progress","priority":"Major","reporter":"Steve Loughran","assignee":"Steve Loughran","labels":["pull-request-available"],"project":"HADOOP","created":"2025-08-18T16:47:04.000+0000","updated":"2025-10-25T09:26:53.000+0000","comments":[{"author":"ASF GitHub Bot","body":"steveloughran opened a new pull request, #7882: URL: https://github.com/apache/hadoop/pull/7882 ### How was this patch tested? Testing in progress; still trying to get the ITests working. JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"pan3793 commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651 > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201641390 @pan3793 maybe. what is unrelated is out the box the SDK doesn't do bulk delete with third party stores which support it (Dell ECS). ``` org.apache.hadoop.fs.s3a.AWSBadRequestException: bulkDelete on job-00-fork-0001/test/org.apache.hadoop.fs.contract.s3a.ITestS3AContractBulkDelete: software.amazon.awssdk.services.s3.model.InvalidRequestException: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1):InvalidRequest: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1) -- ```"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201646178 @pan3793 no, it's lifecycle related. Test needs to set up that minicluster before the test cases. and that's somehow not happening"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3209266234 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 49s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 34m 10s | | trunk passed | | +1 :green_heart: | compile | 17m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 12s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 24s | | trunk passed | | +1 :green_heart: | javadoc | 9m 45s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 49s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 15s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 7s | | the patch passed | | +1 :green_heart: | compile | 15m 9s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 9s | | the patch passed | | +1 :green_heart: | compile | 13m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 4m 18s | | the patch passed | | +1 :green_heart: | mvnsite | 18m 45s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 33s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 48s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 368m 52s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 19s | | The patch does not generate ASF License warnings. | | | | 735m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux cb65e960fd1f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0d3f20b487ebe8cca5f4b91a3197d7e6cc639901 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/testReport/ | | Max. process+thread count | 3658 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3221833528 regressions ## everywhere No logging. Instead we get ``` SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\". SLF4J: Defaulting to no-operation MDCAdapter implementation. SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ``` `ITestS3AContractAnalyticsStreamVectoredRead` failures -stream closed. more on this once I've looked at it. If it is an SDK issue, major regression, though it may be something needing changes in the aal libary ## s3 express ``` [ERROR] ITestTreewalkProblems.testDistCp:319->lambda$testDistCp$3:320 [Exit code of distcp -useiterator -update -delete -direct s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/src s3a://stevel--usw2-az1--x-s3/job-00-fork-0005/test/testDistCp/dest] ``` assumption: now that the store has lifecycle rules, you don't get prefix listings when there's an in-progress upload. Fix: change test but also path capability warning of inconsistency. this is good. Operation costs/auditing count an extra HTTP request, so cost tests fail. I suspect it is always calling CreateSession, but without logging can't be sure"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3222650336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 57s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 32m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 39s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 13m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 15s | | trunk passed | | +1 :green_heart: | mvnsite | 23m 16s | | trunk passed | | +1 :green_heart: | javadoc | 9m 41s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 52s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 65m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 32m 0s | | the patch passed | | +1 :green_heart: | compile | 15m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 22s | | the patch passed | | +1 :green_heart: | compile | 13m 52s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 52s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 11s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 18m 51s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 36s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 53s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 65m 59s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 371m 3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 17s | | The patch does not generate ASF License warnings. | | | | 733m 32s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestRollingUpgrade | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 880f3cb624ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5b9a7e32525c27e876698f49e88ab520eae2d8c4 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/testReport/ | | Max. process+thread count | 3821 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3296808329 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 1s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 31s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 24m 17s | | trunk passed | | +1 :green_heart: | compile | 9m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 50s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 2m 0s | | trunk passed | | +1 :green_heart: | mvnsite | 19m 57s | | trunk passed | | +1 :green_heart: | javadoc | 5m 17s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 37s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 11s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 40m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 40s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 3s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 3s | | the patch passed | | +1 :green_heart: | compile | 7m 24s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 54s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 11m 32s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 26s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 7s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 20s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 913m 40s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 113d355d9ed2 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cc31e5be98b54ee418f5ddad4696de2d40e099a0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/testReport/ | | Max. process+thread count | 4200 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3298415046 Thanks @steveloughran, PR looks good overall. Are then failures in `ITestS3AContractAnalyticsStreamVectoredRead` intermittent? I've not been able to reproduce, am running the test on this SDK upgrade branch."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2352378026 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: do you know what the difference is with the other tests here? As in, why with S3 express is it ok to assert that we'll get a 412, whereas the others tests will throw a 200? ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java: ########## @@ -203,7 +206,7 @@ protected Configuration createValidRoleConf() throws JsonProcessingException { conf.set(ASSUMED_ROLE_SESSION_DURATION, \"45m\"); // disable create session so there's no need to // add a role policy for it. - disableCreateSession(conf); + //disableCreateSession(conf); Review Comment: nit: can just cut this instead of commenting it out, since we're skipping these tests if S3 Express is enabled"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3301096683 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 10 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 23m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 30s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 58s | | trunk passed | | +1 :green_heart: | mvnsite | 14m 30s | | trunk passed | | +1 :green_heart: | javadoc | 5m 33s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 38m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 26s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | +1 :green_heart: | compile | 7m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 7m 15s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 58s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 42 unchanged - 5 fixed = 51 total (was 47) | | +1 :green_heart: | mvnsite | 12m 9s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 27s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 5m 4s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 15s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 38m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 678m 56s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 11s | | The patch does not generate ASF License warnings. | | | | 905m 0s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 3b890eb50412 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3351e41830fbc9230ffe18bd88bfc0e2a60b20bd | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/testReport/ | | Max. process+thread count | 4379 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3304013014 I've attached a log of a test run against an s3 express bucket where the test `ITestAWSStatisticCollection.testSDKMetricsCostOfGetFileStatusOnFile()` is failing because the AWS SDK stats report 2 http requests for the probe. I'd thought it was create-session related but it isn't: it looks like somehow the stream is broken. This happens reliably on every test runs. The relevant stuff is at line 564 where a HEAD request fails because the stream is broken \"end of stream\". ``` 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"amz-sdk-request: attempt=1; max=3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=228a46bb1d008468d38afd0da0ed7b4c354ab12631a63bf4283cb23dc02527a3[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\" 2025-09-17 18:43:49,313 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"Connection: Keep-Alive[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-1 >> \"[\\r][\\n]\" 2025-09-17 18:43:49,314 [setup] DEBUG http.wire (Wire.java:wire(87)) - http-outgoing-1 > \"HEAD /test/testSDKMetricsCostOfGetFileStatusOnFile HTTP/1.1[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Host: stevel--usw2-az1--x-s3.s3express-usw2-az1.us-west-2.amazonaws.com[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-invocation-id: 1804bbcd-04de-cba8-8055-6a09917ca20d[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"amz-sdk-request: attempt=2; max=3[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Authorization: AWS4-HMAC-SHA256 Credential=AKIA/20250917/us-west-2/s3express/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;host;referer;x-amz-content-sha256;x-amz-date, Signature=920d981fad319228c969f5df7f5c1a3c7e4d3c0e2f45ff53bba73e6cf47c5871[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Referer: https://audit.example.org/hadoop/1/op_get_file_status/cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008/?op=op_get_file_status&p1=test/testSDKMetricsCostOfGetFileStatusOnFile&pr=stevel&ps=282e3c5d-c1bd-4859-94b9-82e77ff225d1&id=cf739331-1f2e-42dd-a5d9-f564d6023a23-00000008&t0=1&fs=cf739331-1f2e-42dd-a5d9-f564d6023a23&t1=1&ts=1758131029311[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"User-Agent: Hadoop 3.5.0-SNAPSHOT aws-sdk-java/2.33.8 md/io#sync md/http#Apache ua/2.1 api/S3#2.33.x os/Mac_OS_X#15.6.1 lang/java#17.0.8 md/OpenJDK_64-Bit_Server_VM#17.0.8+7-LTS md/vendor#Amazon.com_Inc. md/en_GB m/F,G hll/cross-region[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"x-amz-content-sha256: UNSIGNED-PAYLOAD[\\r][\\n]\" 2025-09-17 18:43:49,673 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"X-Amz-Date: 20250917T174349Z[\\r][\\n]\" 2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"Connection: Keep-Alive[\\r][\\n]\" 2025-09-17 18:43:49,674 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 >> \"[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"HTTP/1.1 200 OK[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"server: AmazonS3[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-request-id: 01869434dd00019958c6871b05090b3f875a3c90[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-id-2: 9GqfbNyMyUs6[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"etag: \"6036aaaf62444466bf0a21cc7518f738\"[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"accept-ranges: bytes[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"last-modified: Wed, 17 Sep 2025 17:43:49 GMT[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-storage-class: EXPRESS_ONEZONE[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-type: application/octet-stream[\\r][\\n]\" 2025-09-17 18:43:49,859 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-server-side-encryption: AES256[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"content-length: 0[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"x-amz-expiration: NotImplemented[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"date: Wed, 17 Sep 2025 17:43:48 GMT[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG http.wire (Wire.java:wire(73)) - http-outgoing-2 << \"[\\r][\\n]\" 2025-09-17 18:43:49,860 [setup] DEBUG awssdk.request (LoggerAdapter.java:debug(105)) - Received successful response: 200, Request ID: ``` Either the request is being rejected (why?) or the connection has gone stale. But why should it happen at exactly the same place on every single test run? [org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt](https://github.com/user-attachments/files/22391405/org.apache.hadoop.fs.s3a.statistics.ITestAWSStatisticCollection-output.txt)"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #7882: URL: https://github.com/apache/hadoop/pull/7882#discussion_r2356314622 ########## hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java: ########## @@ -390,7 +416,7 @@ public void testIfNoneMatchOverwriteWithEmptyFile() throws Throwable { // close the stream, should throw RemoteFileChangedException RemoteFileChangedException exception = intercept(RemoteFileChangedException.class, stream::close); - assertS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); + verifyS3ExceptionStatusCode(SC_412_PRECONDITION_FAILED, exception); Review Comment: Hey, it's your server code. Go see."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3305933937 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 34s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 12m 12s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 40m 29s | | trunk passed | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 0s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 4m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 21m 27s | | trunk passed | | +1 :green_heart: | javadoc | 9m 42s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 58s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 66m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 3s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 40m 59s | | the patch passed | | +1 :green_heart: | compile | 15m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 18s | | the patch passed | | +1 :green_heart: | compile | 13m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 13m 50s | | the patch passed | | -1 :x: | blanks | 0m 1s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 4m 10s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/results-checkstyle-root.txt) | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | +1 :green_heart: | mvnsite | 19m 25s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 38s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 50s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +0 :ok: | spotbugs | 0m 21s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 66m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 450m 14s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 21s | | The patch does not generate ASF License warnings. | | | | 832m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 40fa101aa5ab 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 661dc6e3caa66f1218db70d8e6959c2ee3cb0a87 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/testReport/ | | Max. process+thread count | 3559 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306500643 @steveloughran discovered completely by accident, but it's something to do with the checksumming code. If you comment out these lines: ``` // builder.addPlugin(LegacyMd5Plugin.create()); // do not do request checksums as this causes third-party store problems. // builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED); // response checksum validation. Slow, even with CRC32 checksums. // if (parameters.isChecksumValidationEnabled()) { // builder.responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED); // } ``` the test will pass. Could be something to do with s3Express not supporting md5, will look into it."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3306741287 Specifically, it's this line: `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED);` that causes this. Comment that out, or change it to `builder.requestChecksumCalculation(RequestChecksumCalculation.WHEN_SUPPORTED)`, it passes. My guess is it's something to do with S3 express not supporting MD5, but for operations where `RequestChecksumCalculation.WHEN_REQUIRED` is true, SDK calculates the m5 and then S3 express rejects it. Have asked the SDK team."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3307416441 ok, so maybe for s3express stores we don't do legacy MD5 plugin stuff all is good? 1. Does imply the far end is breaking the connection when it is unhappy -at least our unit tests found this stuff before the cost of every HEAD doubles. 2. maybe we should make the choice of checksums an enum with md5 the default, so it is something that can be turned off/changed in future. While on the topic of S3 Express, is it now the case that because there's lifecycle rules for cleanup, LIST calls don't return prefixes of paths with incomplete uploads? If so I will need to change production code and the test -with a separate JIRA for that for completeness"},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3311549488 > for s3express stores we don't do legacy MD5 plugin stuff all is good @steveloughran confirming with the SDK team, since the MD5 plugin is supposed to restore previous behaviour, the server rejecting the first request seems wrong. let's see what they have to say. > LIST calls don't return prefixes of paths with incomplete uploads Will check with S3 express team on this"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3312197290 thanks. I don't see it on tests against s3 with the 2.29.52 release, so something is changing with the requests made with new SDK + MD5 stuff."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3318915081 @steveloughran not able to narrow this error down just yet, it looks like it's a combination of S3A's configuration of the S3 client + these new Md5 changes. ``` @Test public void testHead() throws Throwable { // S3Client s3Client = getFileSystem().getS3AInternals().getAmazonS3Client(\"test instance\"); S3Client s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\").build()); } ``` I see the failure when the S3A client, and don't see it when I use a newly created client. So it's not just because of `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` Looking into it some more. S3 express team said there have been no changes in LIST behaviour."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3320014995 able to reproduce the issue outside of S3A. Basically did what would happen when you run a test in S3A: * a probe for the `test/` directory, and then create the `test/` directory, and then do the `headObject()` call. The head fails, but if you comment out `requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED)` it works again. no idea what's going on. but have shared this local reproduction with SDK team. And rules out that it's something in the S3A code. ``` public class TestClass { S3Client s3Client; public TestClass() { this.s3Client = S3Client.builder().region(Region.US_EAST_1) .addPlugin(LegacyMd5Plugin.create()) .requestChecksumCalculation(RequestChecksumCalculation.WHEN_REQUIRED) .responseChecksumValidation(ResponseChecksumValidation.WHEN_SUPPORTED) .overrideConfiguration(o -> o.retryStrategy(b -> b.maxAttempts(1))) .build(); } public void testS3Express(String bucket, String key) { s3Client.listObjectsV2(ListObjectsV2Request.builder() .bucket(\"<>\") .maxKeys(2) .prefix(\"test/\") .build()); try { s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"test\") .build()); } catch (Exception e) { System.out.println(\"Exception thrown: \" + e.getMessage()); } s3Client.putObject(PutObjectRequest .builder() .bucket(\"<>\") .key(\"test/\").build(), RequestBody.empty()); s3Client.headObject(HeadObjectRequest.builder().bucket(\"<>\") .key(\"<>\") .build()); } ```"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3325180154 well, nice and simple code snippet for the regression testing. Shows the value in having sdk metrics tied up...this is the only case which failed because it's the one asserting at the SDK level values."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3353451859 @ahmarsuhail is there a public sdk issue for this for me to link/track?"},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3355348352 Just created https://github.com/aws/aws-sdk-java-v2/issues/6459"},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3370618249 FYI @steveloughran , SDK team was able to root cause the issue, details here: https://github.com/aws/aws-sdk-java-v2/issues/6459#issuecomment-3362570846 Since it's a bit of an edge case, and the SDK retry means we recover from it anyway, you think we can go ahead with the upgrade or should we wait for the fix?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3405709735 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 11 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 11m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 27m 4s | | trunk passed | | +1 :green_heart: | compile | 15m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 48s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 13s | | trunk passed | | -1 :x: | mvnsite | 10m 26s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 10s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 43s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 31m 2s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 12 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 57m 7s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 50s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 28m 35s | | the patch passed | | +1 :green_heart: | compile | 15m 6s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 6s | | the patch passed | | +1 :green_heart: | compile | 15m 39s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 39s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 13s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-checkstyle-root.txt) | root: The patch generated 7 new + 42 unchanged - 5 fixed = 49 total (was 47) | | -1 :x: | mvnsite | 7m 15s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 2s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 1 new + 43015 unchanged - 0 fixed = 43016 total (was 43015) | | +1 :green_heart: | javadoc | 8m 45s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 24s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 57m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 793m 21s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1110m 44s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 503d715744fe 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e319765ca591fc2a0968f3b2e900586bb46ce7c1 | | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/testReport/ | | Max. process+thread count | 3551 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/10/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3408678775 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 15 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 9m 9s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 16m 19s | | trunk passed | | +1 :green_heart: | compile | 8m 32s | | trunk passed | | +1 :green_heart: | checkstyle | 1m 41s | | trunk passed | | -1 :x: | mvnsite | 6m 5s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 5s | | trunk passed | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 28s | [/branch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws.txt) | hadoop-aws in trunk failed. | | -1 :x: | spotbugs | 0m 16s | [/branch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/branch-spotbugs-root.txt) | root in trunk failed. | | +1 :green_heart: | shadedclient | 14m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 7s | | the patch passed | | +1 :green_heart: | compile | 8m 25s | | the patch passed | | +1 :green_heart: | javac | 8m 25s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 32s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-checkstyle-root.txt) | root: The patch generated 11 new + 47 unchanged - 6 fixed = 58 total (was 53) | | -1 :x: | mvnsite | 3m 45s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 4m 54s | [/results-javadoc-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/results-javadoc-javadoc-root.txt) | root generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | -1 :x: | spotbugs | 0m 26s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | -1 :x: | spotbugs | 0m 17s | [/patch-spotbugs-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-spotbugs-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 13m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 227m 31s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 343m 23s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux f49b0547d834 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c2eb04aa497f8d4648a3b457a90843ce96abe7fe | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/testReport/ | | Max. process+thread count | 5153 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/12/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412290514 @ahmarsuhail I'm handling the retries now by requiring the md5 plugin to be explicitly requested (i.e. third party stores); also making it easier to switch checksum generation from ALWAYS to WHEN_REQUESTED. So for AWS S3: stricter checksums, no md5. Other stores: configure it as needed. Still wondering if we should make this more automated, but not in a way which causes problems later. --- I am now seeing failings against s3 express ``` org.opentest4j.AssertionFailedError: [Counter named audit_request_execution with expected value 4] Expecting: to be equal to: but was not. Expected :4 Actual :11 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticValue(IOStatisticAssertions.java:274) at org.apache.hadoop.fs.statistics.IOStatisticAssertions.verifyStatisticCounterValue(IOStatisticAssertions.java:175) at org.apache.hadoop.fs.s3a.ITestS3AAnalyticsAcceleratorStreamReading.testMultiRowGroupParquet(ITestS3AAnalyticsAcceleratorStreamReading.java:186) at java.lang.reflect.Method.invoke(Method.java:498) at java.util.ArrayList.forEach(ArrayList.java:1259) at java.util.ArrayList.forEach(ArrayList.java:1259) ``` I'm changing this test to measure the # of audited requests before the file opening begins and then assert on the difference between them."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412377933 Now that 3rd party is good, I'm getting S3 express happy, mainly by test tuning. But many, many errors with vectored reads ``` [ERROR] Errors: [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testAllRangesMergedIntoOne  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testBufferSlicing  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testConsecutiveRanges  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testMultipleVectoredReads  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead [INFO] Run 1: PASS [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testNormalReadAfterVectoredRead  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testSomeRandomNonOverlappingRanges  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAndReadFully [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220  IO test/vectored_file.txt: Stream is closed! [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:220  IO test/vectored_file.txt: Stream is closed! [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadMultipleRanges [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206  Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:206  Execution java.io.IOException: Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile [ERROR] Run 1: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [ERROR] Run 2: ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadWholeFile  IO Server error accessing s3://stevel--usw2-az1--x-s3/test/vectored_file.txt [INFO] [INFO] ``` ``` [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractAnalyticsStreamVectoredRead.testReadVectoredWithAALStatsCollection"},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3412492676 @steveloughran just ran with the old 2.29.x SDK, failures there too. will look into it and fix"},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415388890 This is happening because those readVectored() tests create a new `vectored-read.txt` file on the setup() before each test. Since the tests are parameterized, they run twice, once for `direct-buffer` and then for `array-buffer`. On the first run for `direct-buffer`, a HEAD for the metadata is made and cached, and the data for `vectored-read.txt` is also cached. Then the stream is `closed()` and since the file ends in `.txt`, AAL clears the data cache. (since it's a sequential format, the chances there will be a backward seek and the same data will be accessed are low so it's better to clear the data cache). The metadata cache is not cleared here (it should be, and I will make that fix). On the second run for `array-buffer`, the `vectored-file` is written again. AAL will get the metadata from the metadata cache, and use that eTag when making the GETS for the block data. Since on S3 express, the eTag is no longer the md5 of the object content, even though the object content is the same, the eTag has changed. And hence the 412s on the GETS. On consistency with caching in general: * AAL provides a `metadatastore.ttl` config, set that to 0 and HEAD responses are never cached. This solves the caching issues we had when overwrite files before, as with that `ttl` 0 we will always get the latest version of the file. * Data blocks will be removed once memory usage is > defined memory threshold (2GB), and clean up happens every 5s by default. The edge case here is that what if data usage is always below 2GB, and data blocks never get evicted? This is why the `metadatastore.ttl` was introduced. * Our `BlockKey` which is the key under which file data is stored is a combination of the S3URI + eTag. If the eTag changes, then we'll have a different BlockKey, which means we don't have any data stored for it. For example: ``` * Data is written to A.parquet, etag is \"1234\". * A.parquet is read fully in to the cache, with key \"A.parquet + 1234\" * A.parquet is overwritten, etag is \"6789\". * A.parquet is opened for reading again: If metadata ttl has not yet expired, and metadata cache has eTag as `1234`, so AAL will return data from the data cache using key \"A.parquet + 1234\". If the requested data is not in the data cache, we'll make a GET with the outdated eTag as `1234` and this will fail with a 412. If metadata TTL has expired, a new HEAD request is made, and we now have the eTag `6789`, this will now create a new BlockKey \"A.parquet + 6789\", and since there is no data stored here, will make GETS for the data. ``` With this we ensure two things: 1/ Once a stream opened it will always serve bytes from the same object version, or fail. 2/ Data will be stale at maximum metadata.tll milliseconds, with the exception of stream's lifetime. Basically, if your data changes often, set the metadataTTL to 0, and AAL will always get the latest data. Otherwise we have eventually consistency."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3415396379 The TLDR is: * I will make a small fix to clear the metadata cache on stream close for sequential formats (which fixes this issue) * Setting the `metadata.ttl` also fixes this issue. I've tested with both, and all `ITestS3AContractAnalyticsStreamVectoredRead` test cases pass."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416145759 good explanation. Though I would have expected a bit broader test coverage of your own stores; something to look for on the next library update. Can I also get improvements in error translation too -we need the error string including request IDs. Relying on the stack entry below to print it isn't enough, as deep exception nesting (hive, spark) can lose that."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3416153341 one more thing here: make sure you can handle `null` as an etag in the cache. Not all stores have it, which is why it can be turned off for classic input stream version checking. You won't be able to detect overwrites, but we can just document having a short TTL here."},{"author":"ASF GitHub Bot","body":"ahmarsuhail commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3422219254 @steveloughran updated exception handling: https://github.com/awslabs/analytics-accelerator-s3/pull/361, next release will have include the requestIDs in the message, eg: ``` java.io.IOException: Server error accessing s3://xxx, request failed with: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: xxxx, Extended Request ID: xxxx) ``` The null as `eTag` will require more work, the only way to do that reliably is to disable the caching fully and provide a pass through stream. Do you know which stores don't support eTags?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428848006 latest iteration works with third party stores without MPU (so no magic or use of memory for upload buffering), or bulk delete. tested google gcs, only underful buffers which can be ignored. ``` [ERROR] Failures: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:108->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: but was: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:61->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: but was: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:71->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: but was: [ERROR] ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testUnbufferOnClosedFile:91->AbstractContractUnbufferTest.validateFullFileContents:141->AbstractContractUnbufferTest.validateFileContents:148 failed to read expected number of bytes from stream. This may be transient ==> expected: but was: [INFO] [ERROR] Tests run: 1253, Failures: 4, Errors: 0, Skipped: 450 [INFO] ```"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3428880917 @ahmarsuhail I think Apache Ozone is the one. I just added an `etag` command to cloudstore to print this stuff out and experimented with various stores: https://github.com/steveloughran/cloudstore/blob/main/src/main/site/etag.md dell ECS and Google both supply etags. We don't retrieve them for directory markers anyway, which isn't an issue * I've updated the third-party docs to cover etags in more detail, and say \"switch to classic and disable version checking\" * I do think the cache needs to handle null/empty string tags, somehow. Certainly by not caching metadata."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3432106892 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 15m 2s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 38 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 43s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 29m 3s | | trunk passed | | +1 :green_heart: | compile | 15m 53s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 3m 15s | | trunk passed | | -1 :x: | mvnsite | 10m 51s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 31s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 28s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 18s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 36m 27s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 61m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 48s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 26s | | the patch passed | | +1 :green_heart: | compile | 14m 53s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 53s | | the patch passed | | +1 :green_heart: | compile | 15m 24s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 24s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/blanks-eol.txt) | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 3m 15s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-checkstyle-root.txt) | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 7m 2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 9m 37s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 8m 41s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 22s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 62m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 741m 49s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 44s | | The patch does not generate ASF License warnings. | | | | 1085m 5s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 8fbc6faf5962 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1cd8a2820c4aadeca61f3a7449c7d98fd34bb9d8 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/testReport/ | | Max. process+thread count | 3717 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/14/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3434634222 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 18s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 15s | | trunk passed | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 25s | | trunk passed | | -1 :x: | mvnsite | 6m 10s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 31s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 43s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 42s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 11s | | the patch passed | | +1 :green_heart: | compile | 7m 52s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 7m 52s | | the patch passed | | +1 :green_heart: | compile | 8m 17s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 17s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/blanks-eol.txt) | The patch has 24 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 33s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-checkstyle-root.txt) | root: The patch generated 14 new + 79 unchanged - 6 fixed = 93 total (was 85) | | -1 :x: | mvnsite | 3m 43s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 5m 23s | [/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 4 new + 46184 unchanged - 0 fixed = 46188 total (was 46184) | | -1 :x: | javadoc | 4m 38s | [/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/results-javadoc-javadoc-root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 4 new + 43015 unchanged - 0 fixed = 43019 total (was 43015) | | +0 :ok: | spotbugs | 0m 12s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 594m 47s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 779m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.server.federation.router.async.TestRouterAsyncRpcClient | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux c90f744f2220 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 96c3f38ea5e033636e1acdb8fe2ed4b398bedb08 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/testReport/ | | Max. process+thread count | 4624 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/15/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3437736945 fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`) ``` [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] ``` This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h When these uploads fail we do leave incomplete uploads in progress: ``` Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. ``` Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. ``` [ERROR] ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190  AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) ``` Yet the uploads list afterwards finds it ``` job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 ``` I have to conclude that the list of pending uploads was briefly offline/inconsistent. This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute. Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\"."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3441063874 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 39 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 48s | | trunk passed | | +1 :green_heart: | compile | 8m 10s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 15s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 6m 44s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 35s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 51s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 15s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 0m 41s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 19m 4s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 34m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 16m 48s | | the patch passed | | +1 :green_heart: | compile | 8m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 49s | | the patch passed | | +1 :green_heart: | compile | 9m 26s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 9m 26s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 41s | | root: The patch generated 0 new + 79 unchanged - 6 fixed = 79 total (was 85) | | -1 :x: | mvnsite | 4m 11s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 17s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 44s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 34m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 591m 32s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 51s | | The patch does not generate ASF License warnings. | | | | 781m 38s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle markdownlint shellcheck shelldocs | | uname | Linux 54d25015775c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fa906dcf97ff8829f50184906bd7433bc2a0a73a | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/testReport/ | | Max. process+thread count | 4675 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/17/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444243683 OK, this is all related to checksums on multipart puts. If you declare that checksums are always required on requests, you MUST define a checksum algorithm to use for multipart put, otherwise upload completions fail. I have no idea why, will file some SDK bug report to say \"this is wrong\" and simply change our settings to - checksums NOT always required - MD5 always enabled - checksum algorithm is CRC32C (will test with third party store) checksums in MPUs breaks a couple of the multipart uploader tests; more worried that about a ITestS3AOpenCost test failing with checksum verification being enabled (slow, expensive). I need to make sure that this is not an SDK regression."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3444263138 Its a change in the default value: downloads have checksums verified unless you say \"no\". we say no."},{"author":"Steve Loughran","body":"AWS SDK issue #6518 shows how checksum generation on uploaded data (fs.s3a.create.checksum) must be set if request checksum calculation is enabled (fs.s3a.checksum.generation) Checksum validation has also been enabled by default; {{ITestS3AOpenCost.testStreamIsNotChecksummed()}} caught that change. It looks like the SDK has really embraced checksums, which first broke compatibility with other stores, but which has also surfaced problems within their own code. All checksum logic will be off by default; MD5 headers will be attached now"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7882: URL: https://github.com/apache/hadoop/pull/7882#issuecomment-3446382053 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 44 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 10m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 15m 50s | | trunk passed | | +1 :green_heart: | compile | 8m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 1m 36s | | trunk passed | | -1 :x: | mvnsite | 5m 54s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 5m 18s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 41s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +0 :ok: | spotbugs | 0m 16s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | -1 :x: | spotbugs | 1m 27s | [/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html) | hadoop-common-project/hadoop-common in trunk has 448 extant spotbugs warnings. | | -1 :x: | spotbugs | 0m 39s | [/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-aws-warnings.html) | hadoop-tools/hadoop-aws in trunk has 188 extant spotbugs warnings. | | -1 :x: | spotbugs | 18m 32s | [/branch-spotbugs-root-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/branch-spotbugs-root-warnings.html) | root in trunk has 9241 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 32m 9s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 15m 37s | | the patch passed | | +1 :green_heart: | compile | 8m 6s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 6s | | the patch passed | | +1 :green_heart: | compile | 8m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 20s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/blanks-eol.txt) | The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 30s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/results-checkstyle-root.txt) | root: The patch generated 6 new + 83 unchanged - 6 fixed = 89 total (was 89) | | -1 :x: | mvnsite | 3m 47s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 5m 16s | | root-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 0 new + 46182 unchanged - 2 fixed = 46182 total (was 46184) | | +1 :green_heart: | javadoc | 4m 41s | | root-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 0 new + 43013 unchanged - 2 fixed = 43013 total (was 43015) | | +0 :ok: | spotbugs | 0m 11s | | hadoop-project has no data from spotbugs | | +1 :green_heart: | shadedclient | 32m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 587m 3s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 770m 3s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.service.TestYarnNativeServices | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7882 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint markdownlint shellcheck shelldocs | | uname | Linux cc0336ad4f43 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 149e98291ada965d1dc2b85b4214f235bb4b5c5d | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/testReport/ | | Max. process+thread count | 4586 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7882/20/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."}]}
{"key":"HADOOP-18640","summary":"ABFS: Enabling Client-side Backoff only for new requests","description":"Enabling backoff only for new requests that happen, and disabling for retried requests.","status":"Open","priority":"Minor","reporter":"Sree Bhattacharyya","assignee":"Sree Bhattacharyya","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-23T09:50:25.000+0000","updated":"2025-10-26T00:24:15.000+0000","comments":[{"author":"ASF GitHub Bot","body":"sreeb-msft opened a new pull request, #5446: URL: https://github.com/apache/hadoop/pull/5446 This PR introduces two changes that allows client-side throttling and backoff only for new requests, and increases the level of control through new configs in AbfsClientThrottlingAnalyzer. For the first change, it checks for whether the current rest operation corresponds to a new or retried request. In case of a new request, it calls the necessary methods to apply throttling and backoff at the client side if necessary. In case of a retried request, these methods are skipped and no backoff is applied, or even checked if necessary. This, however, does not affect any other flow such as updating metrics, which happens for each request, irrespective of whether retried or new. In code, the check for whether it is a retried request or not, and the subsequent call for CST is moved to a separate new method, which directly takes input of the current retry count, and makes the decision based on that. #### Tests Added: Two separate tests have been added as part of this change, one for the read and the other for write/append requests - which are the only cases where client-side throttling is applied. Each test does the following - 1. Validates for a new request: - The method for CST (sendingRequest) is called. - The counters for throttling (read/append) are incremented by 1. 2. Validates for a retried request: - The CST method (sendingRequest) call is skipped. - The counters for throttling (read/append) are not incremented and are the same before and after the apply throttling backoff call happens. For the second change, new configs are introduced for the following - 1. `fs.azure.min.acceptable.error.percentage` 2. `fs.azure.max.equilibrium.error.percentage` 3. `fs.azure.rapid.sleep.decrease.factor` 4. `fs.azure.rapid.sleep.decrease.transition.ms` 5. `fs.azure.sleep.decrease.factor` 6. `fs.azure.sleep.increase.factor` All of these are meant to replace the static values that were in place, by allowing the user more control to configure these directly. The static values for these variables, earlier present in code itself, now would be used as default values for the configs."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1122694142 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -272,6 +273,30 @@ DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MIN_ACCEPTABLE_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE) + private double minAcceptableErrorPercentage; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MAX_EQUILIBRIUM_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MAX_EQUILIBRIUM_ERROR_PERCENTAGE) + private double maxEquilibriumErrorPercentage; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_FACTOR) + private double rapidSleepDecreaseFactor; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_TRANSITION_MS, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) + private double rapidSleepDecreaseTransitionPeriodMs; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_SLEEP_DECREASE_FACTOR) + private double sleepDecreaseFactor; + + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_SLEEP_INCREASE_FACTOR, + DefaultValue = DEFAULT_SLEEP_INCREASE_FACTOR) + private double sleepIncreaseFactor; Review Comment: lets add documentation of what each field means. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +293,67 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { +@Test +public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); Review Comment: why sleep is there? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -334,6 +337,21 @@ private boolean executeHttpOperation(final int retryCount, return true; } + /** + * Makes a call for client side throttling based on + * the request count. + * @param operationType operation type of current request + * @param abfsCounters AbfsCounters instance + */ + @VisibleForTesting + boolean applyThrottlingBackoff(int retryCount, AbfsRestOperationType operationType, AbfsCounters abfsCounters) { + if (retryCount == 0) { + intercept.sendingRequest(operationType, abfsCounters); + return true; + } + return false; + } + Review Comment: once test is refactored. lets keep ``` if (retryCount == 0) { intercept.sendingRequest(operationType, abfsCounters); return true; } ``` inline to line 286. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +293,67 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { +@Test +public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); Review Comment: lets not just test this method. lets test the whole executeRequest method. Have httpOperation mocked. And change the behaviour of httpOperation.processResponse.. Fail it for 2-3 times. and assert."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1451533058 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 2s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 7 new + 4 unchanged - 0 fixed = 11 total (was 4) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 56s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 11s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 35s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 107m 9s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 794084458b10 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0011b04d75d2908ea36540b8958d43d988d3d864 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/testReport/ | | Max. process+thread count | 751 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1453847787 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 9 new + 4 unchanged - 0 fixed = 13 total (was 4) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 13s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 97m 52s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux dc39661540a8 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 91c932722d3e92aa48d19a5d275e4721a29e48f6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1125941455 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -50,6 +50,14 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + // Throttling Analysis defaults. + public static final double DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE = .1; Review Comment: nit:`0.1` ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } Review Comment: what is happening in throttlingIntercept is out of scope for the pr. its not testing what the change we want to have. What its touching is that if the if-else block is working correct or not. even if we want to to test it, lets have intercept mocked and just check if `intercept.sendingRequest` is being called or not. What i want to say is that, lets not get into whats happening inside throttlingIntercept. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -50,6 +50,14 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + // Throttling Analysis defaults. + public static final double DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE = .1; Review Comment: please check other double values in default configs added. nit comment. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -334,6 +337,25 @@ private boolean executeHttpOperation(final int retryCount, return true; } + /** + * Makes a call for client side throttling based on + * the request count. + * @param operationType operation type of current request + * @param abfsCounters AbfsCounters instance + */ + @VisibleForTesting + boolean applyThrottlingBackoff(int retryCount, AbfsRestOperationType operationType, AbfsCounters abfsCounters) { + if (retryCount == 0) { + intercept.sendingRequest(operationType, abfsCounters); + return true; + } + return false; + } + + public AbfsHttpOperation createHttpOperationInstance() throws IOException { Review Comment: dont have it public. have it like: ``` AbfsHttpOperation createHttpOperationInstance() throws IOException { ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,55 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /* + Analysis Period for client-side throttling + */ @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ANALYSIS_PERIOD, DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + /* Review Comment: lets have it in javadocs? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { Review Comment: nit: lets remove this line. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); Review Comment: same comment as above. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); + Mockito.doReturn(-1) + .doReturn(-1) + .doReturn(-1) + .doReturn(HTTP_OK) + .when(mockHttpOp).getStatusCode(); + Mockito.doNothing().when(mockHttpOp).setRequestProperty(nullable(String.class), nullable(String.class)); + Mockito.doNothing().when(mockHttpOp).sendRequest(nullable(byte[].class), nullable(int.class), nullable(int.class)); + Mockito.doNothing().when(mockHttpOp).processResponse(nullable(byte[].class), nullable(int.class), nullable(int.class)); + + Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); + Mockito.doReturn(mockHttpOp).when(mockRestOp).getResult(); + + mockRestOp.execute(getTestTracingContext(fs, false)); + Mockito.verify(intercept, times(1)).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); Review Comment: since you plan to keep `applyThrottlingBackoff` in your code, let check if `applyThrottlingBackoff` is being called 4 times. it would be great addition. What you feel? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); + Mockito.doReturn(-1) + .doReturn(-1) + .doReturn(-1) + .doReturn(HTTP_OK) + .when(mockHttpOp).getStatusCode(); + Mockito.doNothing().when(mockHttpOp).setRequestProperty(nullable(String.class), nullable(String.class)); + Mockito.doNothing().when(mockHttpOp).sendRequest(nullable(byte[].class), nullable(int.class), nullable(int.class)); + Mockito.doNothing().when(mockHttpOp).processResponse(nullable(byte[].class), nullable(int.class), nullable(int.class)); + + Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); + Mockito.doReturn(mockHttpOp).when(mockRestOp).getResult(); + + mockRestOp.execute(getTestTracingContext(fs, false)); + Mockito.verify(intercept, times(1)).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + } + + @Test + public void testWriteThrottleNewRequest() throws IOException { Review Comment: this is not different than `testReadThrottleNewRequest`, just that api is changing. we can remove it, since its not adding any significance. What you feel?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1455548802 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 43s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 generated 2 new + 55 unchanged - 0 fixed = 57 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 9 new + 4 unchanged - 0 fixed = 13 total (was 4) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 37s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 96m 43s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 0f2865589c72 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b036f9285b4b2124273dda736bf44f002f7e27d1 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/testReport/ | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1126166721 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: No need to do spy. ``` AbfsHttpOperation mockHttpOp = Mockito.mock(AbfsHttpOperation.class); Mockito.doReturn(mockHttpOp).when(mockRestOp).createHttpOperationInstance(); ```"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1126325114 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: Just Mocking is leading to NPE under executeHttpOperation method. Keeping it as a spy."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1456190858 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 12 new + 7 unchanged - 0 fixed = 19 total (was 7) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 94m 44s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 6cabcb9c56bb 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 94bab6c212bb2af7792ae25595a1582f68132269 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/testReport/ | | Max. process+thread count | 603 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1127328623 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,62 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /** + * Analysis Period for client-side throttling + */ @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ANALYSIS_PERIOD, DefaultValue = DEFAULT_ANALYSIS_PERIOD_MS) private int analysisPeriod; + /** + * Lower limit of acceptable error percentage + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MIN_ACCEPTABLE_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MIN_ACCEPTABLE_ERROR_PERCENTAGE) + private double minAcceptableErrorPercentage; + + /** + * Maximum equilibrium error percentage + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_MAX_EQUILIBRIUM_ERROR_PERCENTAGE, + DefaultValue = DEFAULT_MAX_EQUILIBRIUM_ERROR_PERCENTAGE) + private double maxEquilibriumErrorPercentage; + + /** + * Rapid sleep decrease factor to increase throughput + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_FACTOR, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_FACTOR) + private double rapidSleepDecreaseFactor; + + /** + * Rapid sleep decrease transition period in milliseconds + */ + @DoubleConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_RAPID_SLEEP_DECREASE_TRANSITION_MS, + DefaultValue = DEFAULT_RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) + private double rapidSleepDecreaseTransitionPeriodMs; + + /** + * Sleep decrease factor to increase throughput + */ Review Comment: fix spacing in all javadocs wherever required. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -20,28 +20,42 @@ import static java.net.HttpURLConnection.HTTP_INTERNAL_ERROR; +import static java.net.HttpURLConnection.HTTP_OK; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_GET; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.HTTP_METHOD_PUT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MAX_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MAX_IO_RETRIES; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.AZURE_MIN_BACKOFF_INTERVAL; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_LEVEL_THROTTLING_ENABLED; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_AUTOTHROTTLING; import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MIN_BUFFER_SIZE; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.IF_MATCH; +import static org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations.RANGE; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_ABFS_ACCOUNT1_NAME; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_ACCOUNT_NAME; import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.TEST_CONFIGURATION_FILE_NAME; import static org.junit.Assume.assumeTrue; -import static org.mockito.Mockito.mock; -import static org.mockito.Mockito.when; +import static org.mockito.Mockito.*; Review Comment: fix wildcard. You can switch off wildcard imports on IDE so in future it doesnt come. ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: lets resolve to not do this. Some issue in our mocking. The way described should work. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: why is it in abfsClient. What risk is there to keep it in abfsRestOperation."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1127368834 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestExponentialRetryPolicy.java: ########## @@ -285,6 +299,154 @@ public void testAbfsConfigConstructor() throws Exception { Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff()); } +// public void testClientBackoffOnlyNewRequest() throws IOException { + @Test + public void testClientBackoffOnlyNewWriteRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = HTTP_METHOD_PUT; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.Append; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(new Long(writeThrottleStatBefore+1), writeThrottleStatAfter); + + + writeThrottleStatBefore = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + writeThrottleStatAfter = counters.toMap().get(AbfsStatistic.WRITE_THROTTLES.getStatName()); + assertEquals(writeThrottleStatBefore, writeThrottleStatAfter); + } + + @Test + public void testClientBackoffOnlyNewReadRequest() throws IOException, InterruptedException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = fs.getAbfsStore().getClient(); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + URL dummyUrl = client.createRequestUrl(\"/\", \"\"); + String dummyMethod = AbfsHttpConstants.HTTP_METHOD_GET; + + AbfsRestOperationType testOperationType = AbfsRestOperationType.ReadFile; + + AbfsRestOperation restOp = new AbfsRestOperation(testOperationType, client, dummyMethod, dummyUrl, new ArrayList<>()); + + Long readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + Thread.sleep(10000); + boolean appliedBackoff = restOp.applyThrottlingBackoff(0, testOperationType, counters); + assertEquals(true, appliedBackoff); + Long readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(new Long(readThrottleStatBefore+1), readThrottleStatAfter); + + + readThrottleStatBefore = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + appliedBackoff = restOp.applyThrottlingBackoff(1, testOperationType, counters); + assertEquals(false, appliedBackoff); + readThrottleStatAfter = counters.toMap().get(AbfsStatistic.READ_THROTTLES.getStatName()); + assertEquals(readThrottleStatBefore, readThrottleStatAfter); + } + + @Test + public void testReadThrottleNewRequest() throws IOException { + AzureBlobFileSystem fs = getFileSystem(); + AbfsClient client = Mockito.spy(fs.getAbfsStore().getClient()); + AbfsConfiguration configuration = client.getAbfsConfiguration(); + Assume.assumeTrue(configuration.isAutoThrottlingEnabled()); + AbfsCounters counters = client.getAbfsCounters(); + + AbfsThrottlingIntercept intercept = Mockito.mock(AbfsThrottlingIntercept.class); + Mockito.doNothing().when(intercept).sendingRequest(Mockito.any(AbfsRestOperationType.class), Mockito.any(AbfsCounters.class)); + Mockito.doReturn(intercept).when(client).getIntercept(); + + // setting up the spy AbfsRestOperation class for read + final List requestHeaders = client.createDefaultHeaders(); + + final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder(); + + final URL url = client.createRequestUrl(\"/dummyReadFile\", abfsUriQueryBuilder.toString()); + final AbfsRestOperation mockRestOp = Mockito.spy(new AbfsRestOperation( + AbfsRestOperationType.ReadFile, + client, + HTTP_METHOD_GET, + url, + requestHeaders)); + + // setting up mock behavior for the AbfsHttpOperation class + AbfsHttpOperation mockHttpOp = Mockito.spy(mockRestOp.createHttpOperationInstance()); Review Comment: everything is possible :). Bit of debug is required. Please merge https://github.com/sreeb-msft/hadoop/pull/1 in your pr."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1458113928 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 23s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 22s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 11 new + 7 unchanged - 0 fixed = 18 total (was 7) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 13s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/artifact/out/results-asflicense.txt) | The patch generated 1 ASF License warnings. | | | | 94m 2s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 133ddfe34150 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4c6f2ae89f22a3d525ec7b7c50ddc9b13687d638 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/testReport/ | | Max. process+thread count | 688 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1129256701 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Within AbfsRestOperation, would have to do client.getAbfsConfiguration.getShouldThrottleRetries. Would that be more preferable?"},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130357512 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Either of two is fine. if we keep in abfsClient, it will be stored account level, and we dont need to check anything till abfsClient object is alived. In abfsRestOp, new field will be created as new object is created for each api call. if we keep it in abfsRestOperation, it is something which is actually requried in abfsRestOperation. Though it doesn't matter. You may please resolve this comment."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130357920 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -268,10 +269,62 @@ DefaultValue = DEFAULT_ACCOUNT_OPERATION_IDLE_TIMEOUT_MS) private int accountOperationIdleTimeout; + /** + * Analysis Period for client-side throttling + */ Review Comment: nit: spacing."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5446: URL: https://github.com/apache/hadoop/pull/5446#discussion_r1130546200 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java: ########## @@ -222,6 +224,10 @@ AbfsThrottlingIntercept getIntercept() { return intercept; } + boolean shouldThrottleRetries() { + return throttleRetries; + } + Review Comment: Right. That makes sense. We can directly invoke it from within AbfsRestOperation."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1461580972 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 50s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 54s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux bc0f9d906cc6 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9bea2eff81d5f897a138d59cb5bb0575e4f6ade6 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/testReport/ | | Max. process+thread count | 705 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462216813 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 53s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 12s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 38s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 7 unchanged - 0 fixed = 8 total (was 7) | | +1 :green_heart: | mvnsite | 0m 36s | | the patch passed | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 15s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 3m 12s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/8/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 97m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5446 | | JIRA Issue | HADOOP-18640 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 137e902f552a 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9c5e993d71f861abc6145cd9d2ca1a78343c9474 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/8/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5446/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462328124 it seems yetus is giving wrong line number. there should be a space on 839."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-1462348876 error was due to space on empty line: try merging https://github.com/sreeb-msft/hadoop/pull/2."},{"author":"Shilun Fan","body":"Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5446: URL: https://github.com/apache/hadoop/pull/5446#issuecomment-3445309605 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5446: HADOOP-18640: [ABFS] Enabling Client-side Backoff only for new requests URL: https://github.com/apache/hadoop/pull/5446"}]}
{"key":"HADOOP-18639","summary":"DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed.","description":"YARN NodeManager's deletion service has two types of deletion tasks: the FileDeletionTask for deleting log, usercache, appcache files and the DockerContainerDeletionTask for deleting Docker containers. The FileDeletionTask is removed from the statestore when the task is completed, but the DockerContainerDeletionTask is not. Therefore, the DockerContainerDeletionTask accumulates continuously in the statestore. This causes the NodeManager's deletion service to run the accumulated DockerContainerDeletionTask in the statestore when the NodeManager restarts. As a result, the FileDeletionTask and DockerContainerDeletionTask are delayed unnecessarily while processing accumulated tasks, which can cause disk full issues in environments where a large number of containers are allocated and released. I will attach a patch soon","status":"Open","priority":"Major","reporter":"Sejin Hwang","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-22T13:43:55.000+0000","updated":"2025-10-26T00:24:19.000+0000","comments":[{"author":"ASF GitHub Bot","body":"Sejin-Hwang opened a new pull request, #5425: URL: https://github.com/apache/hadoop/pull/5425 ### Description of PR When the DockerContainerDeletionTask is completed, it should be deleted from the NodeManager's statestore, just like the FileDeletionTask. JIRA : https://issues.apache.org/jira/browse/HADOOP-18639 ### How was this patch tested? Unit test added"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-1440331866 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 53s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 19s | | trunk passed | | +1 :green_heart: | compile | 1m 31s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 1m 27s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 37s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 34s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 1m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 1m 25s | | the patch passed | | +1 :green_heart: | compile | 1m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 1m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 23s | [/results-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/1/artifact/out/results-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) | hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager: The patch generated 2 new + 7 unchanged - 0 fixed = 9 total (was 7) | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 35s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 31s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | asflicense | 0m 34s | | The patch does not generate ASF License warnings. | | | | 139m 32s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5425 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 71ed66f96066 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ab08810d2c5e73b2a362d9007e83b1a4ec7e33f8 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/1/testReport/ | | Max. process+thread count | 606 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-1440644469 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 32s | | trunk passed | | +1 :green_heart: | compile | 1m 30s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 1m 24s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | +1 :green_heart: | javadoc | 0m 44s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 35s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 42s | | the patch passed | | +1 :green_heart: | compile | 1m 24s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 1m 24s | | the patch passed | | +1 :green_heart: | compile | 1m 17s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 1m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 23s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 37s | | the patch passed | | +1 :green_heart: | javadoc | 0m 30s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 30s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 23m 30s | | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 139m 13s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5425 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 4cd6557679cc 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b9662f3160da82f744a48ac298eb0e68d1a57a33 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/2/testReport/ | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5425/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5425: URL: https://github.com/apache/hadoop/pull/5425#issuecomment-3445309935 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5425: HADOOP-18639 DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed URL: https://github.com/apache/hadoop/pull/5425"}]}
{"key":"HADOOP-19666","summary":"Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension","description":"This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.","status":"Open","priority":"Major","reporter":"Lei Wen","labels":["pull-request-available"],"project":"HADOOP","created":"2025-08-27T03:08:41.000+0000","updated":"2025-10-26T05:02:39.000+0000","comments":[{"author":"ASF GitHub Bot","body":"leiwen2025 opened a new pull request, #7912: URL: https://github.com/apache/hadoop/pull/7912 This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation. ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 30s | | trunk passed | | +1 :green_heart: | compile | 7m 30s | | trunk passed | | -1 :x: | mvnsite | 1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 56m 43s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 7m 3s | | the patch passed | | +1 :green_heart: | cc | 7m 3s | | the patch passed | | +1 :green_heart: | golang | 7m 3s | | the patch passed | | +1 :green_heart: | javac | 7m 3s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-eol.txt) | The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -1 :x: | blanks | 0m 0s | [/blanks-tabs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-tabs.txt) | The patch 2 line(s) with tabs. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 22m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 44s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 123m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/testReport/ | | Max. process+thread count | 1262 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3233497529 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 26s | | trunk passed | | +1 :green_heart: | compile | 7m 37s | | trunk passed | | -1 :x: | mvnsite | 1m 30s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in trunk failed. | | +1 :green_heart: | shadedclient | 57m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 6m 57s | | the patch passed | | +1 :green_heart: | cc | 6m 57s | | the patch passed | | +1 :green_heart: | golang | 6m 57s | | the patch passed | | +1 :green_heart: | javac | 6m 57s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch failed. | | +1 :green_heart: | shadedclient | 23m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 19m 38s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 20s | | The patch does not generate ASF License warnings. | | | | 111m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7912 | | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang | | uname | Linux 9af82b84cde2 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / be2c9c5dfefc7f4ad7591c58f7857b177bef5b0e | | Default Java | Red Hat, Inc.-1.8.0_312-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/testReport/ | | Max. process+thread count | 3150 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/console | | versions | git=2.27.0 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3409341544 @steveloughran @ayushtkn Hi, this PR has been open for a while. Could you please take a look when you have time? Thanks!"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3416334594 It's been a long time since I did C code, I'll have to stare at this a while. In #7903 that creation of the file bulk_crc32_riscv.c should be what the new code goes into; work with @PeterPtroc to get something you are both happy with *and tested*. Having two people who are set up to build and test this on riscv hardware is exactly what we need to get this done"},{"author":"Steve Loughran","body":"how does this differ from HADOOP-19655? I think that PR got there just before this one, so takes precedence number-wise; I'm not in a position to evaluate the code. Can you and [~peterptroc] collaborate on this? I'll give you both credit in the patches"},{"author":"ASF GitHub Bot","body":"leiwen2025 commented on PR #7912: URL: https://github.com/apache/hadoop/pull/7912#issuecomment-3448014826 @steveloughran Thanks for the review! I'll be happy to work with @PeterPtroc on this. If someone in the community could help connect me with Peter, that'd be much appreciated. I'd like to coordinate testing and integration on RISC-V."},{"author":"Lei Wen","body":"Thanks! This patch implements CRC32 on RISC-V using Zvbc vector instructions, while HADOOP-19655 uses Zbc scalar ones. Happy to work together with [~peterptroc] to coordinate and test both implementations."}]}
{"key":"HADOOP-19599","summary":"Fix file permission errors as per the platform","description":"The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly. However, an error message in the same scenario on Windows ends with \"(Access is denied)\" error. This results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*. Thus, we need to make the appropriate check in accordance with the platform.","status":"Open","priority":"Major","reporter":"Gautham Banasandra","assignee":"Gautham Banasandra","labels":["pull-request-available"],"project":"HADOOP","created":"2025-06-28T17:39:22.000+0000","updated":"2025-10-27T00:24:58.000+0000","comments":[{"author":"ASF GitHub Bot","body":"slfan1989 commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3081984805 @GauthamBanasandra Thank you for your contribution  I believe this PR is ready to be merged."},{"author":"ASF GitHub Bot","body":"GauthamBanasandra commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3082098761 Yes @slfan1989 . It's ready. But since this PR changes a source file, I'm trying to get the Yetus to run locally on this one, so that we run all the tests. The Jenkins build agents for Windows are failing due to low disk space. Hence I'm working on getting Yetus to run locally. I should be able to merge this by next week."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3447890882 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #7767: HADOOP-19599. Fix file permission errors as per the platform URL: https://github.com/apache/hadoop/pull/7767"}]}
{"key":"HADOOP-18635","summary":"Expose distcp counters to user via config parameter and distcp contants","description":"Currently users or application such as Hive cannot access directly the distcp counters such as total number of bytes copied by distcp operation. This Jira is to enable this functionality in distcp tool.","status":"Open","priority":"Major","reporter":"Amit Saonerkar","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-15T14:19:19.000+0000","updated":"2025-10-27T00:25:02.000+0000","comments":[{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1431585282 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 58s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) | hadoop-tools/hadoop-distcp: The patch generated 3 new + 18 unchanged - 0 fixed = 21 total (was 18) | | +1 :green_heart: | mvnsite | 0m 25s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 15m 58s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 117m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1c80ac3f44b1 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a3dcbd611b0d06f06431e4128c96bcfacf9c3866 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/testReport/ | | Max. process+thread count | 704 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1432634125 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) | hadoop-tools/hadoop-distcp: The patch generated 5 new + 29 unchanged - 0 fixed = 34 total (was 29) | | +1 :green_heart: | mvnsite | 0m 26s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 57s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 43s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 15m 21s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/2/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 38s | | The patch does not generate ASF License warnings. | | | | 120m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6e382f48f842 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2bce2f6885de6486f50e21fab63479fb77308225 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/2/testReport/ | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1434208005 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 5s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 42m 15s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/3/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 37s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 16s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 26s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 53s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 34s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 16m 0s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/3/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 119m 28s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ac77e5e911bb 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3b4574246a6cbf9d3f04c7a15aa1fd5f7f7fe61c | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/3/testReport/ | | Max. process+thread count | 572 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5402: URL: https://github.com/apache/hadoop/pull/5402#discussion_r1109612693 ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -452,7 +454,12 @@ public void testDistcpLargeFile() throws Exception { }; LOG.info(\"_____ running distcp: \" + args[0] + \" \" + args[1]); - ToolRunner.run(conf, new DistCp(), args); + DistCp distcpTool = new DistCp(); + ToolRunner.run(conf, distcpTool, args); + final long bytesCopied = NumberUtils.toLong(distcpTool.getConf(). + get(CONF_LABEL_DISTCP_TOTAL_BYTES_COPIED), 0); + assertEquals(\"Bytes copied by distcp tool should match source file length\", Review Comment: assert args are the wrong way round for the generated error messages."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1441367924 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 5s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 17s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 30s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 49s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 14m 52s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 123m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a45725ac2cee 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 15669c18810037abf5204f0fd868e23d253532c8 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/4/testReport/ | | Max. process+thread count | 585 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1443788914 I really don't like how the results come back. I'm going to propose adding IOStatistics support to distcp so lined up for future work and to not modify the source config to suddenly become two way exchange of data 1. DistCp to implement IOStatisticsSource 1. until job finishes, getIOStatistics() to return null 3. when job finished, ``` // to create a builder IOStatisticsStore iostats = IOStatisticsBinding.iostatisticsStore() .withCounter(DISTCP_TOTAL_BYTES_COPIED) .build() // then set the counter to the retrieved value iostats.setCounter(DISTCP_TOTAL_BYTES_COPIED, ) ``` This is extra work and you have to learn a new api, but * IOStatisticsAssertions has the asserts * IOStatisticsLogging has pretty printing * you can take an IOStatisticsSnapshot and send over the wire as json or java serialized object * lines it up perfectly for us collecting more detailed stats, not just from the workers (trickier...) but also cost of directory scanning, cleanup etc."},{"author":"Shilun Fan","body":"I deleted 3.4.0 from the fix version. When PR is merged, we will set the fix version again."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-3447890992 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5402: HADOOP-18635 : Expose distcp counters to user via new DistCpConstants \"CONF_LABEL_DI URL: https://github.com/apache/hadoop/pull/5402"}]}
{"key":"HADOOP-18632","summary":"ABFS: Customize and optimize timeouts made based on each separate request","description":"In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success.","status":"Open","priority":"Major","reporter":"Sree Bhattacharyya","assignee":"Sree Bhattacharyya","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-15T09:21:31.000+0000","updated":"2025-10-27T00:25:03.000+0000","comments":[{"author":"ASF GitHub Bot","body":"sreeb-msft opened a new pull request, #5399: URL: https://github.com/apache/hadoop/pull/5399 ### Description of PR In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431009246 ---"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431015489 ------------------------ :::: AGGREGATED TEST RESULT :::: HNS-OAuth ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:329  TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected: but was: [INFO] [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41 HNS-SharedKey ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:334  TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [WARNING] Tests run: 336, Failures: 0, Errors: 0, Skipped: 41 NonHNS-SharedKey ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAzureBlobFileSystemRandomRead.testValidateSeekBounds:269->Assert.assertTrue:42->Assert.fail:89 There should not be any network I/O (elapsedTimeMs=526). [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:336  TestTimedOut test timed o... [INFO] [ERROR] Tests run: 569, Failures: 1, Errors: 1, Skipped: 278 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsTerasort.test_110_teragen:244->executeStage:211->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 teragen(1000, abfs://testcontainer@sreebcitestnonhns.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected: but was: [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected: but was: [ERROR] Errors: [ERROR] ITestAbfsJobThroughManifestCommitter.test_0420_validateJob  OutputValidation ... [ERROR] ITestAbfsManifestCommitProtocol.testCommitLifecycle  OutputValidation `abfs:/... [ERROR] ITestAbfsManifestCommitProtocol.testCommitterWithDuplicatedCommit  OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testConcurrentCommitTaskWithSubDir  OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testMapFileOutputCommitter  OutputValidation ... [ERROR] ITestAbfsManifestCommitProtocol.testOutputFormatIntegration  OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testParallelJobsToAdjacentPaths  OutputValidation [ERROR] ITestAbfsManifestCommitProtocol.testTwoTaskAttemptsCommit  OutputValidation `... [INFO] [ERROR] Tests run: 336, Failures: 2, Errors: 8, Skipped: 46 AppendBlob-HNS-OAuth ======================== [INFO] Results: [INFO] [ERROR] Failures: [ERROR] TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" [INFO] [ERROR] Tests run: 111, Failures: 1, Errors: 0, Skipped: 4 [INFO] Results: [INFO] [ERROR] Errors: [ERROR] ITestAzureBlobFileSystemLease.testAcquireRetry:344->lambda$testAcquireRetry$6:345  TestTimedOut [INFO] [ERROR] Tests run: 569, Failures: 0, Errors: 1, Skipped: 54 [INFO] Results: [INFO] [ERROR] Failures: [ERROR] ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip:919->AbstractContractDistCpTest.verifySkipAndCopyCounter:1000->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in COPY counter value expected: but was: [INFO] [ERROR] Tests run: 336, Failures: 1, Errors: 0, Skipped: 41 Time taken: 39 mins 34 secs."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1431176861 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 40s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 19s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 14 new + 55 unchanged - 0 fixed = 69 total (was 55) | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 37s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/results-asflicense.txt) | The patch generated 3 ASF License warnings. | | | | 105m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle | | uname | Linux 379ced44d885 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c9e53a79ec634a3f9a496425cafe215625784690 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/testReport/ | | Max. process+thread count | 563 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1107015958 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: Lets add it inside else block above. Reason being, if block is always having this key false. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: should RuntimeException be thrown here? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsCustomTimeout.java: ########## @@ -0,0 +1,155 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest; +import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.junit.Test; + +import java.io.IOException; +import java.net.URL; +import java.util.HashMap; +import java.util.Map; + + +public class ITestAbfsCustomTimeout extends AbstractAbfsIntegrationTest { + + private boolean optimizeTimeout; + private int maxRequestTimeout; + private int requestTimeoutIncRate; + private HashMap opMap = new HashMap(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeRequestTimeout(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getConfiguration(); + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"true\"); + abfsConfig.set(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT, \"90\"); + abfsConfig.set(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE, \"2\"); + optimizeTimeout =true; + maxRequestTimeout = 90; + requestTimeoutIncRate = 2; + AbfsConfiguration newConfig = new AbfsConfiguration(abfsConfig.getRawConfiguration(), getAccountName()); + + for (Map.Entry it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + config = ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + config = ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + config = ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + config = ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.ListPaths) { + config = ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.CreatePath) { + config = ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.RenamePath) { + config = ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetAcl) { + config = ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + config = ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + config = ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetAcl) { + config = ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetOwner) { + config = ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.SetPermissions) { + config = ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.CheckAccess) { + config = ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT; + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + config = ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT; + } + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, newConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + public void testInitTimeoutOptimizer(AbfsRestOperationType opType, int maxRetryCount, int expectedReqTimeout, AbfsConfiguration abfsConfig) throws IOException { + + AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + AbfsClient client = fs.getAbfsStore().getClient(); + String query = client.createDefaultUriQueryBuilder().toString(); + URL url = client.createRequestUrl(\"/testPath\", query); + TimeoutOptimizer opt = new TimeoutOptimizer(url, opType, client.getRetryPolicy(), getConfiguration()); + int retryCount = 0; + while (retryCount 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: readTimeout and connTimeout are in ms, requestTimeout is in second. so if requestTimeout is 90 it will set readTimeout as 90 which JDK understands 90 MILLISECONDS and NOT seconds. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: requestTimeout == -1? What would be the implications. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Lets have a NPE check. Agree that abfsclient adds the param. But if in future, some other class wants to use this class. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.DeleteFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ListPaths) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CreatePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.RenamePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPathProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetAcl) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetOwner) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetPermissions) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Append) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.CheckAccess) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetPathStatus) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.Flush) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.ReadFile) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.LeasePath) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + } + if (timeout == null) { Review Comment: timeout == null || timeout.isEmpty() ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Also, what if we have a field in AbfsRestOperationType enum containing the config name?"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1432508287 Please add the test class in https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L601-L608 and https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/pom.xml#L644-L652, else it will break the runTest script runs."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108011727 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -276,14 +280,15 @@ public AbfsHttpOperation(final URL url, final String method, final List<AbfsHttp } } - this.connection.setConnectTimeout(CONNECT_TIMEOUT); - this.connection.setReadTimeout(READ_TIMEOUT); + this.connection.setConnectTimeout(timeoutOptimizer.getConnTimeout(CONNECT_TIMEOUT)); + this.connection.setReadTimeout(timeoutOptimizer.getReadTimeout(READ_TIMEOUT)); this.connection.setRequestMethod(method); for (AbfsHttpHeader header : requestHeaders) { this.connection.setRequestProperty(header.getName(), header.getValue()); } + Review Comment: Can we remove the extra lines, makes it difficult to backport"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108012054 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -555,6 +560,7 @@ public static class AbfsHttpOperationWithFixedResult extends AbfsHttpOperation { public AbfsHttpOperationWithFixedResult(final URL url, final String method, final int httpStatus) { + Review Comment: Remove extra line."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +125,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: URL can be made to final in timeoutoptimizer also"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108031419 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { Review Comment: Add javadoc for the class and comments."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); Review Comment: should we add a null check here as well ?"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108030121 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +125,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: URL can be made to final in timeoutoptimizer also"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108034619 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); Review Comment: should we add a null check here as well or should we have default values for this as we are taking dependency on some config ? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); Review Comment: Same as above."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108035699 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } Review Comment: Line break."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037262 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { Review Comment: Since we are taking dependency on configs everywhere we should add a NP check or add default value for each config."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108037646 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: should we throw back the exception in catch block ?"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048351 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: Are you suggesting moving just the if block code to the above else block? Or including the if check and the following code in the block together in the above else block?"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108048702 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Can try to have an enum with the AbfsRestOperationType and corresponding ConfigurationKey"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108049244 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: Had kept this empty because an already formed URL will be used (just modifying one query parameter), but can throw RuntimeException over here."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108070379 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: Thanks for pointing this out. Have made the necessary changes."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108079007 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: Setting request timeout (and all other timeouts) to -1 can be thought of as a flag value that is being used. Although the value for request timeout does not get checked, the other timeout values get checked (getReadTimeout and getConnTimeout calls). So to keep with the other timeouts initializations this is also set to -1. Would you suggest changing this in any way?"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108081097 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); Review Comment: Added the change."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1108083553 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Are you referring to a check on the url.getQuery() or the timeout parameter itself?"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109277491 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: i understand that url will always be correct on which we would do manipulation, but still better to throw RuntimeException. maybe in future, some other class wants to use this timeoutOptimizer, it can use it. This class should be agnostic to whatever flowing in."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109278671 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: what does server understands from requestTimeout == -1?"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109279454 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } Review Comment: ``` if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { this.shouldOptimizeTimeout = false; } else { this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); if (this.shouldOptimizeTimeout) { this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get( ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get( ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); initTimeouts(); updateUrl(); } } ```"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109283269 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: We understand that the request can take maximum of (connTimeout + requestTimeout) and JDK will not throw any issue? Feel that connTimeout can be kept more aggressive. Why is readTimeout, connTimeout a function of requestTimeout? Feel that diff types of exception should change diff parameters. CC: @snvijaya @anmolanmol1234"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109284406 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Suggestion: lets have timeoutPos as Integer and not primitive. Then: `if(timeoutPos != null && timeoutPos < 0)`"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109316607 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: ReadTimeout can be a function of requestTimeout, but connectionTimeout is totally independent to requestTimeout. Until connection is not established, there is no significance of requestTimeout as server would not have started processing."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { Review Comment: This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient. ex: 10th Read call should use the intellegence it achieved in the first Read call. Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object. CC: @snvijaya @anmolanmol1234"},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1109318466 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { Review Comment: This I understand is per-call level. Should we use the feedback loop across the lifetime of abfsClient. ex: 10th Read call should use the intellegence it achieved in the first Read call. Also, since we say that we will start from aggressive value, the retries would be same across all API calls in the lifetime of AbfsClient object. ex: lets take out connTimeout is aggresive say 100 ms. And network is not able to connect in that time. And through our heuristic lets say on 10th retry, connTimeout become 1 sec which is good with network. Now all API call would have to fail nearly 10 times to work on that network CC: @snvijaya @anmolanmol1234"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1111514936 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; + } + } + + private void updateUrl() { + // updates URL with existing request timeout value + URL updatedUrl = null; + try { + URIBuilder uriBuilder = new URIBuilder(url.toURI()); + uriBuilder.setParameter(HttpQueryParams.QUERY_PARAM_TIMEOUT, Integer.toString(requestTimeout)); + updatedUrl = uriBuilder.build().toURL(); + } catch (URISyntaxException e) { + + } catch (MalformedURLException e) { + + } Review Comment: Have added RuntimeException for both the catch blocks in TimeoutOptimizer class."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1111563391 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.GetFileSystemProperties) { + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + } + else if (opType == AbfsRestOperationType.SetFileSystemProperties) { Review Comment: Null Pointer Check is added at the end, in case any of the configs are not set. At present in case any of the configs are not set, manually setting this timeout value to the default of 90. Setting a default value for the config can also be considered."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1436762577 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 41s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 8s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 23m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/results-asflicense.txt) | The patch generated 3 ASF License warnings. | | | | 105m 0s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 137] | | | Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing At TimeoutOptimizer.java:where default case is missing At TimeoutOptimizer.java:[lines 149-205] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 5f0464b0b1b3 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 10fd6c2ac7054d7e306286dc18e1824e26c3da47 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/testReport/ | | Max. process+thread count | 711 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1112771893 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { Review Comment: Understood, thanks!"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1439558099 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 41s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 29s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 31s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | -1 :x: | mvnsite | 0m 30s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 24s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 0m 29s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 26m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 35s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/results-asflicense.txt) | The patch generated 3 ASF License warnings. | | | | 105m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux e74be7b85008 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0761741b5e6964ee63964d308bdd596c69eecf52 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/testReport/ | | Max. process+thread count | 662 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115313395 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; Review Comment: This value of the request timeout would never reach the server. This is just used as a flag value. In the case where request timeout / read timeout stays set as -1, the optimizer can be considered to not be doing any work (ie, when either the config for whether optimization should happen or not is set to false, or when the request in consideration does not have a timeout value in its query parameters, like create Filesystem, or when any of the configs are incorrectly set)."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115320710 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos 0) { + // retry count is still valid + // timeout increment rate is a valid value + if ((requestTimeout * timeoutIncRate) > maxReqTimeout) { + requestTimeout = maxReqTimeout; + } else { + requestTimeout *= timeoutIncRate; + } + readTimeout = requestTimeout; + connTimeout = requestTimeout - 1; Review Comment: Made the necessary changes."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1115418467 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + public TimeoutOptimizer getTimeoutOptimizer() { Review Comment: seems unused. Lets remove it. In case needed in test, you may have package-protected access and have @visibleForTesting annotation. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -94,6 +95,8 @@ public URL getUrl() { return url; } + public TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } Review Comment: lets have package-protected access and @visibleForTesting annotation. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { Review Comment: initTimeouts can happen only if shouldOptimizeTimeout is true. Lets remove this if-block? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos opMap = new HashMap(); + private HashMap opTimeoutConfigMap = new HashMap(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeConfigs(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + + for (Map.Entry it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + /** + * Test to verify working of timeout optimization with AbfsRestOperation execute calls + * Currently tests only for a single API + * @throws IOException + * @throws IllegalAccessException + */ + @Test + public void testOptimizationInRestCall() throws IOException, IllegalAccessException { + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + for (Map.Entry it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + AbfsRestOperation op = getMockAbfsRestOp(opType, newFs); + final int[] finalTimeout = {timeout}; + Mockito.doAnswer(new Answer() { + int requestCount = 4; Review Comment: Lets keep it outside Mockito.doAnswer. something like: ``` int[] request = new int[1]; request[0]=4; ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos opMap = new HashMap(); + private HashMap opTimeoutConfigMap = new HashMap(); + + public ITestAbfsCustomTimeout() throws Exception { + super(); + initOpTypeConfigs(); + } + + @Test + public void testOptimizer() throws IOException, IllegalAccessException { + + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + + for (Map.Entry it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + testInitTimeoutOptimizer(opType, 3, timeout, abfsConfig); + abfsConfig.unset(config); + } + + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + + } + + /** + * Test to verify working of timeout optimization with AbfsRestOperation execute calls + * Currently tests only for a single API + * @throws IOException + * @throws IllegalAccessException + */ + @Test + public void testOptimizationInRestCall() throws IOException, IllegalAccessException { + AbfsConfiguration abfsConfig = getModifiedTestConfig(); + AzureBlobFileSystem newFs = (AzureBlobFileSystem) FileSystem.newInstance(abfsConfig.getRawConfiguration()); + for (Map.Entry it : opMap.entrySet()) { + AbfsRestOperationType opType = it.getKey(); + int timeout = it.getValue(); + String config = opTimeoutConfigMap.get(opType); + abfsConfig.set(config, Integer.toString(timeout)); + AbfsRestOperation op = getMockAbfsRestOp(opType, newFs); + final int[] finalTimeout = {timeout}; + Mockito.doAnswer(new Answer() { + int requestCount = 4; + + public Object answer(InvocationOnMock invocation) { + if (requestCount > 0) { + requestCount--; + assertEquals(finalTimeout[0], op.getTimeoutOptimizer().getRequestTimeout()); + if (finalTimeout[0] * requestTimeoutIncRate > maxRequestTimeout) { + finalTimeout[0] = maxRequestTimeout; + } else { + finalTimeout[0] *= requestTimeoutIncRate; + } + } + return op.getResult(); + } + }).when(op).createHttpOperationInstance(); + op.execute(getTestTracingContext(newFs, true)); + abfsConfig.unset(config); + } + abfsConfig.set(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS, \"false\"); + } + + private AbfsRestOperation getMockAbfsRestOp(AbfsRestOperationType opType, AzureBlobFileSystem fs) throws IOException { + + AbfsClient spyClient = Mockito.spy(getAbfsClient(fs.getAbfsStore())); + + // creating the parameters (Url and request headers) to initialize AbfsRestOperation + AbfsUriQueryBuilder queryBuilder = spyClient.createDefaultUriQueryBuilder(); + URL url = spyClient.createRequestUrl(\"/\", queryBuilder.toString()); + + AbfsRestOperation spyRestOp = Mockito.spy(new AbfsRestOperation(opType, spyClient, HTTP_METHOD_HEAD, url, new ArrayList<>())); + + AbfsHttpOperation mockHttpOp = Mockito.spy(spyRestOp.createHttpOperationInstance()); + Mockito.doAnswer(new Answer() { + private int count = 0; Review Comment: let keep count outside mockito."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1441540148 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 1s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 17s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 48s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 13s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 38s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 33s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 10 new + 2 unchanged - 0 fixed = 12 total (was 2) | | +1 :green_heart: | mvnsite | 0m 32s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 6s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | shadedclient | 23m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/results-asflicense.txt) | The patch generated 4 ASF License warnings. | | | | 105m 5s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 132] | | | Switch statement found in org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() where default case is missing At TimeoutOptimizer.java:where default case is missing At TimeoutOptimizer.java:[lines 143-199] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 8525889b434d 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 75c6d196ae6cdac9d45872cabe034648bee7342d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/testReport/ | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116842887 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well Review Comment: Not added in cases like createFilesystem"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116849374 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; Review Comment: updated this accordingly, removing any init to -1."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116879406 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +168,10 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + public TimeoutOptimizer getTimeoutOptimizer() { Review Comment: Keeping it for any test cases that might be added in the future. Making package protected."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1116885652 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { Review Comment: Updated!"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1443690838 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 42s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 16s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 17s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2) | | -1 :x: | mvnsite | 0m 18s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 17s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | spotbugs | 0m 18s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 22m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 21s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/results-asflicense.txt) | The patch generated 4 ASF License warnings. | | | | 89m 58s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 56921c473dfe 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a00f099b26b7592853c26deabbfb237994c10766 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/testReport/ | | Max. process+thread count | 597 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118282237 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java: ########## @@ -167,6 +169,11 @@ public String getResponseHeader(String httpHeader) { return connection.getHeaderField(httpHeader); } + @VisibleForTesting + protected TimeoutOptimizer getTimeoutOptimizer() { Review Comment: We have it package-protected: ``` TimeoutOptimizer getTimeoutOptimizer() ``` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -94,6 +96,9 @@ public URL getUrl() { return url; } + @VisibleForTesting + protected TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } Review Comment: we have it package-protected: ``` TimeoutOptimizer getTimeoutOptimizer() { return timeoutOptimizer; } ```"},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118295742 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,227 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +public class TimeoutOptimizer { + AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int connTimeout = -1; + private int maxReqTimeout; + private int timeoutIncRate; + private boolean shouldOptimizeTimeout; + + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + if (abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS) == null) { + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS)); + } + if (this.shouldOptimizeTimeout) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + initTimeouts(); + updateUrl(); + } + + } else { + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { return this.shouldOptimizeTimeout; } + + public int getRequestTimeout() { return requestTimeout; } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + public int getConnTimeout() { + return connTimeout; + } + + public int getConnTimeout(final int defaultTimeout) { + if (connTimeout == -1) { + return defaultTimeout; + } + return connTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + return; + } + + String query = url.getQuery(); + int timeoutPos = query.indexOf(\"timeout\"); + if (timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + connTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + if (opType == AbfsRestOperationType.CreateFileSystem) { Review Comment: Resolving for future addition."},{"author":"ASF GitHub Bot","body":"sreeb-msft commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118303110 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,244 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.fs.azurebfs.contracts.exceptions.InvalidConfigurationValueException; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + if (!shouldOptimizeTimeout) { + requestTimeout = -1; + readTimeout = -1; + return; + } + + String query = url.getQuery(); + Integer timeoutPos = new Integer(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + requestTimeout = -1; + readTimeout = -1; + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + switch(opType) { + case CreateFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_FS_REQUEST_TIMEOUT); + break; + case GetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_FS_REQUEST_TIMEOUT); + break; + case SetFileSystemProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_FS_REQUEST_TIMEOUT); + break; + case DeleteFileSystem: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_DELETE_FS_REQUEST_TIMEOUT); + break; + case ListPaths: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LIST_PATH_REQUEST_TIMEOUT); + break; + case CreatePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CREATE_PATH_REQUEST_TIMEOUT); + break; + case RenamePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_RENAME_PATH_REQUEST_TIMEOUT); + break; + case GetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_ACL_REQUEST_TIMEOUT); + break; + case GetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetPathProperties: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PATH_PROPERTIES_REQUEST_TIMEOUT); + break; + case SetAcl: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_ACL_REQUEST_TIMEOUT); + break; + case SetOwner: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_OWNER_REQUEST_TIMEOUT); + break; + case SetPermissions: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_SET_PERMISSIONS_REQUEST_TIMEOUT); + break; + case Append: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_APPEND_REQUEST_TIMEOUT); + break; + case CheckAccess: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_CHECK_ACCESS_REQUEST_TIMEOUT); + break; + case GetPathStatus: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_GET_PATH_STATUS_REQUEST_TIMEOUT); + break; + case Flush: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_FLUSH_REQUEST_TIMEOUT); + break; + case ReadFile: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_READFILE_REQUEST_TIMEOUT); + break; + case LeasePath: + timeout = abfsConfiguration.get(ConfigurationKeys.AZURE_LEASE_PATH_REQUEST_TIMEOUT); + break; + } + if (timeout == null || timeout.isEmpty()) { + // if any of the timeout values are not set + // despite optimize config set to true + timeout = DEFAULT_TIMEOUT; + } + requestTimeout = Integer.parseInt(timeout); Review Comment: Added a check for this."},{"author":"ASF GitHub Bot","body":"saxenapranav commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1118348558 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -134,7 +140,7 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: same comment for final. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java: ########## @@ -117,9 +122,10 @@ String getSasToken() { AbfsRestOperation(final AbfsRestOperationType operationType, final AbfsClient client, final String method, - final URL url, + URL url, Review Comment: it can be left final. Any reason to remove it?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1445810167 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 4s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 39s | | trunk passed | | -1 :x: | javadoc | 0m 43s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 18s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 17s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 15s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 12 new + 2 unchanged - 0 fixed = 14 total (was 2) | | -1 :x: | mvnsite | 0m 17s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 15s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | spotbugs | 0m 17s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 23m 30s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 19s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | asflicense | 0m 35s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/results-asflicense.txt) | The patch generated 4 ASF License warnings. | | | | 92m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux d974df9dd342 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6ded5e25c369a53787cbdf2950a5d751d0921842 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/testReport/ | | Max. process+thread count | 761 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1446163972 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 43s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 41s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | -1 :x: | javac | 0m 34s | [/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-compile-javac-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 generated 15 new + 55 unchanged - 0 fixed = 70 total (was 55) | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javadoc | 0m 23s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08 with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 generated 1 new + 15 unchanged - 0 fixed = 16 total (was 15) | | -1 :x: | spotbugs | 1m 9s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 21m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 36s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/results-asflicense.txt) | The patch generated 4 ASF License warnings. | | | | 94m 7s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.services.TimeoutOptimizer.initTimeouts() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:constructor; use Integer.valueOf(int) instead At TimeoutOptimizer.java:[line 126] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux c6eee2203c75 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 9390100e60530e7c19139900d27b9d22d0d65b66 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/testReport/ | | Max. process+thread count | 664 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1451568775 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 38m 4s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 43s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 35s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 21s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 56s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 31s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 31s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2) | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 25s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | -1 :x: | asflicense | 0m 39s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/results-asflicense.txt) | The patch generated 4 ASF License warnings. | | | | 105m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5399 | | Optional Tests | dupname asflicense codespell detsecrets compile javac javadoc mvninstall mvnsite unit shadedclient xmllint spotbugs checkstyle | | uname | Linux 76f35de8c855 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 08000d1a7f2db671247d677ebed9be4f0f364c07 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/testReport/ | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5399/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5399: URL: https://github.com/apache/hadoop/pull/5399#discussion_r1134213829 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/TimeoutOptimizer.java: ########## @@ -0,0 +1,243 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs.services; + +import org.apache.hadoop.fs.azurebfs.AbfsConfiguration; +import org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys; +import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams; +import org.apache.hadoop.util.Preconditions; +import org.apache.http.client.utils.URIBuilder; + +import java.net.MalformedURLException; +import java.net.URISyntaxException; +import java.net.URL; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DEFAULT_TIMEOUT; + +/** + * Class handling whether timeout values should be optimized. + * Timeout values optimized per request level, + * based on configs in the settings. + */ +public class TimeoutOptimizer { + private AbfsConfiguration abfsConfiguration; + private URL url; + private AbfsRestOperationType opType; + private ExponentialRetryPolicy retryPolicy; + private int requestTimeout; + private int readTimeout = -1; + private int maxReqTimeout = -1; + private int timeoutIncRate = -1; + private boolean shouldOptimizeTimeout; + + /** + * Constructor to initialize the parameters in class, + * depending upon what is configured in the settings. + * @param url request URL + * @param opType operation type + * @param retryPolicy retry policy set for this instance of AbfsClient + * @param abfsConfiguration current configuration + */ + public TimeoutOptimizer(URL url, AbfsRestOperationType opType, ExponentialRetryPolicy retryPolicy, AbfsConfiguration abfsConfiguration) { + this.url = url; + this.opType = opType; + if (opType != null) { + this.retryPolicy = retryPolicy; + this.abfsConfiguration = abfsConfiguration; + String shouldOptimize = abfsConfiguration.get(ConfigurationKeys.AZURE_OPTIMIZE_TIMEOUTS); + if (shouldOptimize == null || shouldOptimize.isEmpty()) { + // config is not set + this.shouldOptimizeTimeout = false; + } + else { + this.shouldOptimizeTimeout = Boolean.parseBoolean(shouldOptimize); + if (this.shouldOptimizeTimeout) { + // config is set to true + if (abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT) != null) { + this.maxReqTimeout = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_MAX_REQUEST_TIMEOUT)); + } + if (abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE) != null) { + this.timeoutIncRate = Integer.parseInt(abfsConfiguration.get(ConfigurationKeys.AZURE_REQUEST_TIMEOUT_INCREASE_RATE)); + } + if (this.maxReqTimeout == -1 || this.timeoutIncRate == -1) { + this.shouldOptimizeTimeout = false; + } else { + initTimeouts(); + updateUrl(); + } + } + } + } else { + // optimization not required for opType == null + this.shouldOptimizeTimeout = false; + } + } + + public void updateRetryTimeout(int retryCount) { + if (!this.shouldOptimizeTimeout) { + return; + } + + // update all timeout values + updateTimeouts(retryCount); + updateUrl(); + } + + public URL getUrl() { + return url; + } + public boolean getShouldOptimizeTimeout() { + return this.shouldOptimizeTimeout; + } + + public int getRequestTimeout() { + return requestTimeout; + } + + public int getReadTimeout() { + return readTimeout; + } + + public int getReadTimeout(final int defaultTimeout) { + if (readTimeout != -1 && shouldOptimizeTimeout) { + return readTimeout; + } + return defaultTimeout; + } + + private void initTimeouts() { + String query = url.getQuery(); + Integer timeoutPos = Integer.valueOf(query.indexOf(\"timeout\")); + if (timeoutPos != null && timeoutPos < 0) { + // no value of timeout exists in the URL + // no optimization is needed for this particular request as well + shouldOptimizeTimeout = false; + return; + } + + String timeout = \"\"; + switch(opType) { + case CreateFileSystem: Review Comment: why not add a field to AbfsRestOperationType giving the string prefix for all parameters, e.g. \"createfilesystem\" which is then mapped to fs.azure.request.createfilesystem.timeout and would allow for any new per-request options to be added. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -46,6 +46,32 @@ public final class ConfigurationKeys { public static final String AZURE_BACKOFF_INTERVAL = \"fs.azure.io.retry.backoff.interval\"; public static final String AZURE_MAX_IO_RETRIES = \"fs.azure.io.retry.max.retries\"; public static final String AZURE_CUSTOM_TOKEN_FETCH_RETRY_COUNT = \"fs.azure.custom.token.fetch.retry.count\"; + public static final String AZURE_REQUEST_TIMEOUT_INCREASE_RATE = \"fs.azure.timeout.increase.rate\"; + public static final String AZURE_MAX_REQUEST_TIMEOUT = \"fs.azure.max.request.timeout\"; + + // API-specific request timeout configurations + public static final String AZURE_CREATE_FS_REQUEST_TIMEOUT = \"fs.azure.createfs.request.timeout\"; Review Comment: prefer fs.azure.request.createfs.timeout why? isolates all requests under the \"fs.azure.request\" prefix and avoids mixing them with any other config option"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-1469788058 currently the max timeout is 90s, right? is there any way to extend this for those operations we know may be extra slow (directory delete...)"},{"author":"Shilun Fan","body":"Bulk update: moved all 3.4.0 non-blocker issues, please move back if it is a blocker. Retarget 3.5.0."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5399: URL: https://github.com/apache/hadoop/pull/5399#issuecomment-3447891002 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5399: HADOOP-18632: [ABFS] Customize and optimize timeouts based on each separate request URL: https://github.com/apache/hadoop/pull/5399"}]}
{"key":"HADOOP-18629","summary":"Hadoop DistCp supports specifying favoredNodes for data copying","description":"When importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp. Could I submit a pull request to optimize it?","status":"Open","priority":"Major","reporter":"zhuyaogai","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-14T03:12:19.000+0000","updated":"2025-10-27T00:25:04.000+0000","comments":[{"author":"ASF GitHub Bot","body":"zhuyaogai opened a new pull request, #5391: URL: https://github.com/apache/hadoop/pull/5391 ### Description of PR Hadoop DistCp supports specifying favoredNodes for data copying. ### How was this patch tested? Add new UT. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429555546 -1 to anything exposing internal hdfs implementation methods. Sorry People start using them and expect them to be stable and maintained. There is also the little detail that in cloud deployments do not always have hdfs jars on the class path; this PR would break those deployments. What would make sense would be to use createFile() and for hdfs to add a .opt() option for those favoured nodes, createFile() is the public api, .opt() options can be ignorred by other filesystems, *or reimplemented*. There is a lot more in terms of design and wiring up but the benefit is that portability and maintainability."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429650998 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 32s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 31m 33s | | trunk passed | | +1 :green_heart: | compile | 23m 39s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 20m 40s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 6s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 54s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 29s | | the patch passed | | +1 :green_heart: | compile | 22m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 41s | | the patch passed | | +1 :green_heart: | compile | 20m 43s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 43s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/results-checkstyle-root.txt) | root: The patch generated 9 new + 54 unchanged - 1 fixed = 63 total (was 55) | | +1 :green_heart: | mvnsite | 2m 4s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 27m 55s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 1s | | The patch does not generate ASF License warnings. | | | | 251m 27s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestDistCpSync | | | hadoop.tools.TestDistCpWithRawXAttrs | | | hadoop.tools.TestDistCpSystem | | | hadoop.tools.TestDistCpWithAcls | | | hadoop.tools.TestDistCpWithXAttrs | | | hadoop.tools.contract.TestLocalContractDistCp | | | hadoop.tools.contract.TestHDFSContractDistCp | | | hadoop.tools.TestDistCpSyncReverseFromTarget | | | hadoop.tools.TestDistCpSyncReverseFromSource | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 060e891702b0 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / be67a5984771d510d7cfd96439215bccc3d5baca | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/testReport/ | | Max. process+thread count | 566 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1429680256 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 42s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 37m 13s | | trunk passed | | +1 :green_heart: | compile | 23m 10s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 20m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 6s | | trunk passed | | +1 :green_heart: | javadoc | 1m 47s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 56s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 53s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 27s | | the patch passed | | +1 :green_heart: | compile | 22m 26s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 26s | | the patch passed | | +1 :green_heart: | compile | 20m 28s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/results-checkstyle-root.txt) | root: The patch generated 8 new + 55 unchanged - 1 fixed = 63 total (was 56) | | +1 :green_heart: | mvnsite | 1m 59s | | the patch passed | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 7s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 29m 24s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 247m 24s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestDistCpSync | | | hadoop.tools.TestDistCpWithRawXAttrs | | | hadoop.tools.TestDistCpSystem | | | hadoop.tools.TestDistCpSyncReverseFromTarget | | | hadoop.tools.TestDistCpWithAcls | | | hadoop.tools.contract.TestHDFSContractDistCp | | | hadoop.tools.TestDistCpSyncReverseFromSource | | | hadoop.tools.contract.TestLocalContractDistCp | | | hadoop.tools.TestDistCpWithXAttrs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a25803cbcfde 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 08c0cafef93ba76b6b69613bdfcc2542467ca5d3 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/testReport/ | | Max. process+thread count | 735 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430118718 > @steveloughran hi, I have fixed some problems, could you please review it again? and please correct me if I'm wrong. Thank you :)"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1430405499 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 45s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 17s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 31m 37s | | trunk passed | | +1 :green_heart: | compile | 24m 53s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 22m 10s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 44s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 4s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 55s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 58s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 28s | | the patch passed | | +1 :green_heart: | compile | 22m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 25s | | the patch passed | | +1 :green_heart: | compile | 20m 25s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 36s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 3s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 8s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 45s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 14m 46s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 240m 40s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.contract.TestLocalContractDistCp | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d770bec9be9c 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1182895b78d4d92012d09f1fa11d88abd50ef6b7 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/testReport/ | | Max. process+thread count | 667 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1432392281 @steveloughran hi, thanks for your suggestion:) I know what you mean, but I find that it also uses the hdfs public/stable API in source code. https://github.com/apache/hadoop/blob/723535b788070f6b103be3bae621fefe3b753081/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java#L230 I just refer to its practice in the if branch, and if you think my code changes affect too much, can I just change the else branch code and add favoredNodes option in it? Please correct me if I'm wrong. Thank you :)"},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901558 @steveloughran @toddlipcon @phunt hi, could you give me some advice? Thank you!"},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1442901997 @steveloughran @toddlipcon @phunt hi, could you give me some advice? Thank you!"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1443728823 This is interesting. I know there are deployments without hdfs around (e.g. azure clusters), but do see that the import is there for snapshot updates (hdfs only) with explicit imports of SnapshotDiffReport. tracing it back, if you use \"-diff\" on the command line then hdfs *must* be on the classpath. your link flags up that it is already in copymapper; looking at that I don't see it in branch-3.3; it came in with HADOOP-14254. Given it is in there on a codepath hit with the option to copy erasure policy (and skipped if not), then again, provided the change goes in such that it is optional, your patch isn't going to force in a new run-time dependency, is it? let me look at the new patch some more without worrying about that detail...the builder API is public. Be aware that we are always very nervous about touching distcp because it is fairly old and brittle code that is used incredibly broadly -not just on the command line but actually at the Java API from applications like hive. I think this is fairly low risk but will highlight the JIRA on the HDFS mailing list so they can review it to."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1117059919 ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -594,4 +595,49 @@ public void testUpdateRoot() throws Exception { assertEquals(srcStatus.getModificationTime(), destStatus2.getModificationTime()); } + + @Test + public void testFavoredNodes() throws Exception { + final String testRoot = \"/testdir\"; + final String testSrc = testRoot + \"/\" + SRCDAT; + final String testDst = testRoot + \"/\" + DSTDAT; + + String nnUri = FileSystem.getDefaultUri(conf).toString(); + String rootStr = nnUri + testSrc; + String tgtStr = nnUri + testDst; + + FileEntry[] srcFiles = { + new FileEntry(SRCDAT, true), + new FileEntry(SRCDAT + \"/file10\", false) + }; + + DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf); + createFiles(fs, testRoot, srcFiles, -1); + + // sad path + assertNotEquals(ToolRunner.run(conf, new DistCp(), Review Comment: the args are the wrong way round fro these assertions. switch to AssertJ assertions and add good .descriptions, so if a jenkins runs fails we know what went wrong, rather than just what line. Right ########## hadoop-tools/hadoop-distcp/src/site/markdown/DistCp.md.vm: ########## @@ -364,6 +364,7 @@ Command Line Options | `-direct` | Write directly to destination paths | Useful for avoiding potentially very expensive temporary file rename operations when the destination is an object store | | `-useiterator` | Uses single threaded listStatusIterator to build listing | Useful for saving memory at the client side. Using this option will ignore the numListstatusThreads option | | `-updateRoot` | Update root directory attributes (eg permissions, ownership ...) | Useful if you need to enforce root directory attributes update when using distcp | +| `-favoredNodes` | Specify favored nodes (Desired option input format: host1:port1,host2:port2,...) | Useful if you need to specify favored nodes when using distcp | Review Comment: +. Requires the destination to be an hdfs filesystem ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java: ########## @@ -32,6 +32,7 @@ import java.util.List; import java.util.Random; +import org.apache.hadoop.hdfs.server.datanode.DataNode; Review Comment: nit: must go in the right place in the org.apache.hadoop imports. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { + String[] split = hostAndPort.split(\":\"); Review Comment: log at debug, or maybe even at info. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: if you pull this out to a @VisibleForTesting package scoped method then unit tests could to try to break it through invalid args, e.g * trailing , * empty string invalid node/valid node bad port. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { + String[] split = hostAndPort.split(\":\"); + if (split.length != 2) { + throw new IllegalArgumentException(\"Illegal favoredNodes parameter: \" + hostAndPort); Review Comment: prefer `org.apache.hadoop.util.Preconditions` here, e.g ``` checkArgument(split.length == 2, \"\"Illegal favoredNodes parameter: %s\", hostAndPort) ``` ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java: ########## @@ -164,6 +164,8 @@ public final class DistCpOptions { private final boolean updateRoot; + private final String favoredNodes; + Review Comment: nit: add javadocs. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { Review Comment: javadocs to explain what happens, when exceptions are raised. ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { Review Comment: what happens if an empty string is passed in? it should be an error, right? so add a test ########## hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpOptions.java: ########## @@ -574,4 +574,15 @@ public void testUpdateRoot() { .build(); Assert.assertTrue(options.shouldUpdateRoot()); } + + @Test + public void testFavoredNodes() { + final DistCpOptions options = new DistCpOptions.Builder( + Collections.singletonList( + new Path(\"hdfs://localhost:8020/source\")), + new Path(\"hdfs://localhost:8020/target/\")) + .withFavoredNodes(\"localhost:50010\") + .build(); + Assert.assertNotNull(options.getFavoredNodes()); Review Comment: prefer assertJ for new tests, with a message, and ideally verification the node value went all the way through ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: There should be one path if preserving ec or favoredNodes is set, so switch to hdfsbuilder, leaving the other path as create()"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455156173 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 28s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 50s | | trunk passed | | +1 :green_heart: | compile | 23m 12s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 37s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 49s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 5s | | trunk passed | | +1 :green_heart: | javadoc | 1m 50s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 15s | | the patch passed | | +1 :green_heart: | compile | 22m 29s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 29s | | the patch passed | | +1 :green_heart: | compile | 20m 27s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 41s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 1s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 10s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 44s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 14m 7s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 3s | | The patch does not generate ASF License warnings. | | | | 222m 28s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e50831db0918 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 662045ad8a6c5d2c5dff9907a0ac7e6842844153 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/testReport/ | | Max. process+thread count | 686 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125831678 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -247,6 +276,22 @@ private long copyToFile(Path targetPath, FileSystem targetFS, context); } + private InetSocketAddress[] toFavoredNodes(String favoredNodesStr) throws UnknownHostException { + List result = new ArrayList<>(); + for (String hostAndPort : favoredNodesStr.split(\",\")) { Review Comment: I have checked if `favoredNodesStr ` is an empty string above, and will skip `favoredNodesStr` if empty."},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`?"},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125837868 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java: ########## @@ -239,6 +239,21 @@ public static DistCpOptions parse(String[] args) } } + if (command.hasOption(DistCpOptionSwitch.FAVORED_NODES.getSwitch())) { + String favoredNodesStr = getVal(command, DistCpOptionSwitch.FAVORED_NODES.getSwitch().trim()); + if (StringUtils.isEmpty(favoredNodesStr)) { Review Comment: Add new UT in `TestOptionsParser`, and I think it's ok that YARN cluster can resolve favored nodes. BTW, it seems that DN does not care about `port` but `hostname`? https://github.com/apache/hadoop/blob/2a0dc2ab2f5fb46dc540ed440d6c8b2896dd195b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java#L703"},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125843372 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: If I only change else branch, some UT will be fail when I have set `favoredNodes`, because in my UT it just goes into the if branch (ec settings if not set). Is there a better solution?"},{"author":"ASF GitHub Bot","body":"zhuyaogai commented on code in PR #5391: URL: https://github.com/apache/hadoop/pull/5391#discussion_r1125847429 ########## hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java: ########## @@ -223,9 +233,25 @@ private long copyToFile(Path targetPath, FileSystem targetFS, FSDataOutputStream out; ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum); if (!preserveEC || ecPolicy == null) { - out = targetFS.create(targetPath, permission, - EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, - repl, blockSize, context, checksumOpt); + if (targetFS instanceof DistributedFileSystem Review Comment: Thank you for your code review and so many valuable comments"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1455526533 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 17m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 36s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 40s | | trunk passed | | +1 :green_heart: | compile | 23m 0s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 26s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 45s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 3s | | trunk passed | | +1 :green_heart: | javadoc | 1m 49s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 2s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 46s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 15s | [/patch-mvninstall-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch failed. | | -1 :x: | compile | 12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | javac | 12m 30s | [/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | root in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | compile | 11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | -1 :x: | javac | 11m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | root in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 18s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/results-checkstyle-root.txt) | root: The patch generated 1 new + 63 unchanged - 0 fixed = 64 total (was 63) | | -1 :x: | mvnsite | 0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch failed. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-distcp-jdkUbuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-distcp in the patch failed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-distcp-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | hadoop-distcp in the patch failed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09. | | -1 :x: | spotbugs | 0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch failed. | | +1 :green_heart: | shadedclient | 21m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 33s | | hadoop-hdfs-client in the patch passed. | | -1 :x: | unit | 0m 21s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch failed. | | +1 :green_heart: | asflicense | 0m 40s | | The patch does not generate ASF License warnings. | | | | 198m 42s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6b1c4e22c346 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b20d228c6939cded9fb2ca92ef667feb84b5397d | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/testReport/ | | Max. process+thread count | 648 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-1456058542 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 42s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 15m 13s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 25m 49s | | trunk passed | | +1 :green_heart: | compile | 22m 59s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 20m 26s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 3m 47s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 4s | | trunk passed | | +1 :green_heart: | javadoc | 1m 48s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 3m 59s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 28s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 15s | | the patch passed | | +1 :green_heart: | compile | 22m 24s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 22m 24s | | the patch passed | | +1 :green_heart: | compile | 20m 33s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 20m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 38s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/artifact/out/results-checkstyle-root.txt) | root: The patch generated 1 new + 62 unchanged - 0 fixed = 63 total (was 62) | | +1 :green_heart: | mvnsite | 2m 3s | | the patch passed | | +1 :green_heart: | javadoc | 1m 41s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 4m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 42s | | hadoop-hdfs-client in the patch passed. | | +1 :green_heart: | unit | 13m 56s | | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 1m 2s | | The patch does not generate ASF License warnings. | | | | 221m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5391 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux fecad139efb7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c906c78e3e7aecd12a818fcb76d281f713387323 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/testReport/ | | Max. process+thread count | 731 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-tools/hadoop-distcp U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5391/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5391: URL: https://github.com/apache/hadoop/pull/5391#issuecomment-3447891027 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5391: HADOOP-18629. Hadoop DistCp supports specifying favoredNodes for data copying URL: https://github.com/apache/hadoop/pull/5391"}]}
{"key":"HADOOP-18624","summary":"Leaked calls may cause ObserverNameNode OOM.","description":"Leaked calls may cause ObserverNameNode OOM. During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM.","status":"Reopened","priority":"Major","reporter":"ZanderXu","assignee":"ZanderXu","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-08T03:46:04.000+0000","updated":"2025-10-27T01:14:41.000+0000","comments":[{"author":"ASF GitHub Bot","body":"ZanderXu opened a new pull request, #5367: URL: https://github.com/apache/hadoop/pull/5367 ### Description of PR Jira: [HADOOP-18624](https://issues.apache.org/jira/browse/HADOOP-18624) Leaked calls may cause ObserverNameNode OOM. During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5367: URL: https://github.com/apache/hadoop/pull/5367#issuecomment-1422166864 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 55s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 2s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 2s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 33s | | trunk passed | | +1 :green_heart: | compile | 25m 24s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 21m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 6s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 8s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 37s | | trunk passed | | +1 :green_heart: | shadedclient | 28m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 6s | | the patch passed | | +1 :green_heart: | compile | 24m 30s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 24m 30s | | the patch passed | | +1 :green_heart: | compile | 21m 34s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 21m 34s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 0s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 0m 58s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 19s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 9s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 53s | | The patch does not generate ASF License warnings. | | | | 231m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5367/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5367 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1bc85cfd5cea 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ad4c2cada5f011c342feeeb13f78c83805da9204 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5367/1/testReport/ | | Max. process+thread count | 1801 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5367/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"ZanderXu commented on PR #5367: URL: https://github.com/apache/hadoop/pull/5367#issuecomment-1442702773 > @ZanderXu . Could you please a test case that is fixed by your change. TestIPC#checkConnect has a pattern you can use to create failing calls. Then you can assert that client.connect.calls.isEmpty() @simbadzina sure, I will complete it later."},{"author":"ASF GitHub Bot","body":"xinglin commented on code in PR #5367: URL: https://github.com/apache/hadoop/pull/5367#discussion_r1116566616 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java: ########## @@ -1485,6 +1487,10 @@ Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, releaseAsyncCall(); } throw e; + } finally { + if (!success) { + connection.calls.remove(call.id); Review Comment: sendRpcRequest(call) will try to add a pair of into rpcRequestQueue. Should we remove that on failure as well? ` rpcRequestQueue.put(Pair.of(call, buf)); `"},{"author":"ASF GitHub Bot","body":"ZanderXu commented on code in PR #5367: URL: https://github.com/apache/hadoop/pull/5367#discussion_r1116588469 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java: ########## @@ -1485,6 +1487,10 @@ Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, releaseAsyncCall(); } throw e; + } finally { + if (!success) { + connection.calls.remove(call.id); Review Comment: Thanks @xinglin for your comment. We don't need to remove the call from `rpcRequestQueue`, because the `rpcRequestThread` will poll it and send to the connection."},{"author":"ASF GitHub Bot","body":"xinglin commented on code in PR #5367: URL: https://github.com/apache/hadoop/pull/5367#discussion_r1130534446 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java: ########## @@ -1485,6 +1487,10 @@ Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, releaseAsyncCall(); } throw e; + } finally { + if (!success) { + connection.calls.remove(call.id); Review Comment: Then the change makes sense to me. Thanks,"},{"author":"ASF GitHub Bot","body":"nstang01 commented on PR #5367: URL: https://github.com/apache/hadoop/pull/5367#issuecomment-2286005513 I encountered a similar problem, will this mr be merged?"},{"author":"Takanobu Asanuma","body":"[~xuzq_zander] Why did you close this jira? The PR hasn't been merged."},{"author":"Takanobu Asanuma","body":"I reopened it. Please let me know if I'm wrong."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5367: URL: https://github.com/apache/hadoop/pull/5367#issuecomment-3449071736 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"tasanuma commented on PR #5367: URL: https://github.com/apache/hadoop/pull/5367#issuecomment-3449138929 I'd like to keep this PR, since it is important."}]}
{"key":"HADOOP-19730","summary":"upgrade bouncycastle to 1.82 due to CVE-2025-8916","description":"https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.","status":"Resolved","priority":"Major","reporter":"PJ Fanning","assignee":"PJ Fanning","labels":["pull-request-available"],"project":"HADOOP","created":"2025-10-19T09:09:36.000+0000","updated":"2025-10-27T05:48:06.000+0000","comments":[{"author":"ASF GitHub Bot","body":"pjfanning opened a new pull request, #8039: URL: https://github.com/apache/hadoop/pull/8039 ### Description of PR HADOOP-19730 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 20m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 51s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 28m 35s | | trunk passed | | +1 :green_heart: | compile | 15m 12s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 15m 28s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | mvnsite | 8m 58s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | javadoc | 9m 36s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 43m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 27m 23s | | the patch passed | | +1 :green_heart: | compile | 14m 49s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 14m 49s | | the patch passed | | +1 :green_heart: | compile | 15m 37s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 7m 1s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 42s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 8m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | shadedclient | 45m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 807m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 36s | | The patch does not generate ASF License warnings. | | | | 1064m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.TestRollingUpgrade | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.service.TestYarnNativeServices | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8039 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/testReport/ | | Max. process+thread count | 4498 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/console | | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"slfan1989 merged PR #8039: URL: https://github.com/apache/hadoop/pull/8039"},{"author":"ASF GitHub Bot","body":"slfan1989 commented on PR #8039: URL: https://github.com/apache/hadoop/pull/8039#issuecomment-3424343993 @pjfanning Thanks for the contribution! Merged into trunk. Could we also open a PR for branch-3.4?"},{"author":"ASF GitHub Bot","body":"pjfanning opened a new pull request, #8047: URL: https://github.com/apache/hadoop/pull/8047 ### Description of PR backport #6976 * HADOOP-19730. Upgrade Bouncycastle to 1.82 due to CVE-2025-8916 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #8047: URL: https://github.com/apache/hadoop/pull/8047#issuecomment-3438232725 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | markdownlint | 0m 0s | | markdownlint was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.4 Compile Tests _ | | +0 :ok: | mvndep | 2m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 39m 30s | | branch-3.4 passed | | +1 :green_heart: | compile | 18m 44s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 17m 41s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 22m 5s | | branch-3.4 passed | | +1 :green_heart: | javadoc | 9m 20s | | branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 36s | | branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 50m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 34s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 29m 11s | | the patch passed | | +1 :green_heart: | compile | 17m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 17m 19s | | the patch passed | | +1 :green_heart: | compile | 15m 51s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 15m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 18m 13s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 9m 11s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 31s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 52m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 720m 55s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 49s | | The patch does not generate ASF License warnings. | | | | 1024m 55s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSubmission | | | hadoop.mapred.gridmix.TestLoadJob | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/8047 | | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs | | uname | Linux d69a46a8ee67 5.15.0-160-generic #170-Ubuntu SMP Wed Oct 1 10:06:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.4 / c8b8fb4e82d33a470f10a447f4799cc872fb3c01 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/testReport/ | | Max. process+thread count | 3660 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8047/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"slfan1989 merged PR #8047: URL: https://github.com/apache/hadoop/pull/8047"},{"author":"ASF GitHub Bot","body":"slfan1989 commented on PR #8047: URL: https://github.com/apache/hadoop/pull/8047#issuecomment-3449617778 @pjfanning Thanks for the contribution! Merged into trunk."}]}
{"key":"HADOOP-19621","summary":"hadoop-client-api exclude webapps/static front-end resources","description":"","status":"Open","priority":"Minor","reporter":"dzcxzl","labels":["pull-request-available"],"project":"HADOOP","created":"2025-07-15T04:50:36.000+0000","updated":"2025-10-28T00:21:59.000+0000","comments":[{"author":"ASF GitHub Bot","body":"cxzl25 opened a new pull request, #7804: URL: https://github.com/apache/hadoop/pull/7804 ### Description of PR `hadoop-client-api` contains some front-end resources in the webapps/static path. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7804: URL: https://github.com/apache/hadoop/pull/7804#issuecomment-3076022688 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 21m 33s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 72m 32s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 115m 21s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 8m 29s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 16s | | the patch passed | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 41m 51s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 19s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 190m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7804 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 8a0fed5ae6d7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0f2cc88fd55ceb31d30a1157bebd34a35e39289f | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api U: hadoop-client-modules/hadoop-client-api | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pan3793 commented on PR #7804: URL: https://github.com/apache/hadoop/pull/7804#issuecomment-3078313651 should they go `hadoop-client-minicluster` instead of removing?"},{"author":"ASF GitHub Bot","body":"zeekling commented on code in PR #7804: URL: https://github.com/apache/hadoop/pull/7804#discussion_r2210654919 ########## hadoop-client-modules/hadoop-client-api/pom.xml: ########## @@ -120,6 +120,7 @@ org/apache/hadoop/yarn/factory/providers/package-info.class org/apache/hadoop/yarn/client/api/impl/package-info.class org/apache/hadoop/yarn/client/api/package-info.class + webapps/static/**/* Review Comment: Maybe can be `webapps/**/*`"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7804: URL: https://github.com/apache/hadoop/pull/7804#issuecomment-3082718434 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 40s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 67m 59s | | trunk passed | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | shadedclient | 111m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 8m 52s | | the patch passed | | +1 :green_heart: | compile | 0m 15s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 15s | | the patch passed | | +1 :green_heart: | compile | 0m 14s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 14s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 15s | | the patch passed | | +1 :green_heart: | javadoc | 0m 14s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | shadedclient | 41m 37s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 17s | | hadoop-client-api in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 178m 44s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7804 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux a6bbba666c3e 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 39406663dedf88a91c5773897c541bff1c9a66ab | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/2/testReport/ | | Max. process+thread count | 533 (vs. ulimit of 5500) | | modules | C: hadoop-client-modules/hadoop-client-api U: hadoop-client-modules/hadoop-client-api | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/2/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #7804: URL: https://github.com/apache/hadoop/pull/7804#issuecomment-3449071470 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #7804: HADOOP-19621: hadoop-client-api exclude webapps/static front-end resources URL: https://github.com/apache/hadoop/pull/7804"}]}
{"key":"HADOOP-18623","summary":"S3A delegation token implementations to be able to update tokens from the user credentials","description":"Spark never renews tokens, instead it can create new ones and attach them to the current users credentials. This means long-running S3A instances which can pick up new tokens/credentials need a way to look for new tokens in the credential chain. Proposed * class AbstractDelegationTokenBinding adds a CallableRaisingIOE field which can be updated with a callback * S3ADelegationTokens to add method boolean maybeUpdateTokenFromOwner() to look for any new token and switch to it if new * S3ADelegationTokens serviceInit() to pass the method down to the instantiated DT binding as the callback It is up to the token binding implementation to decide what to do about it; the standard implementations will do: nothing.","status":"Open","priority":"Minor","reporter":"Steve Loughran","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-07T17:06:10.000+0000","updated":"2025-10-28T00:21:59.000+0000","comments":[{"author":"ASF GitHub Bot","body":"steveloughran opened a new pull request, #5365: URL: https://github.com/apache/hadoop/pull/5365 ### Description of PR Adds the production side changes; no tests ### How was this patch tested? will let yetus do that ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"Steve Loughran","body":"+[~mthakur] [~mehakmeet]"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5365: URL: https://github.com/apache/hadoop/pull/5365#issuecomment-1421286401 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 15s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 14s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | -1 :x: | compile | 0m 22s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-aws in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | javac | 0m 22s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-aws in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | -1 :x: | compile | 0m 20s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-aws in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | -1 :x: | javac | 0m 20s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_352-8u352-ga-1~20.04-b08.txt) | hadoop-aws in the patch failed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) | hadoop-tools/hadoop-aws: The patch generated 1 new + 1 unchanged - 0 fixed = 2 total (was 1) | | -1 :x: | mvnsite | 0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | -1 :x: | spotbugs | 0m 21s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | +1 :green_heart: | shadedclient | 25m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 25s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) | hadoop-aws in the patch failed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 102m 44s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5365 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 311bfa3c72b2 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3d55a4841c2987e6183c831c9ac3ed1b4b210762 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/testReport/ | | Max. process+thread count | 575 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5365: URL: https://github.com/apache/hadoop/pull/5365#issuecomment-1424564307 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 36s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 46s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 15s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 44s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 30s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 9s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 37s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 32s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 105m 26s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5365 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 9422d6baf192 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ebd4fd44b2646ccd7657fffd27ff244202d03f94 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/2/testReport/ | | Max. process+thread count | 631 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5365/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5365: URL: https://github.com/apache/hadoop/pull/5365#issuecomment-3449071771 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5365: HADOOP-18623. S3A delegation token implementations to be able to update tokens URL: https://github.com/apache/hadoop/pull/5365"}]}
{"key":"HADOOP-18544","summary":"S3A: add option to disable probe for dir marker recreation on delete/rename.","description":"In applications which do many single-file deletions on the same dir, a lot of time is wasted in {{maybeCreateFakeParentDirectory()}}. Proposed: add an option to disable the probe, for use by applications which are happy for parent dirs to sometimes disappear after a cleanup. file by file delete is still woefully inefficient because of the HEAD request on every file, but there's no need to amplify the damage.","status":"Resolved","priority":"Major","reporter":"Steve Loughran","assignee":"Harshit Gupta","labels":["pull-request-available"],"project":"HADOOP","created":"2022-11-28T17:48:10.000+0000","updated":"2025-10-28T00:22:00.000+0000","comments":[{"author":"Steve Loughran","body":"[~harshit.gupta] assigning to you this'll need a new s3a option (Constants.java) read to a field in s3afs. initialize(), then checked in {{maybeCreateFakeParentDirectory()}} to skip the delete. then need a test (similar to ITestS3ARenameCost/ITestS3ADeleteCost) which asserts that no HEAD request is made on rename and delete. those existing suites will need to set the new option to false to stop all their existing tests failing; see their parent class's createConfiguration() to see what to do there."},{"author":"ASF GitHub Bot","body":"HarshitGupta11 opened a new pull request, #5354: URL: https://github.com/apache/hadoop/pull/5354 ### Description of PR In applications which do many single-file deletions on the same dir, a lot of time is wasted in maybeCreateFakeParentDirectory(). Proposed: add an option to disable the probe, for use by applications which are happy for parent dirs to sometimes disappear after a cleanup. file by file delete is still woefully inefficient because of the HEAD request on every file, but there's no need to amplify the damage. ### How was this patch tested? The patch was tested against s3 bucket in US-West 2 ### For code changes: ##Caveats: Parent directories might disappear on delete or on renames. ##What breaks: The rename tests are failing for the FileContext renames as both S3AFileSystem and the FileContext have different probes and different rules. - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5354: URL: https://github.com/apache/hadoop/pull/5354#issuecomment-1419083013 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 2 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 59s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 30m 53s | | trunk passed | | +1 :green_heart: | compile | 23m 1s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 20m 23s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 3m 46s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 44s | | trunk passed | | -1 :x: | javadoc | 1m 15s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 1m 39s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 57s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 29s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 1m 49s | | the patch passed | | +1 :green_heart: | compile | 22m 26s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 22m 26s | | the patch passed | | +1 :green_heart: | compile | 20m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 20m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 3m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/artifact/out/results-checkstyle-root.txt) | root: The patch generated 40 new + 49 unchanged - 0 fixed = 89 total (was 49) | | +1 :green_heart: | mvnsite | 2m 38s | | the patch passed | | -1 :x: | javadoc | 1m 6s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 1m 38s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 4m 12s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 15s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 18m 21s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 2m 50s | | hadoop-aws in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 237m 26s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.TestFilterFs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5354 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 8f26950b449b 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0f96e64c55639970ea3fc6013c6942b21807ed8c | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/testReport/ | | Max. process+thread count | 1302 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5354/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5354: URL: https://github.com/apache/hadoop/pull/5354#issuecomment-2256199776 harshit, now that #6789 is in, we support an option `fs.s3a.performance.flags` for a CSV list of flags. You could make this one of them and people can enable it for applications which are ok with the changed semantics"},{"author":"Steve Loughran","body":"Harshit tried this and too many things broke. pity"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5354: URL: https://github.com/apache/hadoop/pull/5354#issuecomment-3449071811 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5354: HADOOP-18544: S3A: add option to disable probe for dir marker recreation on delete/rename. URL: https://github.com/apache/hadoop/pull/5354"}]}
{"key":"HADOOP-18613","summary":"Upgrade ZooKeeper to version 3.8.3","description":"","status":"Resolved","priority":"Major","reporter":"Tamas Penzes","assignee":"Bilwa S T","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-02T15:56:06.000+0000","updated":"2025-10-28T00:22:02.000+0000","comments":[{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-1415919530 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 16s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 72m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | compile | 0m 11s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 13s | | the patch passed | | +1 :green_heart: | javadoc | 0m 12s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 11s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 27m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 14s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 104m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5345 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 97bfd0150be1 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ed7f25f252f90c2e2580e849e44a6e70fa3e242 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/testReport/ | | Max. process+thread count | 540 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"virajjasani commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-1457023698 We also need to upgrade curator to 5.4.0 right?"},{"author":"ASF GitHub Bot","body":"virajjasani commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-1457027626 Curator 5.4.0 supports Zookeeper 3.7 at least, not sure if it would work with 3.8 as well, but let's try in our build first. Thanks"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-1459179436 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 16m 43s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 28m 44s | | trunk passed | | +1 :green_heart: | compile | 25m 33s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 21m 55s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | mvnsite | 26m 22s | | trunk passed | | +1 :green_heart: | javadoc | 8m 27s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 7m 27s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | shadedclient | 38m 33s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 1m 1s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 25m 17s | | the patch passed | | +1 :green_heart: | compile | 24m 57s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 24m 57s | | the patch passed | | +1 :green_heart: | compile | 21m 56s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | -1 :x: | javac | 21m 56s | [/results-compile-javac-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/2/artifact/out/results-compile-javac-root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09.txt) | root-jdkPrivateBuild-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 generated 1 new + 2624 unchanged - 1 fixed = 2625 total (was 2625) | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 21m 19s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 8m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 7m 21s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | shadedclient | 39m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 486m 42s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/2/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 12s | | The patch does not generate ASF License warnings. | | | | 783m 58s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.blockmanagement.TestPendingReconstruction | | | hadoop.hdfs.TestErasureCodingExerciseAPIs | | | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetCache | | | hadoop.cli.TestAclCLI | | | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA | | | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength | | | hadoop.hdfs.server.namenode.TestStartup | | | hadoop.hdfs.server.namenode.TestFSEditLogLoader | | | hadoop.hdfs.qjournal.server.TestJournaledEditsCache | | | hadoop.hdfs.server.namenode.TestAuditLoggerWithCommands | | | hadoop.hdfs.TestQuota | | | hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode | | | hadoop.hdfs.server.datanode.TestDataNodeMetricsLogger | | | hadoop.hdfs.server.namenode.TestCheckpoint | | | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithDFSAdmin | | | hadoop.hdfs.tools.TestECAdmin | | | hadoop.hdfs.server.diskbalancer.TestDiskBalancer | | | hadoop.hdfs.tools.TestDebugAdmin | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.web.TestWebHDFS | | | hadoop.hdfs.server.namenode.TestFSNamesystemLock | | | hadoop.hdfs.TestEncryptedTransfer | | | hadoop.hdfs.TestDFSShell | | | hadoop.hdfs.server.namenode.TestAuditLogger | | | hadoop.hdfs.server.namenode.TestFSNamesystemLockReport | | | hadoop.hdfs.server.namenode.TestFsck | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithFSCommands | | | hadoop.hdfs.TestViewDistributedFileSystem | | | hadoop.hdfs.server.namenode.TestCacheDirectives | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.cli.TestAclCLIWithPosixAclInheritance | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.hdfs.server.datanode.TestLargeBlockReport | | | hadoop.hdfs.server.namenode.TestNamenodeRetryCache | | | hadoop.hdfs.server.datanode.TestFsDatasetCacheRevocation | | | hadoop.hdfs.server.namenode.TestNameNodeMetricsLogger | | | hadoop.hdfs.server.namenode.TestAuditLogs | | | hadoop.hdfs.TestMaintenanceState | | | hadoop.cli.TestXAttrCLI | | | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy | | | hadoop.hdfs.server.namenode.TestCacheDirectivesWithViewDFS | | | hadoop.hdfs.TestRollingUpgrade | | | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer | | | hadoop.hdfs.TestDFSUpgradeFromImage | | | hadoop.hdfs.TestDistributedFileSystem | | | hadoop.hdfs.server.namenode.TestAuditLogAtDebug | | | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby | | | hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer | | | hadoop.hdfs.TestDFSRename | | | hadoop.hdfs.tools.TestDFSAdminWithHA | | | hadoop.hdfs.server.namenode.TestEditsDoubleBuffer | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5345 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 101f71a1699b 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 79777a29e5aaa44f53a3517154b55a2c832bf668 | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/2/testReport/ | | Max. process+thread count | 2246 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"jojochuang commented on code in PR #5345: URL: https://github.com/apache/hadoop/pull/5345#discussion_r1200819683 ########## hadoop-project/pom.xml: ########## @@ -37,7 +37,7 @@ true true - 9.4.48.v20220622 + 9.4.49.v20220914 Review Comment: This is unrelated. Are you updating Jetty because Zookeeper has jetty dependency and you're trying to harmonize them? If so I'd suggest to exclude the jetty from zookeeper. We update Jetty frequently and do not necessarily use the same version as Zookeeper."},{"author":"Bilwa S T","body":"Hi [~szucsvillo] Are you planning to work on this issue? If not, can i take it up? We want to upgrade to zookeeper 3.8.2"},{"author":"Szucs Villo","body":"Hi [~BilwaST] , I'm not planning to work on this issue. If you're interested, feel free to take it up."},{"author":"ASF GitHub Bot","body":"BilwaST opened a new pull request, #6296: URL: https://github.com/apache/hadoop/pull/6296 on 5.4.0 ### Description of PR ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"BilwaST commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1825925014 cc @brahmareddybattula @virajjasani @ayushtkn"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1826215393 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 8s | | trunk passed | | +1 :green_heart: | compile | 8m 19s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 30s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | mvnsite | 11m 54s | | trunk passed | | +1 :green_heart: | javadoc | 4m 47s | | trunk passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 56s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | shadedclient | 28m 44s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 17m 3s | | the patch passed | | +1 :green_heart: | compile | 8m 4s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 4s | | the patch passed | | +1 :green_heart: | compile | 7m 30s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 7m 30s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 7m 4s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 4m 33s | | the patch passed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 53s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | shadedclient | 29m 2s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 673m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/1/artifact/out/patch-unit-root.txt) | root in the patch passed. | | -1 :x: | asflicense | 0m 52s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/1/artifact/out/results-asflicense.txt) | The patch generated 6 ASF License warnings. | | | | 836m 14s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.tools.TestDFSAdminWithHA | | | hadoop.hdfs.server.namenode.TestNamenodeRetryCache | | | hadoop.hdfs.server.namenode.TestFsck | | | hadoop.hdfs.tools.TestECAdmin | | | hadoop.hdfs.TestQuota | | | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetCache | | | hadoop.hdfs.TestDFSUpgradeFromImage | | | hadoop.hdfs.server.namenode.TestFSNamesystemLock | | | hadoop.hdfs.server.namenode.TestEditsDoubleBuffer | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithFSCommands | | | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy | | | hadoop.cli.TestAclCLI | | | hadoop.cli.TestHDFSCLI | | | hadoop.hdfs.server.namenode.TestAuditLogAtDebug | | | hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider | | | hadoop.hdfs.TestDFSShell | | | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength | | | hadoop.hdfs.server.datanode.TestDataNodeMetricsLogger | | | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby | | | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA | | | hadoop.hdfs.TestEncryptedTransfer | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.TestDFSRename | | | hadoop.hdfs.server.datanode.TestLargeBlockReport | | | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer | | | hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer | | | hadoop.hdfs.server.datanode.TestFsDatasetCacheRevocation | | | hadoop.hdfs.tools.TestDebugAdmin | | | hadoop.hdfs.server.namenode.TestAuditLoggerWithCommands | | | hadoop.hdfs.server.namenode.TestNameNodeResourcePolicy | | | hadoop.cli.TestAclCLIWithPosixAclInheritance | | | hadoop.hdfs.server.blockmanagement.TestPendingReconstruction | | | hadoop.hdfs.server.namenode.TestAuditLogger | | | hadoop.hdfs.server.namenode.TestCacheDirectivesWithViewDFS | | | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.hdfs.server.namenode.TestNameNodeMetricsLogger | | | hadoop.hdfs.qjournal.server.TestJournaledEditsCache | | | hadoop.hdfs.server.diskbalancer.TestDiskBalancer | | | hadoop.hdfs.server.namenode.TestCheckpoint | | | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA | | | hadoop.hdfs.server.namenode.TestFSEditLogLoader | | | hadoop.hdfs.server.namenode.TestAuditLogs | | | hadoop.hdfs.server.namenode.TestFSNamesystemLockReport | | | hadoop.hdfs.server.namenode.TestStartup | | | hadoop.hdfs.TestMaintenanceState | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.namenode.snapshot.TestFsShellMoveToTrashWithSnapshots | | | hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode | | | hadoop.cli.TestXAttrCLI | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithDFSAdmin | | | hadoop.hdfs.server.namenode.TestCacheDirectives | | | hadoop.util.curator.TestSecureZKCuratorManager | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6296 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux bafa1fc414ac 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3965474a8f646aa6c546021e31d46cec6d186478 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/1/testReport/ | | Max. process+thread count | 4214 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/1/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"ayushtkn commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1831922688 @BilwaST I think curator has some issue with ZK 3.8.2, they upgraded to 3.7.2 now in https://github.com/apache/curator/pull/489, which is targeted to 5.6.0, not sure if that is gonna help or not... can raise it to the curator ML & see if there is any response I think curator 5.5.0 is there as well, you tried with 5.4.0, you can check if 5.5.0 works or not, maybe run any failing test locally to see if it goes through"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1839175427 +can you give us the full dependency tree so we can see what we can strip (yetus annotations etc) ``` mvn dependency:tree -Dverbose ```"},{"author":"ASF GitHub Bot","body":"BilwaST opened a new pull request, #6327: URL: https://github.com/apache/hadoop/pull/6327 on 5.4.0"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6327: URL: https://github.com/apache/hadoop/pull/6327#issuecomment-1843903381 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 1s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ branch-3.3 Compile Tests _ | | +0 :ok: | mvndep | 0m 20s | | Maven dependency ordering for branch | | -1 :x: | mvninstall | 0m 23s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/branch-mvninstall-root.txt) | root in branch-3.3 failed. | | -1 :x: | compile | 0m 23s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/branch-compile-root.txt) | root in branch-3.3 failed. | | -1 :x: | mvnsite | 0m 22s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/branch-mvnsite-root.txt) | root in branch-3.3 failed. | | -1 :x: | javadoc | 0m 23s | [/branch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/branch-javadoc-root.txt) | root in branch-3.3 failed. | | -1 :x: | shadedclient | 1m 34s | | branch has errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 15s | | Maven dependency ordering for patch | | -1 :x: | mvninstall | 0m 22s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-mvninstall-root.txt) | root in the patch failed. | | -1 :x: | mvninstall | 0m 23s | [/patch-mvninstall-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-mvninstall-hadoop-project.txt) | hadoop-project in the patch failed. | | -1 :x: | compile | 1m 31s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | javac | 1m 31s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-compile-root.txt) | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 0m 35s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-javadoc-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 1m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 489m 18s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +0 :ok: | asflicense | 0m 39s | | ASF License check generated no output? | | | | 510m 2s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.web.TestWebHDFS | | | hadoop.hdfs.TestDecommissionWithBackoffMonitor | | | hadoop.cli.TestHDFSCLI | | | hadoop.hdfs.server.namenode.TestCheckpoint | | | hadoop.hdfs.server.namenode.TestFSNamesystemLock | | | hadoop.hdfs.TestQuota | | | hadoop.hdfs.TestDFSShell | | | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetCache | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithDFSAdmin | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithFSCommands | | | hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer | | | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA | | | hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider | | | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer | | | hadoop.cli.TestAclCLIWithPosixAclInheritance | | | hadoop.cli.TestXAttrCLI | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | | hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier | | | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA | | | hadoop.hdfs.TestEncryptedTransfer | | | hadoop.hdfs.server.namenode.TestFsck | | | hadoop.cli.TestAclCLI | | | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength | | | hadoop.hdfs.tools.TestECAdmin | | | hadoop.hdfs.server.namenode.TestFSEditLogLoader | | | hadoop.hdfs.server.namenode.TestStartup | | | hadoop.hdfs.server.blockmanagement.TestPendingReconstruction | | | hadoop.hdfs.tools.TestDebugAdmin | | | hadoop.hdfs.server.namenode.TestEditsDoubleBuffer | | | hadoop.hdfs.server.diskbalancer.TestDiskBalancer | | | hadoop.hdfs.TestMaintenanceState | | | hadoop.hdfs.tools.TestDFSAdminWithHA | | | hadoop.hdfs.TestDFSUpgradeFromImage | | | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby | | | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.hdfs.TestErasureCodingExerciseAPIs | | | hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade | | | hadoop.hdfs.qjournal.server.TestJournaledEditsCache | | | hadoop.yarn.client.TestApplicationClientProtocolOnHA | | | hadoop.yarn.client.TestRMFailover | | | hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown | | | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRun | | | hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown | | | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowActivity | | | hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRunCompaction | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6327 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 1b231422f0c8 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | branch-3.3 / 5a4b446bfb1f4c1f3313084a6e81b61d0be0338b | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~18.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/testReport/ | | Max. process+thread count | 3660 (vs. ulimit of 5500) | | modules | C: . hadoop-project U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6327/1/console | | versions | git=2.17.1 maven=3.6.0 shellcheck=0.4.6 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"BilwaST commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1855990891 > @BilwaST I think curator has some issue with ZK 3.8.2, they upgraded to 3.7.2 now in [apache/curator#489](https://github.com/apache/curator/pull/489), which is targeted to 5.6.0, not sure if that is gonna help or not... > > can raise it to the curator ML & see if there is any response > > I think curator 5.5.0 is there as well, you tried with 5.4.0, you can check if 5.5.0 works or not, maybe run any failing test locally to see if it goes through @ayushtkn i tried running with curator 5.5.0, but still it fails. It works fine with curator 5.2.0. I just verified it by running it locally. can we just upgrade zk version?"},{"author":"ASF GitHub Bot","body":"ayushtkn commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1857888275 Should be ok, if the build comes clean"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1858679114 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 28s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 21s | | trunk passed | | +1 :green_heart: | compile | 8m 23s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 33s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 11m 59s | | trunk passed | | +1 :green_heart: | javadoc | 4m 40s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 54s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 28m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 17m 0s | | the patch passed | | +1 :green_heart: | compile | 8m 7s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 7s | | the patch passed | | +1 :green_heart: | compile | 7m 28s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 7m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 7m 0s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 4m 34s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 53s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 29m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 1960m 22s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/2/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 54s | | The patch does not generate ASF License warnings. | | | | 2124m 3s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.namenode.TestFsck | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithFSCommands | | | hadoop.hdfs.server.namenode.TestCheckpoint | | | hadoop.hdfs.server.datanode.TestLargeBlockReport | | | hadoop.hdfs.server.namenode.TestEditsDoubleBuffer | | | hadoop.hdfs.TestDFSUpgradeFromImage | | | hadoop.cli.TestAclCLIWithPosixAclInheritance | | | hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode | | | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.hdfs.TestEncryptedTransfer | | | hadoop.hdfs.server.namenode.TestNameNodeMetricsLogger | | | hadoop.fs.TestEnhancedByteBufferAccess | | | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy | | | hadoop.cli.TestAclCLI | | | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy | | | hadoop.hdfs.TestDistributedFileSystem | | | hadoop.hdfs.server.namenode.TestFSNamesystemLock | | | hadoop.hdfs.server.namenode.TestAuditLoggerWithCommands | | | hadoop.cli.TestHDFSCLI | | | hadoop.hdfs.server.namenode.TestCacheDirectivesWithViewDFS | | | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.hdfs.TestQuota | | | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA | | | hadoop.hdfs.tools.TestViewFileSystemOverloadSchemeWithDFSAdmin | | | hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider | | | hadoop.hdfs.TestDFSRename | | | hadoop.hdfs.server.namenode.TestCacheDirectives | | | hadoop.hdfs.tools.TestDebugAdmin | | | hadoop.hdfs.server.namenode.TestAuditLogs | | | hadoop.hdfs.server.datanode.TestDataNodeMetricsLogger | | | hadoop.hdfs.server.blockmanagement.TestPendingReconstruction | | | hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer | | | hadoop.hdfs.server.namenode.TestAuditLogger | | | hadoop.hdfs.server.namenode.snapshot.TestFsShellMoveToTrashWithSnapshots | | | hadoop.hdfs.TestMaintenanceState | | | hadoop.hdfs.server.namenode.TestNameNodeResourcePolicy | | | hadoop.hdfs.qjournal.server.TestJournaledEditsCache | | | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby | | | hadoop.hdfs.TestErasureCodingExerciseAPIs | | | hadoop.hdfs.server.diskbalancer.TestDiskBalancer | | | hadoop.hdfs.tools.TestDFSAdmin | | | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetCache | | | hadoop.hdfs.TestDFSShell | | | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer | | | hadoop.hdfs.server.namenode.TestFSNamesystemLockReport | | | hadoop.hdfs.server.namenode.TestStartup | | | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA | | | hadoop.hdfs.tools.TestECAdmin | | | hadoop.cli.TestXAttrCLI | | | hadoop.hdfs.server.namenode.TestFSEditLogLoader | | | hadoop.hdfs.server.namenode.TestAuditLogAtDebug | | | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6296 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 6807d4e18fca 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1d91ad178c7a99a3953fa2b49be726b8dfebf44a | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/2/testReport/ | | Max. process+thread count | 4976 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/2/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1858877014 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 15s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 38s | | trunk passed | | +1 :green_heart: | compile | 8m 10s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 40s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 12m 7s | | trunk passed | | +1 :green_heart: | javadoc | 4m 41s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 54s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 28m 16s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 30s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 16m 53s | | the patch passed | | +1 :green_heart: | compile | 8m 3s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 3s | | the patch passed | | +1 :green_heart: | compile | 7m 37s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 7m 37s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 7m 5s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 4m 43s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 51s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 29m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 638m 11s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/3/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 56s | | The patch does not generate ASF License warnings. | | | | 802m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6296 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 0d5d230b424b 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 00db9d1fb1739c026e3a73047031d6bf2b5d6c18 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/3/testReport/ | | Max. process+thread count | 3965 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/3/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"BilwaST commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1858883717 @ayushtkn I guess TestDirectoryScanner is a random failure as i can see it in other PR too and i don't think TestTimelineAuthFilterForV2 failure is related to my changes. It fails with and without my changes in local. Please review updated patch. Thank you"},{"author":"ASF GitHub Bot","body":"ayushtkn commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1858889154 Rethinking: Why are we not moving to 3.8.3?"},{"author":"ASF GitHub Bot","body":"BilwaST commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1859083647 > Rethinking: Why are we not moving to 3.8.3? Earlier when i had checked, the latest stable version was 3.8.2. Looks like 3.8.3 is stable too. I will update the patch"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1859313932 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 7m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 1s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 21s | | trunk passed | | +1 :green_heart: | compile | 8m 18s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 7m 35s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 12m 0s | | trunk passed | | +1 :green_heart: | javadoc | 4m 43s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 55s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 28m 24s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 26s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 17m 7s | | the patch passed | | +1 :green_heart: | compile | 8m 1s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 8m 1s | | the patch passed | | +1 :green_heart: | compile | 7m 32s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 7m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 7m 10s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 4m 37s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 4m 57s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 29m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 640m 26s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/4/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 0m 51s | | The patch does not generate ASF License warnings. | | | | 810m 54s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes | | | hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2 | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6296 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 573619f45acd 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 172e83b0c3a16e04301e9b442d792d2f39b6d653 | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/4/testReport/ | | Max. process+thread count | 3997 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/4/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"BilwaST commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1859498825 @ayushtkn Testcases are random failures. I ran TestBalancerWithHANameNodes locally and it passes"},{"author":"ASF GitHub Bot","body":"ayushtkn commented on code in PR #6296: URL: https://github.com/apache/hadoop/pull/6296#discussion_r1429596566 ########## LICENSE-binary: ########## @@ -337,7 +337,7 @@ org.apache.kerby:kerby-xdr:2.0.3 org.apache.kerby:token-provider:2.0.3 org.apache.solr:solr-solrj:8.11.2 org.apache.yetus:audience-annotations:0.5.0 -org.apache.zookeeper:zookeeper:3.7.2 +org.apache.zookeeper:zookeeper:3.8.2 Review Comment: should be 3.8.3 here as well"},{"author":"ASF GitHub Bot","body":"BilwaST commented on code in PR #6296: URL: https://github.com/apache/hadoop/pull/6296#discussion_r1430847951 ########## LICENSE-binary: ########## @@ -337,7 +337,7 @@ org.apache.kerby:kerby-xdr:2.0.3 org.apache.kerby:token-provider:2.0.3 org.apache.solr:solr-solrj:8.11.2 org.apache.yetus:audience-annotations:0.5.0 -org.apache.zookeeper:zookeeper:3.7.2 +org.apache.zookeeper:zookeeper:3.8.2 Review Comment: Updated it"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #6296: URL: https://github.com/apache/hadoop/pull/6296#issuecomment-1862138496 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 47s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 30m 47s | | trunk passed | | +1 :green_heart: | compile | 16m 18s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 14m 48s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 19m 15s | | trunk passed | | +1 :green_heart: | javadoc | 8m 25s | | trunk passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 31s | | trunk passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 47m 26s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 37s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 28m 38s | | the patch passed | | +1 :green_heart: | compile | 15m 51s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 15m 51s | | the patch passed | | +1 :green_heart: | compile | 14m 51s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | javac | 14m 51s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 13m 37s | | the patch passed | | +1 :green_heart: | shellcheck | 0m 0s | | No new issues. | | +1 :green_heart: | javadoc | 8m 19s | | the patch passed with JDK Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 7m 37s | | the patch passed with JDK Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 49m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 753m 16s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/5/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 1m 36s | | The patch does not generate ASF License warnings. | | | | 1026m 10s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner | | | hadoop.fs.http.server.TestHttpFSServerNoACLs | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/6296 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs | | uname | Linux 774b23ed49ae 5.15.0-86-generic #96-Ubuntu SMP Wed Sep 20 08:23:49 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 4b1f99a81f88bd79880d0a0733c54780fba82f5e | | Default Java | Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.21+9-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_392-8u392-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/5/testReport/ | | Max. process+thread count | 3641 (vs. ulimit of 5500) | | modules | C: hadoop-project . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6296/5/console | | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"ayushtkn merged PR #6296: URL: https://github.com/apache/hadoop/pull/6296"},{"author":"Ayush Saxena","body":"Committed to trunk. Thanx [~BilwaST] for the contribution!!!"},{"author":"Steve Loughran","body":"This is pulling in some extra transitive stuff which surfaces in applications which import hadoop-common into their poms {code} | +- org.apache.zookeeper:zookeeper:jar:3.8.3:compile | | +- org.apache.zookeeper:zookeeper-jute:jar:3.8.3:compile | | | \\- (org.apache.yetus:audience-annotations:jar:0.12.0:compile - omitted for duplicate) | | +- org.apache.yetus:audience-annotations:jar:0.12.0:compile | | +- (io.netty:netty-handler:jar:4.1.94.Final:compile - omitted for conflict with 4.1.100.Final) | | +- (io.netty:netty-transport-native-epoll:jar:4.1.94.Final:compile - omitted for conflict with 4.1.100.Final) | | +- (org.slf4j:slf4j-api:jar:1.7.30:compile - omitted for duplicate) | | +- ch.qos.logback:logback-core:jar:1.2.10:compile | | +- ch.qos.logback:logback-classic:jar:1.2.10:compile | | | +- (ch.qos.logback:logback-core:jar:1.2.10:compile - omitted for duplicate) | | | \\- (org.slf4j:slf4j-api:jar:1.7.32:compile - omitted for conflict with 1.7.30) | | \\- (commons-io:commons-io:jar:2.11.0:compile - omitted for conflict with 2.14.0) {code} the key pain point is logback because we don't override it. proposed: exclude the zk dependencies we either override outselves or don't need."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #6327: URL: https://github.com/apache/hadoop/pull/6327#issuecomment-3374717652 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #6327: HADOOP-18613. Upgrade ZooKeeper to version 3.8.2 and Curator to versi URL: https://github.com/apache/hadoop/pull/6327"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-3449071878 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5345: HADOOP-18613. Upgrade ZooKeeper to version 3.8.1 URL: https://github.com/apache/hadoop/pull/5345"}]}
{"key":"HADOOP-18614","summary":"Ensure that the config writers are closed","description":"Use AutoCloseable to ensure that the config writers are closed between tests.","status":"Open","priority":"Major","reporter":"Steve Vaughan","assignee":"Steve Vaughan","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-02T17:11:29.000+0000","updated":"2025-10-28T00:22:03.000+0000","comments":[{"author":"ASF GitHub Bot","body":"snmvaughan opened a new pull request, #5341: URL: https://github.com/apache/hadoop/pull/5341 ### Description of PR Use AutoCloseable to ensure that the config writers are closed between tests. ### How was this patch tested? Tested using the Hadoop development environment docker image. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5341: URL: https://github.com/apache/hadoop/pull/5341#issuecomment-1414381460 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 50s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 53m 45s | | trunk passed | | +1 :green_heart: | compile | 25m 18s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 21m 34s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 7s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | -1 :x: | javadoc | 1m 8s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/1/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 28m 3s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 6s | | the patch passed | | +1 :green_heart: | compile | 24m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 24m 41s | | the patch passed | | +1 :green_heart: | compile | 21m 43s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 21m 43s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 0s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | -1 :x: | javadoc | 0m 59s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 56s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 5s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 52s | | The patch does not generate ASF License warnings. | | | | 237m 54s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5341 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f2a3bbd83ba5 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0d675da130241a2a369c672df9c3628562543b81 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/1/testReport/ | | Max. process+thread count | 1287 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5341: URL: https://github.com/apache/hadoop/pull/5341#issuecomment-1423724019 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 56s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 36s | | trunk passed | | +1 :green_heart: | compile | 25m 23s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 21m 41s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 6s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 39s | | trunk passed | | +1 :green_heart: | javadoc | 1m 7s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 28m 45s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 8s | | the patch passed | | +1 :green_heart: | compile | 24m 41s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 24m 41s | | the patch passed | | +1 :green_heart: | compile | 21m 55s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 21m 55s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 2s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | +1 :green_heart: | javadoc | 1m 0s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 26s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 4s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 52s | | The patch does not generate ASF License warnings. | | | | 232m 36s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5341 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 543f36505445 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a76b225ec0af3c4099a06bb0cdf6043a8420c27b | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/2/testReport/ | | Max. process+thread count | 1287 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5341/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"Shilun Fan","body":"I deleted 3.4.0 from the fix version. When PR is merged, we will set the fix version again."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5341: URL: https://github.com/apache/hadoop/pull/5341#issuecomment-3449071948 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5341: HADOOP-18614. Ensure that the config writers are closed URL: https://github.com/apache/hadoop/pull/5341"}]}
{"key":"HADOOP-18618","summary":"Support custom property for credential provider path","description":"Hadoop allows the configuration of a credential provider path through the property \"{*}hadoop.security.credential.provider.path{*}\", and the {{Configuration#getPassword()}} method retrieves the credentials from this provider. However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site.xml property file. To resolve this, all components should share a common credential provider path and add all their credentials. Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property \"{*}fs.azure.account.credential.provider.path..blob.core.windows.net{*}\". To accommodate this, the {{Configuration#getPassword()}} method should accept a custom property for the credential provider path and retrieve its value. The current default property can be overridden to achieve this. {code:java} public char[] getPassword(String name) throws IOException { ...... ...... } public char[] getPassword(String name, String providerKey) throws IOException { ...... ...... }{code} One Example is, Ambari [CustomServiceOrchestrator|https://github.com/apache/ambari/blob/trunk/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py#L312] service override the core-site.xml value for other component. This fix is very much needed for Ambari.","status":"Open","priority":"Minor","reporter":"Surendra Singh Lilhore","assignee":"Surendra Singh Lilhore","labels":["pull-request-available"],"project":"HADOOP","created":"2023-02-04T08:10:40.000+0000","updated":"2025-10-29T00:23:46.000+0000","comments":[{"author":"ASF GitHub Bot","body":"surendralilhore opened a new pull request, #5352: URL: https://github.com/apache/hadoop/pull/5352 Hadoop allows the configuration of a credential provider path through the property \"hadoop.security.credential.provider.path\", and the Configuration#getPassword() method retrieves the credentials from this provider. However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site.xml property file. To resolve this, all components should share a common credential provider path and add all their credentials. Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property \"fs.azure.account.credential.provider.path..blob.core.windows.net\". To accommodate this, the Configuration#getPassword() method should accept a custom property for the credential provider path and retrieve its value. The current default property can be overridden to achieve this. public char[] getPassword(String name) throws IOException { ...... ...... } public char[] getPassword(String name, String providerKey) throws IOException { ...... ...... }"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-1418207313 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 0s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 46m 2s | | trunk passed | | +1 :green_heart: | compile | 25m 24s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 21m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 7s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 40s | | trunk passed | | -1 :x: | javadoc | 1m 9s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 43s | | trunk passed | | +1 :green_heart: | shadedclient | 28m 18s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 11s | | the patch passed | | +1 :green_heart: | compile | 24m 39s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 24m 39s | | the patch passed | | +1 :green_heart: | compile | 21m 37s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 21m 37s | | the patch passed | | -1 :x: | blanks | 0m 0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/artifact/out/blanks-eol.txt) | The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply | | -0 :warning: | checkstyle | 1m 0s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 8 new + 117 unchanged - 0 fixed = 125 total (was 117) | | +1 :green_heart: | mvnsite | 1m 37s | | the patch passed | | -1 :x: | javadoc | 0m 59s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 41s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 57s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 12s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 53s | | The patch does not generate ASF License warnings. | | | | 230m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5352 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c0fb78311c1c 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6e7e232a37bfc4121200f28b185e883c61e2e8a4 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/testReport/ | | Max. process+thread count | 1375 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"Steve Loughran","body":"+[~lmccay@apache.org] Going near JCEKS and Configuration are both big changes, so a lot of thought is needed here. * What if a different engine actually wants to use a completely different credential provider mechanism, rather than just a path? * If you look at s3a, it has a custom option \"fs.s3a.security.credential.provider.path\", which is evaluated after the per bucket settings are evaluated. But that is (a) convoluted and (b) doesn't work in deployments where the base credential.provider.path has been locked down."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1097271398 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2405,6 +2405,30 @@ public char[] getPassword(String name) throws IOException { return pass; } + /** + * Get the value for a known password configuration element. + * In order to enable the elimination of clear text passwords in config, + * this method attempts to resolve the property name as an alias through + * the CredentialProvider API and conditionally fallsback to config. This + * method accept external provider property name. + * @param name property name + * @param providerKey provider property name + * @return password + * @throws IOException when error in fetching password + */ + public char[] getPassword(String name, String providerKey) Review Comment: this needs a better name to make clear that arg2 is a key, such as `getPasswordFromCredentialsOrConfig()` ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/alias/TestCredentialProviderFactory.java: ########## @@ -258,6 +259,51 @@ public void testLocalBCFKSProvider() { assertEquals(\"Can't create keystore\", exception.getMessage()); } + @Test + public void testCustomKeyProviderProperty() throws Exception { + Configuration conf = new Configuration(); + final String DEFAULT_CREDENTIAL_KEY = \"default.credential.key\"; + final char[] DEFAULT_CREDENTIAL_PASSWORD = { 'p', 'a', 's', 's', 'w', 'o', + 'r', 'd', '1', '2', '3' }; + + final String CUSTOM_CREDENTIAL_PROVIDER_KEY = + \"fs.cloud.storage.account.key.provider.path\"; + final String CUSTOM_CREDENTIAL_KEY = \"custom.credential.key\"; + final char[] CUSTOM_CREDENTIAL_PASSWORD = { 'c', 'u', 's', 't', 'o', 'm', '.', + 'p', 'a', 's', 's', 'w', 'o', 'r', 'd' }; + + // Set provider in default credential path property + createCredentialProviderPath(conf, \"default.jks\", + CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, + DEFAULT_CREDENTIAL_KEY, DEFAULT_CREDENTIAL_PASSWORD); + // Set provider in custom credential path property + createCredentialProviderPath(conf, \"custom.jks\", + CUSTOM_CREDENTIAL_PROVIDER_KEY, CUSTOM_CREDENTIAL_KEY, + CUSTOM_CREDENTIAL_PASSWORD); + + assertTrue(\"Password should match for default provider path\", Arrays.equals( + conf.getPassword(DEFAULT_CREDENTIAL_KEY), DEFAULT_CREDENTIAL_PASSWORD)); + + assertTrue(\"Password should match for custom provider path\", Arrays.equals( Review Comment: *never* use assertTrue for asserting on equality as the error messages are inadequate. Use assertJ for new code, assertEquals(expected, actual) for old stuff"},{"author":"Larry McCay","body":"Yes, this is a tricky area and in practice there are various ways that folks accommodate what you want. That said, extending Configuration.getPassword in a backward compatible way that does not leak cred provider implementation details into the interface should be fine."},{"author":"ASF GitHub Bot","body":"lmccay commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1098032159 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2405,6 +2405,30 @@ public char[] getPassword(String name) throws IOException { return pass; } + /** + * Get the value for a known password configuration element. + * In order to enable the elimination of clear text passwords in config, + * this method attempts to resolve the property name as an alias through + * the CredentialProvider API and conditionally fallsback to config. This + * method accept external provider property name. + * @param name property name + * @param providerKey provider property name + * @return password + * @throws IOException when error in fetching password + */ + public char[] getPassword(String name, String providerKey) Review Comment: This is an overloaded method with the same name and just an extra argument. I think keeping the same name here is fine as the change isn't adding the fallback to config semantics just the argument for providerKey so that it can be customized instead of use the default."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1101478817 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java: ########## @@ -72,8 +72,16 @@ public abstract CredentialProvider createProvider(URI providerName, public static List getProviders(Configuration conf ) throws IOException { + return getProviders(conf, CREDENTIAL_PROVIDER_PATH); + } + + public static List getProviders(Configuration conf, Review Comment: needs a javadoc ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2405,6 +2405,30 @@ public char[] getPassword(String name) throws IOException { return pass; } + /** + * Get the value for a known password configuration element. + * In order to enable the elimination of clear text passwords in config, + * this method attempts to resolve the property name as an alias through + * the CredentialProvider API and conditionally fallsback to config. This + * method accept external provider property name. + * @param name property name + * @param providerKey provider property name + * @return password + * @throws IOException when error in fetching password + */ + public char[] getPassword(String name, String providerKey) Review Comment: ok. consider this issue resolved. ########## hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/alias/TestCredentialProviderFactory.java: ########## @@ -258,6 +259,51 @@ public void testLocalBCFKSProvider() { assertEquals(\"Can't create keystore\", exception.getMessage()); } + @Test + public void testCustomKeyProviderProperty() throws Exception { + Configuration conf = new Configuration(); + final String DEFAULT_CREDENTIAL_KEY = \"default.credential.key\"; + final char[] DEFAULT_CREDENTIAL_PASSWORD = { 'p', 'a', 's', 's', 'w', 'o', + 'r', 'd', '1', '2', '3' }; + + final String CUSTOM_CREDENTIAL_PROVIDER_KEY = + \"fs.cloud.storage.account.key.provider.path\"; + final String CUSTOM_CREDENTIAL_KEY = \"custom.credential.key\"; + final char[] CUSTOM_CREDENTIAL_PASSWORD = { 'c', 'u', 's', 't', 'o', 'm', '.', + 'p', 'a', 's', 's', 'w', 'o', 'r', 'd' }; + + // Set provider in default credential path property + createCredentialProviderPath(conf, \"default.jks\", + CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, + DEFAULT_CREDENTIAL_KEY, DEFAULT_CREDENTIAL_PASSWORD); + // Set provider in custom credential path property + createCredentialProviderPath(conf, \"custom.jks\", + CUSTOM_CREDENTIAL_PROVIDER_KEY, CUSTOM_CREDENTIAL_KEY, + CUSTOM_CREDENTIAL_PASSWORD); + + assertTrue(\"Password should match for default provider path\", Arrays.equals( + conf.getPassword(DEFAULT_CREDENTIAL_KEY), DEFAULT_CREDENTIAL_PASSWORD)); + + assertTrue(\"Password should match for custom provider path\", Arrays.equals( + conf.getPassword(CUSTOM_CREDENTIAL_KEY, CUSTOM_CREDENTIAL_PROVIDER_KEY), + CUSTOM_CREDENTIAL_PASSWORD)); + } + + private void createCredentialProviderPath(Configuration conf, String jksName, Review Comment: does the file need cleaning up? if so the method should return it and a finally{} clause in the test method delete it"},{"author":"ASF GitHub Bot","body":"surendralilhore commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1102299179 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2405,6 +2405,30 @@ public char[] getPassword(String name) throws IOException { return pass; } + /** + * Get the value for a known password configuration element. + * In order to enable the elimination of clear text passwords in config, + * this method attempts to resolve the property name as an alias through + * the CredentialProvider API and conditionally fallsback to config. This + * method accept external provider property name. + * @param name property name + * @param providerKey provider property name + * @return password + * @throws IOException when error in fetching password + */ + public char[] getPassword(String name, String providerKey) Review Comment: Thank @lmccay and @steveloughran for review and suggestion. I have a suggestion, if you all agree to it. We will not modify the `Configuration#getPassword()` API. This JIRA is to provide an API to retrieve the credentials from an external provider path, rather than the one configured in `hadoop.security.credential.provider.path`. Can we simply pass the credential provider path in `Configuration#getPasswordFromCredentialProvider()` so that the name of the API makes it clear that it is reading the credentials from the specified provider path or from the default property `hadoop.security.credential.provider.path`? ``` /** * Read credential for name from provider path provided in common property * hadoop.security.credential.provider.path. */ public char[] getPasswordFromCredentialProvider(String name) throws IOException { } /** * Read credential for name from given provider path. */ public char[] getPasswordFromCredentialProvider(String name, String providerPath) throws IOException { } ``` I plan to use `Configuration#getPasswordFromCredentialProvider(name, providerPath)` in JIRA HADOOP-18626 to retrieve the key in the [SimpleKeyProvider](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java#L47) as below : ``` public String getStorageAccountKey(String accountName, Configuration conf) throws KeyProviderException { String key = null; try { ....... ....... String wasbKeyProvider = conf .get(getStorageAccountCredentialProviderPath(accountName)); char[] keyChars = null; if (wasbKeyProvider != null) { LOG.debug(\"Tying to get wasb key from configured provider path in \" + getStorageAccountCredentialProviderPath(accountName)); keyChars = c.getPasswordFromCredentialProvider( getStorageAccountKeyName(accountName), wasbKeyProvider); } ....... ....... return key; } ```"},{"author":"ASF GitHub Bot","body":"surendralilhore commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-1620023029 updated patch"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-1620400622 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 49m 56s | | trunk passed | | +1 :green_heart: | compile | 18m 33s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 56s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 40s | | trunk passed | | +1 :green_heart: | javadoc | 1m 12s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 39s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 37s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 57s | | the patch passed | | +1 :green_heart: | compile | 17m 41s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 41s | | the patch passed | | +1 :green_heart: | compile | 16m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 49s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 1m 12s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/2/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) | hadoop-common-project/hadoop-common: The patch generated 8 new + 117 unchanged - 0 fixed = 125 total (was 117) | | +1 :green_heart: | mvnsite | 1m 38s | | the patch passed | | -1 :x: | javadoc | 1m 8s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/2/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) | | +1 :green_heart: | javadoc | 0m 50s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 51s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 239m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5352 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 46b516257d86 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / d55b17e059d97c5e7a40650f50cab314bf021af7 | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/2/testReport/ | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1253054904 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2483,6 +2483,36 @@ public char[] getPasswordFromCredentialProviders(String name) return pass; } + /** + * Try and resolve the provided element name as a credential provider + * alias from the given provider path. + * @param name alias of the provisioned credential + * @param name credProviderPath path for credential provider + * @return password or null if not found + * @throws IOException when error in fetching password + */ + public char[] getPasswordFromCredentialProvider(String name, String credProviderPath) + throws IOException { + try { + CredentialProvider provider = CredentialProviderFactory.getProvider(this, + credProviderPath); + if (provider != null) { + try { + CredentialEntry entry = getCredentialEntry(provider, name); + if (entry != null) { + return entry.getCredential(); + } + } catch (IOException ioe) { + throw new IOException(\"Can't get key \" + name + \" from key provider\" Review Comment: this loses the original class. how about making NetUtils.wrapException() public and using it to generate an IOE of the same type, where possible. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java: ########## @@ -110,4 +110,31 @@ public static List getProviders(Configuration conf } return result; } + + /** + * Get CredentialProvider for hive provider path. + * + * @param conf configuration object + * @param credProviderPath provider path + * @return credProviderPath object + */ + public static CredentialProvider getProvider(Configuration conf, + String credProviderPath) throws IOException { Review Comment: have caller pass in the URI; makes it their problem to create that from a Path, string, etc. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java: ########## @@ -110,4 +110,31 @@ public static List getProviders(Configuration conf } return result; } + + /** + * Get CredentialProvider for hive provider path. Review Comment: nit: not just hive ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2483,6 +2483,36 @@ public char[] getPasswordFromCredentialProviders(String name) return pass; } + /** + * Try and resolve the provided element name as a credential provider + * alias from the given provider path. + * @param name alias of the provisioned credential + * @param name credProviderPath path for credential provider + * @return password or null if not found + * @throws IOException when error in fetching password + */ + public char[] getPasswordFromCredentialProvider(String name, String credProviderPath) Review Comment: have caller pass in the URI; makes it their problem to create that from a Path, string, etc. change name appropriately"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-1622577718 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 51m 54s | | trunk passed | | +1 :green_heart: | compile | 18m 33s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 16m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 14s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 38s | | trunk passed | | +1 :green_heart: | javadoc | 1m 14s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 48s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 40m 41s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 17m 43s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 43s | | the patch passed | | +1 :green_heart: | compile | 16m 50s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 16m 50s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 35s | | the patch passed | | -1 :x: | javadoc | 1m 8s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/3/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 39m 52s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 50s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 58s | | The patch does not generate ASF License warnings. | | | | 241m 19s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5352 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1bc6e15dea92 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6c83999a3429409b29550ca1d6d0c3724e6a9b7a | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/3/testReport/ | | Max. process+thread count | 1727 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-1625717179 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 56m 51s | | trunk passed | | +1 :green_heart: | compile | 23m 18s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 22m 4s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 1m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 55s | | trunk passed | | +1 :green_heart: | javadoc | 1m 30s | | trunk passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 52s | | trunk passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 3m 3s | | trunk passed | | +1 :green_heart: | shadedclient | 45m 48s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 58s | | the patch passed | | +1 :green_heart: | compile | 17m 49s | | the patch passed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 17m 49s | | the patch passed | | +1 :green_heart: | compile | 17m 7s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 17m 7s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 36s | | the patch passed | | -1 :x: | javadoc | 1m 8s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/4/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1. | | +1 :green_heart: | javadoc | 0m 49s | | the patch passed with JDK Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 2m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 20m 26s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 266m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5352 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1cfb36abbd3d 4.15.0-212-generic #223-Ubuntu SMP Tue May 23 13:09:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 27babb4d8aed95ff22853b97f190bca78e290b6a | | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.19+7-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/4/testReport/ | | Max. process+thread count | 1253 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5352/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5352: URL: https://github.com/apache/hadoop/pull/5352#discussion_r1257309280 ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java: ########## @@ -2483,6 +2483,38 @@ public char[] getPasswordFromCredentialProviders(String name) return pass; } + /** + * Try and resolve the provided element name as a credential provider + * alias from the given provider path. + * @param name alias of the provisioned credential + * @param providerURI The URI of the provider path + * @return password or null if not found + * @throws IOException when error in fetching password + */ + public char[] getPasswordFromCredentialProvider(String name, URI providerUri) + throws IOException { + try { + CredentialProvider provider = CredentialProviderFactory.getProvider(this, + providerUri); + if (provider != null) { + try { + CredentialEntry entry = getCredentialEntry(provider, name); + if (entry != null) { + return entry.getCredential(); + } + } catch (IOException ioe) { + String msg = String.format( + \"Can't get key %s from key provider of type: %s.\", name, + provider.getClass().getName()); + throw NetUtils.wrapWithMessage(ioe, msg); + } + } + } catch (IOException ioe) { Review Comment: this will ignore the work done on L2509; really wrapWithMessage() should be invoked for all, and exactly once. ########## hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java: ########## @@ -110,4 +110,26 @@ public static List getProviders(Configuration conf } return result; } + + /** + * Get the CredentialProvider for a given provider URI. + * + * @param conf The configuration object + * @param providerURI The URI of the provider path + * @return The CredentialProvider + * @throws IOException If an I/O error occurs + */ + public static CredentialProvider getProvider(Configuration conf, + URI providerUri) throws IOException { + synchronized (serviceLoader) { + for (CredentialProviderFactory factory : serviceLoader) { + CredentialProvider kp = factory.createProvider(providerUri, conf); + if (kp != null) { + return kp; + } + } + } + throw new IOException( Review Comment: PathIOException with path as URI.toString or FileNotFoundException"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5352: URL: https://github.com/apache/hadoop/pull/5352#issuecomment-3449071848 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5352: HADOOP-18618 : Support custom property for credential provider. URL: https://github.com/apache/hadoop/pull/5352"}]}
{"key":"HADOOP-17377","summary":"ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service","description":"*Summary* The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling] In particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability when running multiple processes on the same host. *Environment* * Spark talking to an ABFS store * Hadoop 3.2.1 * Running on an Azure VM with user-assigned identity, ABFS configured to use MsiTokenProvider * 6 executor processes on each VM *Example* Here's an example error message and stack trace. It's always the same stack trace. This appears in logs a few hundred to low thousands of times a day. It's luckily skating by since the download operation is wrapped in 3 retries. {noformat} AADToken: HTTP connection failed for getting token from AzureAD. Http response: 429 null Content-Type: application/json; charset=utf-8 Content-Length: 90 Request ID: Proxies: none First 1K of Body: {\"error\":\"invalid_request\",\"error_description\":\"Temporarily throttled, too many requests\"} at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437) at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717) at org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:747) at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:724) at org.apache.spark.util.Utils$.fetchFile(Utils.scala:496) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7(Executor.scala:812) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7$adapted(Executor.scala:803) at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792) at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149) at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237) at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44) at scala.collection.mutable.HashMap.foreach(HashMap.scala:149) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791) at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748){noformat} CC [~mackrorysd], [~stevel@apache.org]","status":"Open","priority":"Major","reporter":"Brandon","labels":["pull-request-available"],"project":"HADOOP","created":"2020-11-13T03:16:31.000+0000","updated":"2025-10-29T06:51:43.000+0000","comments":[{"author":"Steve Loughran","body":"[~snvijaya]"},{"author":"Brandon","body":"Another note. Very rarely, I've also seen HTTP 410 errors from the Instance Metadata Service. ABFS currently doesn't retry those. Azure documentation suggests 410 and 500 response codes should be retried: [https://docs.microsoft.com/en-in/azure/virtual-machines/linux/instance-metadata-service?tabs=windows#errors-and-debugging] Here's the full error message and stack trace for reference: {noformat} AADToken: HTTP connection failed for getting token from AzureAD. Http response: 410 Gone Content-Type: text/html Content-Length: 35 Request ID: Proxies: none First 1K of Body: The page you requested was removed. at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437) at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717){noformat}"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370941112 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 14s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 13s | | trunk passed | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 42s | | trunk passed | | -1 :x: | javadoc | 0m 39s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 42s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 35s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 21s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 36s | | the patch passed | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 16s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 0s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 103m 38s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6b243f21df19 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/testReport/ | | Max. process+thread count | 627 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370942634 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 54s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 42m 33s | | trunk passed | | +1 :green_heart: | compile | 0m 42s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 18s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 39s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 35s | | the patch passed | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 14s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 25s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 103m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ab9041391762 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 56c6f7de6ead9500d2c449ba768d2301fc7e4d85 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/testReport/ | | Max. process+thread count | 536 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1370943150 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 9s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 36s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 34s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 46s | | trunk passed | | -1 :x: | javadoc | 0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 19m 55s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 1s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 39s | | The patch does not generate ASF License warnings. | | | | 94m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 6f4158513412 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2c25b39b727a259ca0feb754705adbcfe5e80331 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/testReport/ | | Max. process+thread count | 694 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371868477 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 11s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 35s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | -1 :x: | javadoc | 0m 40s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 14s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 29s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 40s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 18s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 33s | | the patch passed | | -1 :x: | javadoc | 0m 24s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 93m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7099b9bc66b4 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/testReport/ | | Max. process+thread count | 560 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1371873722 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 47s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 39m 22s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 12s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 31s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 16s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 5s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 24s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 99m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 8916fc660cd7 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3be0b00a59ca2e0286bdd44dbb20e3d0f76b8d4d | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/testReport/ | | Max. process+thread count | 586 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1372011908 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 33s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 38s | | trunk passed | | -1 :x: | javadoc | 0m 36s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 10s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 38s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 27s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 27s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 30s | | the patch passed | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 3s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 2s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. | | | | 102m 31s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7d84852c9f33 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / e04011ca7ca13cb0ab3146ee54e5c9bbf7aefddb | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/testReport/ | | Max. process+thread count | 534 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1063226881 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount = HttpURLConnection.HTTP_BAD_REQUEST) { throw new AbfsRestOperationException(result.getStatusCode(), result.getStorageErrorCode(), result.getStorageErrorMessage(), null, result); } ``` would throw exception."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1064481413 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT + || statusCode == HttpURLConnection.HTTP_GONE + || statusCode == HTTP_TOO_MANY_REQUESTS Review Comment: Removing from ExponentialRetryPolicy, we can have https://github.com/apache/hadoop/pull/5273/files#diff-dff9c93d1668203c206aa1c092aef9d2921dc6e20af8888d06fae34778991531R320-R321 as ``` !succeeded && isRecoverableFailure && (tokenFetchRetryPolicy.shouldRetry(retryCount, httperror)||httpError==429 ||httpError==410); ``` reason being, this check is only required in ADAuthenticator."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1066593607 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -128,6 +138,8 @@ public boolean shouldRetry(final int retryCount, final int statusCode) { return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT + || statusCode == HttpURLConnection.HTTP_GONE + || statusCode == HTTP_TOO_MANY_REQUESTS Review Comment: We plan to retry for these status codes, if they come as a response from backend as well. Hence adding these into a centralized exponential retry policy class."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1494098831 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 1s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 23s | | trunk passed | | +1 :green_heart: | compile | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | compile | 0m 38s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | checkstyle | 0m 32s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 44s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 34s | | trunk passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 37s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 20m 56s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | compile | 0m 29s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | javac | 0m 29s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 19s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 34s | | the patch passed | | +1 :green_heart: | javadoc | 0m 23s | | the patch passed with JDK Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 | | +1 :green_heart: | javadoc | 0m 24s | | the patch passed with JDK Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 9s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 37s | | The patch does not generate ASF License warnings. | | | | 94m 6s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | JIRA Issue | HADOOP-17377 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 78e6cbb88986 4.15.0-206-generic #217-Ubuntu SMP Fri Feb 3 19:10:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 62834104a7e75dfb14c6774e9ef4dfc6e803e10e | | Default Java | Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.18+10-post-Ubuntu-0ubuntu120.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/testReport/ | | Max. process+thread count | 559 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1164119014 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ org.mockito mockito-core + 4.11.0 Review Comment: sorry, hadoop-project defines the version, and through properties. revert this ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ org.mockito mockito-core + 4.11.0 test + + + org.mockito + mockito-inline + 4.11.0 Review Comment: if this is new to hadoop, declare it in hadoop-project/pom.xml, with versions and exclusions, then declare here without those ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -40,13 +47,16 @@ import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT; +import static org.mockito.Mockito.times; /** * Test MsiTokenProvider. */ public final class ITestAbfsMsiTokenProvider extends AbstractAbfsIntegrationTest { + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: refer to the value in the src/main code ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on Review Comment: needs a . in it, maybe split into \"qualifies for retry.\" and \"see...\" ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, Review Comment: what if these are undefined? skip the test? ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception Review Comment: cut this @throws ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java: ########## @@ -341,7 +341,7 @@ private static boolean isRecoverableFailure(IOException e) { || e instanceof FileNotFoundException); } - private static AzureADToken getTokenSingleCall(String authEndpoint, + public static AzureADToken getTokenSingleCall(String authEndpoint, Review Comment: now it is public, add a javadoc ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on + * https://learn.microsoft.com/en-us/azure/active-directory/ + * managed-identities-azure-resources/how-to-use-vm-token#error-handling + */ + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: make public and refer from tests, maybe put in a different file for this"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687757407 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 12s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 16s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 29s | | The patch does not generate ASF License warnings. | | | | 95m 37s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux d2957048b6f1 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cf73a70fc72b27a28f18058b8399784e3d7e891e | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/testReport/ | | Max. process+thread count | 554 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1687925288 @anmolanmol1234 still need those (minor) changes -otherwise it is ready to merge"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1301867025 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +100,109 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + * @throws Exception + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, Review Comment: these are hardcoded here, no need for skipping"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1302010099 ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsHandlerImpl.java: ########## @@ -192,7 +192,7 @@ public void testMountController() throws IOException { assertTrue(\"cgroup dir should be cerated\", cgroup.mkdirs()); //Since we enabled (deferred) cgroup controller mounting, no interactions //should have occurred, with this mock - verifyZeroInteractions(privilegedOperationExecutorMock); + Mockito.verifyNoInteractions(privilegedOperationExecutorMock); Review Comment: use static import for consistency with the others. ########## hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java: ########## @@ -24,7 +24,6 @@ import static org.junit.Assert.assertNotNull; import static org.junit.Assert.assertTrue; import static org.junit.Assert.fail; -import static org.mockito.Matchers.any; Review Comment: reinstate so this file doesn't change ########## hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java: ########## @@ -423,7 +423,7 @@ public void testSubclusterDown() throws Exception { FSNamesystem ns0 = nn0.getNamesystem(); HAContext nn0haCtx = (HAContext)getInternalState(ns0, \"haContext\"); HAContext mockCtx = mock(HAContext.class); - doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(any()); + doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(Mockito.any()); Review Comment: return to the existing any() static import ########## hadoop-project/pom.xml: ########## @@ -1308,7 +1308,20 @@ org.mockito mockito-core - 2.28.2 + 4.11.0 Review Comment: add a new property mockito.version and reference in both places ########## hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/TestGpuResourceAllocator.java: ########## @@ -210,7 +210,7 @@ private void assertAllocatedGpus(int gpus, int deniedGpus, private void assertNoAllocation(GpuAllocation allocation) { assertEquals(1, allocation.getDeniedGPUs().size()); assertEquals(0, allocation.getAllowedGPUs().size()); - verifyZeroInteractions(nmStateStore); + Mockito.verifyNoInteractions(nmStateStore); Review Comment: use static import"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689282173 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 16 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 14m 41s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 19m 48s | | trunk passed | | +1 :green_heart: | compile | 10m 41s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 9m 40s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 2m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 8m 56s | | trunk passed | | +1 :green_heart: | javadoc | 6m 53s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 6m 22s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +0 :ok: | spotbugs | 0m 19s | | branch/hadoop-project no spotbugs output file (spotbugsXml.xml) | | +1 :green_heart: | shadedclient | 21m 15s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 35s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 33s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 5m 35s | | the patch passed | | +1 :green_heart: | compile | 10m 1s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 10m 1s | | the patch passed | | +1 :green_heart: | compile | 9m 39s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 9m 39s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 2m 24s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/results-checkstyle-root.txt) | root: The patch generated 3 new + 86 unchanged - 0 fixed = 89 total (was 86) | | +1 :green_heart: | mvnsite | 8m 48s | | the patch passed | | -1 :x: | javadoc | 0m 27s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 25s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +0 :ok: | spotbugs | 0m 20s | | hadoop-project has no data from spotbugs | | -1 :x: | shadedclient | 21m 14s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 22s | | hadoop-project in the patch passed. | | +1 :green_heart: | unit | 16m 5s | | hadoop-common in the patch passed. | | +1 :green_heart: | unit | 192m 15s | | hadoop-hdfs in the patch passed. | | +1 :green_heart: | unit | 86m 6s | | hadoop-yarn-server-resourcemanager in the patch passed. | | -1 :x: | unit | 22m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) | hadoop-yarn-server-nodemanager in the patch passed. | | +1 :green_heart: | unit | 19m 25s | | hadoop-hdfs-rbf in the patch passed. | | +1 :green_heart: | unit | 20m 26s | | hadoop-yarn-services-core in the patch passed. | | -1 :x: | unit | 208m 56s | [/patch-unit-hadoop-yarn-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/patch-unit-hadoop-yarn-project.txt) | hadoop-yarn-project in the patch passed. | | +1 :green_heart: | unit | 2m 26s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 1m 8s | | The patch does not generate ASF License warnings. | | | | 778m 13s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime | | | hadoop.yarn.server.nodemanager.containermanager.linux.runtime.TestDockerContainerRuntime | | | hadoop.yarn.client.TestRMFailover | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint | | uname | Linux 8b3e21c22fc4 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 0688809c43766c8d001a39c972e263d102b2df82 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/testReport/ | | Max. process+thread count | 3325 (vs. ulimit of 5500) | | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core hadoop-yarn-project hadoop-tools/hadoop-azure U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/9/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689324456 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/10/console | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689328770 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 18s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/11/console | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689330087 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 17s | | https://github.com/apache/hadoop/pull/5273 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/12/console | | versions | git=2.17.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689440680 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 52s | | trunk passed | | +1 :green_heart: | compile | 0m 29s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 24s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 53s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 15s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 33s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 11s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 25s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 10s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 12s | | ASF License check generated no output? | | | | 68m 5s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 1b921f5b2a8a 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/testReport/ | | Max. process+thread count | 625 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/13/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1689442903 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 32m 58s | | trunk passed | | +1 :green_heart: | compile | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 9s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 26s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 10s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | compile | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 11s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 9s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 10s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 10s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 11s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 11s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 27s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 11s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 12s | | ASF License check generated no output? | | | | 68m 8s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 5b0ba2fb9772 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8b5e8839d108e3d02d0b1b0f872701bd79d8354c | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/testReport/ | | Max. process+thread count | 552 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/14/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1697434060 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 32s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 35m 9s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | +1 :green_heart: | compile | 0m 28s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 26s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 33s | | trunk passed | | +1 :green_heart: | javadoc | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 53s | | trunk passed | | -1 :x: | shadedclient | 21m 27s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 15s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | -1 :x: | shadedclient | 21m 21s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 53s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 29s | | The patch does not generate ASF License warnings. | | | | 90m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 04bde9dbd212 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ba573ff39a6fa1ac0a78391d345bd2219fc9c97 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/testReport/ | | Max. process+thread count | 412 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/15/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1698648099 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 29s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 34m 36s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | -1 :x: | shadedclient | 25m 29s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 15s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 19s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | -1 :x: | shadedclient | 24m 24s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 47s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 96m 43s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux e4005f0bafa0 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 462a3b6a0c7aadefe39960218092651bad10c979 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/testReport/ | | Max. process+thread count | 455 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/16/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310529985 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( Review Comment: this kind of stuff is trouble as it makes maintenance a nightmare; you can't see where the field is access, all you have is a mocking test failing. proposed: add a static setter to AzureADAuthenticator; mark as @VisibleForTesting."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310534958 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( + \"tokenFetchRetryPolicy\"); + tokenFetchRetryPolicy.setAccessible(true); + tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class, + exponentialRetryPolicy); + + AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint, + tenantGuid, clientId, authority); + AzureADToken token = null; + intercept(AzureADAuthenticator.HttpException.class, + tokenProvider::getToken); + + // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends. + // It being called multiple times verifies that the retry was done for the throttling status code 429. + Mockito.verify(exponentialRetryPolicy, Review Comment: so ExponentialRetryPolicy.getRetryCount() is there to let you pass a non-mocked policy in and then assert on it. how about using that here? it probably needs making the accessors public, rather than package scoped, but that's all. The less use we make of mockito, the less things will break with every mockito upgrade"},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1310523986 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ org.mockito mockito-core + 4.11.0 Review Comment: again, cut this now; the version in hadoop project is the one you now expect"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1701148507 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 35m 51s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | checkstyle | 0m 25s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 32s | | trunk passed | | +1 :green_heart: | javadoc | 0m 32s | | trunk passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | spotbugs | 0m 50s | | trunk passed | | -1 :x: | shadedclient | 27m 31s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 27m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 28s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04. | | -1 :x: | javadoc | 0m 21s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | -1 :x: | shadedclient | 26m 8s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 1m 54s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 31s | | The patch does not generate ASF License warnings. | | | | 102m 59s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 7a1812c7f588 4.15.0-213-generic #224-Ubuntu SMP Mon Jun 19 13:30:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 78329dece95a9e2d53d4f523a95c3325b09a2d6d | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20+8-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/testReport/ | | Max. process+thread count | 455 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/17/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"Agnes Tevesz","body":"[~stevel@apache.org] [~brandonvin] Can you help to move this change forward? Who should be the owner of this task? The ticket is not assigned to anyone and there is no activity on the change since end of August. This fix should land in hadoop. The pod identity in azure was deprecated: [https://github.com/Azure/aad-pod-identity] If we get the token directly from the instance metadata service we hit this HTTP 429 issue with tpcds tests very frequently: [https://azure.github.io/azure-workload-identity/docs/] The pod identity component most likely provided the retry logic before, but we cannot install depreciated components on an AKS cluster. Can this change get finished?"},{"author":"ASF GitHub Bot","body":"nandorKollar commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1808268461 I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1809005084 I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix. However, that PR to update mockito bounced, so either 1. another attempt is made to update mockito, including the shaded client 2. this PR can be done without updating mockito (easier)"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811834929 > I'll go with whatever @saxenapranav thinks here...we have seen this ourselves and need a fix. > > However, that PR to update mockito bounced, so either > > 1. another attempt is made to update mockito, including the shaded client > 2. this PR can be done without updating mockito (easier) The mockito upgrade was needed as part of this PR to mock static methods. So would it be fine if we remove that test method or if not I will attempt to upgrade mockito, including the shaded client."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811839555 > I think this this PR is great, however there's still one related open problem: the default values (2) for `fs.azure.oauth.token.fetch.retry.delta.backoff` is incorrect. The value of 2 is consistent with MS recommendation (https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/how-to-use-vm-token#retry-guidance), but it is assumed in **seconds**, but as this is used in Thread.sleep [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L326), it will be measured in **milliseconds**. I think we should change the default to 2000. @steveloughran @anmolanmol1234 do you think we can implement this minimal change in this PR, or we should open a separate one? Will update this change as an iteration of this PR, but will some time for the mockito upgrade PR."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811896245 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -0 :warning: | checkstyle | 0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 22s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 21s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 21s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 2m 21s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 2m 42s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 20s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 20s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 20s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 3m 53s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 20s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 20s | | ASF License check generated no output? | | | | 12m 10s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 58c5a67f711c 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b0563a19157c485d782faa96d8300020c408ea8e | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/testReport/ | | Max. process+thread count | 24 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/18/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-1811906117 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 0m 20s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 20s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in trunk failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -0 :warning: | checkstyle | 0m 18s | [/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/buildtool-branch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 20s | [/branch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | -1 :x: | javadoc | 0m 20s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | javadoc | 4m 44s | | trunk passed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in trunk failed. | | +1 :green_heart: | shadedclient | 7m 10s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 7m 32s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | javac | 0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 19s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 24s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 9s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_382-8u382-ga-1~20.04.1-b05.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05. | | -1 :x: | spotbugs | 0m 9s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | shadedclient | 2m 52s | | patch has errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 9s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 11s | | ASF License check generated no output? | | | | 15m 33s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.43 ServerAPI=1.43 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 5348f26896a5 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 225541ef7a765d856eb28966b7340744fe049654 | | Default Java | Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.20.1+1-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_382-8u382-ga-1~20.04.1-b05 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/testReport/ | | Max. process+thread count | 88 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/19/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r1419336218 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -323,6 +323,13 @@ mockito-core test + + Review Comment: is this needed? because its not in the base project pom. I would rather this PR doesn't need that mockito upgrade as mockito upgrades are always a painful piece of work which never gets backported. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -48,7 +48,7 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; - public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000; Review Comment: use 2_000"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3432466591 @anujmodi2021 can you revisit this so I can get it in? we do appear to have been using it internally since december 2023, so I'm happy it works."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3435471034 @steveloughran will backport the PR for merge"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437618030 fun test run today, against s3 london. Most of the multipart upload/commit tests were failing \"missing part\", from cli or IDE. Testing with S3 express was happy. (`-Dparallel-tests -DtestsThreadCount=8 -Panalytics -Dscale`) ``` [ERROR] ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src [ERROR] ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276  AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1) [ERROR] ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433  FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [ERROR] ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414  FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src [INFO] [ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13 [INFO] ``` This has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437685484 When these uploads fail we do leave incomplete uploads in progress: ``` Listing uploads under path \"\" job-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98 job-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 job-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV job-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh job-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC job-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw-- job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq test/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA-- test/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA-- Total 10 uploads found. ``` Most interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. The attempt to complete failed. ``` [ERROR] ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:380->createFileWithFlags:190  AWSBadRequest Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found. The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1) ``` Yet the uploads list afterwards finds it ``` job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4 ``` I have to conclude that the list of pending uploads was briefly offline/inconsistent. This is presumably so, so rare that there's almost no point retrying here. With no retries, every active write/job would have failed, even though the system had recovered within a minute. Maybe we should retry here? I remember a long long time ago the v1 sdk didn't retry on failures of the final POST to commit an upload, and how that sporadically caused problems. Retrying on MPU failures will allow for recovery in the presence of a transient failure here, and the cost of \"deletion of all pending uploads will take longer to fail all active uploads\"."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3437730498 sorry, commenting on wrong PR. will cut."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438000237 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 57s | | trunk passed | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 51s | | trunk passed | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 35s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 30m 20s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 30m 40s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 35s | | the patch passed | | +1 :green_heart: | compile | 0m 34s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 34s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 1m 26s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 2m 58s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 31s | | The patch does not generate ASF License warnings. | | | | 116m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle | | uname | Linux 8088428d1bb5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c7d46718136c7e1585bcd3f1d82becc8446f0b50 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/testReport/ | | Max. process+thread count | 611 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/20/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456024943 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ org.mockito mockito-core + 4.11.0 Review Comment: taken ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -321,8 +321,23 @@ org.mockito mockito-core + 4.11.0 test + + + org.mockito + mockito-inline + 4.11.0 Review Comment: removed dependency"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456032575 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java: ########## @@ -58,6 +58,13 @@ public class ExponentialRetryPolicy { */ private static final double MAX_RANDOM_RATIO = 1.2; + /** + * Qualifies for retry based on + * https://learn.microsoft.com/en-us/azure/active-directory/ + * managed-identities-azure-resources/how-to-use-vm-token#error-handling + */ + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: taken ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -40,13 +47,16 @@ import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT; import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT; +import static org.mockito.Mockito.times; /** * Test MsiTokenProvider. */ public final class ITestAbfsMsiTokenProvider extends AbstractAbfsIntegrationTest { + private static final int HTTP_TOO_MANY_REQUESTS = 429; Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456039983 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456041107 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsMsiTokenProvider.java: ########## @@ -90,4 +99,55 @@ private String getTrimmedPasswordString(AbfsConfiguration conf, String key, return value.trim(); } + /** + * Test to verify that token fetch is retried for throttling errors (too many requests 429). + */ + @Test + public void testRetryForThrottling() throws Exception { + AbfsConfiguration conf = getConfiguration(); + + // Exception to be thrown with throttling error code 429. + AzureADAuthenticator.HttpException httpException + = new AzureADAuthenticator.HttpException(HTTP_TOO_MANY_REQUESTS, + \"abc\", \"abc\", \"abc\", \"abc\", \"abc\"); + + String tenantGuid = \"abcd\"; + String clientId = \"abcd\"; + String authEndpoint = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT); + String authority = getTrimmedPasswordString(conf, + FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, + DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY); + + // Mock the getTokenSingleCall to throw exception so the retry logic comes into place. + try (MockedStatic adAuthenticator = Mockito.mockStatic( + AzureADAuthenticator.class, Mockito.CALLS_REAL_METHODS)) { + adAuthenticator.when( + () -> AzureADAuthenticator.getTokenSingleCall(Mockito.anyString(), + Mockito.anyString(), Mockito.any(), Mockito.anyString(), + Mockito.anyBoolean())).thenThrow(httpException); + + // Mock the tokenFetchRetryPolicy to verify retries. + ExponentialRetryPolicy exponentialRetryPolicy = Mockito.spy( + conf.getOauthTokenFetchRetryPolicy()); + Field tokenFetchRetryPolicy = AzureADAuthenticator.class.getDeclaredField( + \"tokenFetchRetryPolicy\"); + tokenFetchRetryPolicy.setAccessible(true); + tokenFetchRetryPolicy.set(ExponentialRetryPolicy.class, + exponentialRetryPolicy); + + AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint, + tenantGuid, clientId, authority); + AzureADToken token = null; + intercept(AzureADAuthenticator.HttpException.class, + tokenProvider::getToken); + + // If the status code doesn't qualify for retry shouldRetry returns false and the loop ends. + // It being called multiple times verifies that the retry was done for the throttling status code 429. + Mockito.verify(exponentialRetryPolicy, Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456045985 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -48,7 +48,7 @@ public final class FileSystemConfigurations { public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS = 5; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL = 0; public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS; - public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2; + public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2 * 1000; Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #5273: URL: https://github.com/apache/hadoop/pull/5273#discussion_r2456046384 ########## hadoop-tools/hadoop-azure/pom.xml: ########## @@ -323,6 +323,13 @@ mockito-core test + + Review Comment: removed dependency"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438376772 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 48s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 16m 49s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 17m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 16s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 0m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 16m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 12s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 69m 12s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux df85de01587f 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/testReport/ | | Max. process+thread count | 613 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/21/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3438560491 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 35s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 18s | | trunk passed | | +1 :green_heart: | compile | 0m 45s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 31s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 50s | | trunk passed | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 1m 26s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 27m 54s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 28m 14s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 20s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 40s | | the patch passed | | -1 :x: | javadoc | 0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | spotbugs | 1m 23s | | the patch passed | | +1 :green_heart: | shadedclient | 28m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 55s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 35s | | The patch does not generate ASF License warnings. | | | | 110m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5273 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c145c3f5b775 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fd260666ade501f736fe7af3476e96f6231f2f0b | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/testReport/ | | Max. process+thread count | 629 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5273/22/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anujmodi2021 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441679165 Thanks @anmolanmol1234 for refreshing up this. @steveloughran this LGTM now and should be ready to merge. The spotbugs adnd javadoc warnings are due to https://issues.apache.org/jira/browse/HADOOP-19731"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3441710243 ------------------------------ :::: AGGREGATED TEST RESULT :::: ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 169 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 715, Failures: 0, Errors: 0, Skipped: 282 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 873, Failures: 0, Errors: 0, Skipped: 228 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 722, Failures: 0, Errors: 0, Skipped: 140 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 284 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 719, Failures: 0, Errors: 0, Skipped: 152 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 198 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 747, Failures: 0, Errors: 0, Skipped: 226 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 194, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 712, Failures: 0, Errors: 0, Skipped: 281 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #5273: URL: https://github.com/apache/hadoop/pull/5273#issuecomment-3460088218 Hi @steveloughran requesting your review on this PR"}]}
{"key":"HADOOP-18594","summary":"ProxyUserAuthenticationFilter add properties 'hadoop.security.impersonation.provider.class'  to enable  load custom ImpersonationProvider class when start namenode","description":"h3. h3. the phenomenon I made a custom ImpersonationProvider class and configured in core-site.xml {code:none} hadoop.security.impersonation.provider.class org.apache.hadoop.security.authorize.MyImpersonationProvider {code} {color:#ff0000}However, when start namenode, MyImpersonationProvider could't be load automatically, but DefaultImpersonationProvider is loaded.{color} When execute the following command, custom ImpersonationProvider could be load. {code:java} bin/hdfs dfsadmin -refreshSuperUserGroupsConfiguration{code} h3. h3. what I see else custom ImpersonationProvider was load in org.apache.hadoop.security.authorize.ProxyUsers#refreshSuperUserGroupsConfiguration through the property \"hadoop.security.impersonation.provider.class\" [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/ProxyUsers.java#L70] {code:java} public static void refreshSuperUserGroupsConfiguration(Configuration conf, String proxyUserPrefix) { Preconditions.checkArgument(proxyUserPrefix != null && !proxyUserPrefix.isEmpty(), \"prefix cannot be NULL or empty\"); // sip is volatile. Any assignment to it as well as the object's state // will be visible to all the other threads. ImpersonationProvider ip = getInstance(conf); ip.init(proxyUserPrefix); sip = ip; ProxyServers.refresh(conf); } private static ImpersonationProvider getInstance(Configuration conf) { Class clazz = conf.getClass( CommonConfigurationKeysPublic.HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS, DefaultImpersonationProvider.class, ImpersonationProvider.class); return ReflectionUtils.newInstance(clazz, conf); }{code} when namenode start, refreshSuperUserGroupsConfiguration was called in ProxyUserAuthenticationFilter, [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java#L56] {code:java} public void init(FilterConfig filterConfig) throws ServletException { Configuration conf = getProxyuserConfiguration(filterConfig); ProxyUsers.refreshSuperUserGroupsConfiguration(conf, PROXYUSER_PREFIX); super.init(filterConfig); } {code} here is the stack trace {code:none} init:70, DefaultImpersonationProvider (org.apache.hadoop.security.authorize) refreshSuperUserGroupsConfiguration:77, ProxyUsers (org.apache.hadoop.security.authorize) init:56, ProxyUserAuthenticationFilter (org.apache.hadoop.security.authentication.server) initialize:140, FilterHolder (org.eclipse.jetty.servlet) lambda$initialize$0:731, ServletHandler (org.eclipse.jetty.servlet) accept:-1, 1541075662 (org.eclipse.jetty.servlet.ServletHandler$$Lambda$36) forEachRemaining:948, Spliterators$ArraySpliterator (java.util) forEachRemaining:742, Streams$ConcatSpliterator (java.util.stream) forEach:580, ReferencePipeline$Head (java.util.stream) initialize:755, ServletHandler (org.eclipse.jetty.servlet) startContext:379, ServletContextHandler (org.eclipse.jetty.servlet) doStart:910, ContextHandler (org.eclipse.jetty.server.handler) doStart:288, ServletContextHandler (org.eclipse.jetty.servlet) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) start:423, Server (org.eclipse.jetty.server) doStart:110, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) doStart:387, Server (org.eclipse.jetty.server) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:1276, HttpServer2 (org.apache.hadoop.http) start:170, NameNodeHttpServer (org.apache.hadoop.hdfs.server.namenode) startHttpServer:954, NameNode (org.apache.hadoop.hdfs.server.namenode) initialize:765, NameNode (org.apache.hadoop.hdfs.server.namenode) :1020, NameNode (org.apache.hadoop.hdfs.server.namenode) :995, NameNode (org.apache.hadoop.hdfs.server.namenode) createNameNode:1769, NameNode (org.apache.hadoop.hdfs.server.namenode) main:1834, NameNode (org.apache.hadoop.hdfs.server.namenode) {code} {color:#ff0000}but the filterConfig in ProxyUserAuthenticationFilter did't contains properties ''hadoop.security.impersonation.provider.class''{color} filterConfig in ProxyUserAuthenticationFilter is controled by ProxyUserAuthenticationFilterInitializer or AuthFilterInitializer filterConfig only put property which start with \"hadoop.proxyuser\", but not put \"hadoop.security.impersonation.provider.class\" {code:java} protected Map createFilterConfig(Configuration conf) { Map filterConfig = AuthenticationFilterInitializer .getFilterConfigMap(conf, configPrefix); //Add proxy user configs for (Map.Entry entry : conf.getPropsWithPrefix( ProxyUsers.CONF_HADOOP_PROXYUSER).entrySet()) { filterConfig.put(\"proxyuser\" + entry.getKey(), entry.getValue()); } return filterConfig; } {code} [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java#L46] [https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/AuthFilterInitializer.java#L46] it leads to custome ImpersonationProvider can't be load during namenode start.","status":"Open","priority":"Major","reporter":"Xie Yi","labels":["pull-request-available"],"project":"HADOOP","created":"2023-01-16T10:06:37.000+0000","updated":"2025-10-30T00:23:25.000+0000","comments":[{"author":"Xie Yi","body":"Could we add the property \"hadoop.security.impersonation.provider.class\" in ProxyUserAuthenticationFilterInitializer#createFilterConfig and AuthFilterInitializer#createFilterConfig"},{"author":"ASF GitHub Bot","body":"xieyi888 opened a new pull request, #5304: URL: https://github.com/apache/hadoop/pull/5304 ### Description of PR JIRA: https://issues.apache.org/jira/browse/HADOOP-18594 ProxyUserAuthenticationFilter add properties 'hadoop.security.impersonation.provider.class' to enable load custom ImpersonationProvider class when start namenode ### How was this patch tested? by local test"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5304: URL: https://github.com/apache/hadoop/pull/5304#issuecomment-1385862039 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 27s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 17m 6s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 33m 49s | | trunk passed | | +1 :green_heart: | compile | 27m 31s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 23m 7s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 4m 5s | | trunk passed | | +1 :green_heart: | mvnsite | 3m 21s | | trunk passed | | -1 :x: | javadoc | 1m 9s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 2m 24s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 6m 17s | | trunk passed | | +1 :green_heart: | shadedclient | 29m 23s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 24s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 2m 38s | | the patch passed | | +1 :green_heart: | compile | 24m 52s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 24m 52s | | the patch passed | | +1 :green_heart: | compile | 22m 6s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 22m 6s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 3m 53s | | the patch passed | | +1 :green_heart: | mvnsite | 3m 14s | | the patch passed | | -1 :x: | javadoc | 1m 2s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 2m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 6m 27s | | the patch passed | | +1 :green_heart: | shadedclient | 29m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 12s | | hadoop-common in the patch passed. | | -1 :x: | unit | 411m 1s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt) | hadoop-hdfs in the patch passed. | | +1 :green_heart: | asflicense | 1m 5s | | The patch does not generate ASF License warnings. | | | | 677m 39s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.hdfs.TestLeaseRecovery2 | | | hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5304 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3fc5f9b24937 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 13e543671496dff9043ea4070a131433e1eab617 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/testReport/ | | Max. process+thread count | 3144 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5304/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5304: URL: https://github.com/apache/hadoop/pull/5304#issuecomment-3453911337 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5304: HADOOP-18594 ProxyUserAuthenticationFilter add properties for custom ImpersonationProvider URL: https://github.com/apache/hadoop/pull/5304"}]}
{"key":"HADOOP-18585","summary":"DataNode's internal infoserver redirects with http scheme, not https when https enabled.","description":"After HADOOP-16314, WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme. (Hostname is modified.) {code:java} $ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location < Location: http://dn1.example.com:50475/index.html {code} I can't ensure that which solution is the best among: - Use DefaultServlet instead of WebServlet. DataNode can answer with index.html when accessed in '/'. - According to \"dfs.http.policy\" in hdfs-site.xml, run internal infoserver as https or http server each. - Make redirection on URLDispatcher.java","status":"Open","priority":"Minor","reporter":"YUBI LEE","labels":["pull-request-available"],"project":"HADOOP","created":"2022-12-24T09:28:22.000+0000","updated":"2025-10-30T00:23:26.000+0000","comments":[{"author":"ASF GitHub Bot","body":"eubnara opened a new pull request, #5259: URL: https://github.com/apache/hadoop/pull/5259 ### Description of PR After [HADOOP-16314](https://issues.apache.org/jira/browse/HADOOP-16314), WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme. (Hostname is modified.) ``` $ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location < Location: http://dn1.example.com:50475/index.html ``` I can't ensure that which solution is the best between: - Use DefaultServlet instead of WebServlet. DataNode can answer with index.html when accessed in '/'. - According to \"dfs.http.policy\" in hdfs-site.xml, run internal infoserver as https or http server each. ### How was this patch tested? Manually tested with internal cluster. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5259: URL: https://github.com/apache/hadoop/pull/5259#issuecomment-1364536444 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 48s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 44m 46s | | trunk passed | | +1 :green_heart: | compile | 36m 7s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 31m 30s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 2m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 13s | | trunk passed | | -1 :x: | javadoc | 1m 22s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 47s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 5s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 36s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 11s | | the patch passed | | +1 :green_heart: | compile | 32m 3s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 32m 3s | | the patch passed | | +1 :green_heart: | compile | 28m 36s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 28m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 29s | | the patch passed | | +1 :green_heart: | mvnsite | 2m 33s | | the patch passed | | -1 :x: | javadoc | 1m 6s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 43s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 3m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 26m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 19m 14s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 59s | | The patch does not generate ASF License warnings. | | | | 267m 54s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.http.TestGlobalFilter | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5259 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 848d9af04a91 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b09920f48a1f51a1d69326db0ebcea33424a2262 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/testReport/ | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5259: URL: https://github.com/apache/hadoop/pull/5259#issuecomment-1364537231 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 2m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 59s | | trunk passed | | +1 :green_heart: | compile | 40m 19s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 30m 52s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 28s | | trunk passed | | +1 :green_heart: | mvnsite | 2m 16s | | trunk passed | | -1 :x: | javadoc | 1m 13s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 41s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 51s | | trunk passed | | +1 :green_heart: | shadedclient | 27m 0s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 44s | | the patch passed | | +1 :green_heart: | compile | 32m 37s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 32m 37s | | the patch passed | | +1 :green_heart: | compile | 28m 4s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 28m 4s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 9s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 51s | | the patch passed | | -1 :x: | javadoc | 1m 1s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 40s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 19m 6s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 1m 0s | | The patch does not generate ASF License warnings. | | | | 269m 11s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.http.TestGlobalFilter | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5259 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux d60bd5ca7b3a 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5866dc64e83ddc9e62349416933226340ce5ae63 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/testReport/ | | Max. process+thread count | 1376 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5259: URL: https://github.com/apache/hadoop/pull/5259#issuecomment-1364610415 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 36s | | trunk passed | | +1 :green_heart: | compile | 26m 40s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 23m 18s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 1m 8s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 42s | | trunk passed | | -1 :x: | javadoc | 1m 11s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/3/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 30s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 1m 3s | | the patch passed | | +1 :green_heart: | compile | 25m 59s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 25m 59s | | the patch passed | | +1 :green_heart: | compile | 23m 5s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 23m 5s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 1m 1s | | the patch passed | | +1 :green_heart: | mvnsite | 1m 37s | | the patch passed | | -1 :x: | javadoc | 1m 0s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/3/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-common in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 43s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 2m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 13s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 18m 25s | | hadoop-common in the patch passed. | | +1 :green_heart: | asflicense | 0m 54s | | The patch does not generate ASF License warnings. | | | | 228m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5259 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e5e7fae8f7af 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 567cf421b35717044a5ecb5db10100c7405e6f28 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/3/testReport/ | | Max. process+thread count | 3137 (vs. ulimit of 5500) | | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5259/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"eubnara commented on PR #5259: URL: https://github.com/apache/hadoop/pull/5259#issuecomment-1370375831 It ruins Ambari's checking DataNode's Web UI and redirection from NameNode web UI to DataNode web UI."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5259: URL: https://github.com/apache/hadoop/pull/5259#issuecomment-3459165353 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5259: HADOOP-18585. DataNode's internal infoserver redirects with http scheme, not https when https enabled. URL: https://github.com/apache/hadoop/pull/5259"}]}
{"key":"HADOOP-19472","summary":"ABFS: Enhance performance of ABFS driver for write-heavy workloads","description":"The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.","status":"Open","priority":"Minor","reporter":"Anmol Asrani","assignee":"Anmol Asrani","labels":["pull-request-available"],"project":"HADOOP","created":"2025-02-25T09:54:55.000+0000","updated":"2025-10-30T10:40:30.000+0000","comments":[{"author":"ASF GitHub Bot","body":"anmolanmol1234 opened a new pull request, #7669: URL: https://github.com/apache/hadoop/pull/7669 Enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes. ![{05B55BCA-EF1F-496D-B1ED-17DCD394DDA1}](https://github.com/user-attachments/assets/5ebd5ad7-51db-4028-812f-ce9da9266984) The proposed design advocates for a centralized `WriteThreadPoolSizeManager` class to handle the collective thread allocation required for all write operations across the system, replacing the current CachedThreadPool in AzureBlobFileSystemStore. This centralized approach ensures that the initial thread pool size is set at `4 * number of available processors` and dynamically adjusts the pool size based on the system's current CPU utilization. This adaptive scaling and descaling mechanism optimizes resource usage and responsiveness. Moreover, this shared thread pool is accessible and utilized by all output streams, streamlining resource management and promoting efficient concurrency across write operations."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848521492 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 164 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 801, Failures: 0, Errors: 0, Skipped: 117 [ERROR] Tests run: 146, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 640, Failures: 0, Errors: 0, Skipped: 215 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 171 [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 643, Failures: 0, Errors: 0, Skipped: 144 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [ERROR] Tests run: 640, Failures: 0, Errors: 0, Skipped: 146 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [ERROR] Tests run: 638, Failures: 0, Errors: 0, Skipped: 164 [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4 [ERROR] Tests run: 672, Failures: 0, Errors: 0, Skipped: 167 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 215 [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6 [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848547561 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 26m 3s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | spotbugs | 0m 45s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 19s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 31s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 14 new + 2 unchanged - 0 fixed = 16 total (was 2) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | -1 :x: | spotbugs | 0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 20m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 19s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 77m 36s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths At WriteThreadPoolSizeManager.java:on all exception paths At WriteThreadPoolSizeManager.java:[line 268] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a1000f66baec 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1ba12f6247567e3f5e9087c00fb52e741b1eb98c | | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/testReport/ | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2857800447 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 39s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 18s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 30s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 19s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | -1 :x: | spotbugs | 0m 40s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) | | +1 :green_heart: | shadedclient | 22m 6s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 78m 44s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths At WriteThreadPoolSizeManager.java:on all exception paths At WriteThreadPoolSizeManager.java:[line 263] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3b526ffc404b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8fe08428cb90f286b3d7408e60bd3c05bbde7ba6 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/testReport/ | | Max. process+thread count | 558 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2858500737 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 23m 7s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 50s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 2s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 29s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 17s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 74m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 3ee980188ead 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c56bdcbeec1025c521eae998c50aaf72d72458d7 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2911679089 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 29s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 25s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 23m 37s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 49s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 22s | | The patch does not generate ASF License warnings. | | | | 80m 29s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 251c9a94f809 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ca7be8805b4a932ebcfbef19749c9b8b3e0497e1 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/testReport/ | | Max. process+thread count | 545 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-2912198409 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 22s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 53s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 5s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 15s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 17s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 76m 56s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 957e28bff80b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 5e79b10358006820caa9e39d1d472571aba1f56c | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/testReport/ | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3164446290 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 23s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 20s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 39s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 52s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 25m 35s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 20s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 25s | | The patch does not generate ASF License warnings. | | | | 91m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux b233612874b4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 73a8012c216fb9222f1be6bb639378f77091a305 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/testReport/ | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195908154 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 16s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 25m 36s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 25m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 39s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 15s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 22s | | The patch does not generate ASF License warnings. | | | | 86m 0s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux a58d8ee4661b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 3164050e6fccde89f26d597bee73fa42a725dc3a | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/testReport/ | | Max. process+thread count | 729 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195944600 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 27m 49s | | trunk passed | | +1 :green_heart: | compile | 0m 22s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 25s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 38s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 26m 50s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 40s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 86m 51s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 345d7c0ea8f7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / db7ae1fa85a844e69cbdfde57081ce77091c3799 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/testReport/ | | Max. process+thread count | 546 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3197457197 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 30m 15s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 31s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 16s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 16s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 32 new + 2 unchanged - 0 fixed = 34 total (was 2) | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 7s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 25s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 84m 9s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 806146c83fa5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 812ea4660ae805dd76ddb18bf25a92a80c70e0b0 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/testReport/ | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2313716838 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -321,16 +330,13 @@ public void close() throws IOException { try { Futures.allAsList(futures).get(); // shutdown the threadPool and set it to null. - HadoopExecutors.shutdown(boundedThreadPool, LOG, - 30, TimeUnit.SECONDS); - boundedThreadPool = null; } catch (InterruptedException e) { LOG.error(\"Interrupted freeing leases\", e); Thread.currentThread().interrupt(); } catch (ExecutionException e) { LOG.error(\"Error freeing leases\", e); } finally { - IOUtils.cleanupWithLogger(LOG, getClient()); + IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient()); Review Comment: For the non-dynamic pool- how are we closing the boundedThreadPool? Would we need HadoopExecutors.shutdown(..) for it?"},{"author":"ASF GitHub Bot","body":"manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2313736451 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); Review Comment: we're naming the threads in non-dynamic pool and the manager pool for dynamic write pool. Should we also name the threads for dynamic case?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3242214939 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 30s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 38s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 51s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2) | | +1 :green_heart: | mvnsite | 0m 18s | | the patch passed | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 4s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 78m 49s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2e870a45b9cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cafe1e5e618e22865f26a57b6891dc57a368e658 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/testReport/ | | Max. process+thread count | 555 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314852491 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); + + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); + LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize()); + LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount()); + } + } + + /** + * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly. + */ + synchronized void startCPUMonitoring() { + cpuMonitorExecutor.scheduleAtFixedRate(() -> { + double cpuUtilization = getCpuUtilization(); + LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization); + try { + adjustThreadPoolSizeBasedOnCPU(cpuUtilization); + } catch (InterruptedException e) { + throw new RuntimeException(String.format( + \"Thread pool size adjustment interrupted for filesystem %s\", + filesystemName), e); + } + }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS); + } + + /** + * Gets the current CPU utilization. + * + * @return the CPU utilization as a percentage (0.0 to 1.0). + */ + private double getCpuUtilization() { + OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean(); + if (osBean instanceof com.sun.management.OperatingSystemMXBean) { + com.sun.management.OperatingSystemMXBean sunOsBean + = (com.sun.management.OperatingSystemMXBean) osBean; + double cpuLoad = sunOsBean.getSystemCpuLoad(); + if (cpuLoad >= 0) { Review Comment: if cpuLoad is -1.0, should we log it?"},{"author":"ASF GitHub Bot","body":"manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: Nit: we can remove it (part of prev PR)"},{"author":"ASF GitHub Bot","body":"manika137 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: Nit: we can remove it and related ones below (part of prev PR)"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252145988 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | patch | 0m 17s | | https://github.com/apache/hadoop/pull/7669 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/11/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252822256 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 28m 24s | | trunk passed | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 21s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 42s | | trunk passed | | +1 :green_heart: | shadedclient | 27m 36s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 27m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 12s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 17s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 47s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 27s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 27s | | The patch does not generate ASF License warnings. | | | | 93m 53s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux f783f8b2200a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6f5cd826e33f1d86cc5f4aa373268b6ef03fb4bc | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/testReport/ | | Max. process+thread count | 563 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3257523521 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 0s | | Docker mode activated. | | -1 :x: | docker | 16m 4s | | Docker failed to build run-specific yetus/hadoop:tp-31571}. | | Subsystem | Report/Notes | |----------:|:-------------| | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/13/console | | versions | git=2.34.1 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3258488704 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 9m 39s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | -1 :x: | mvninstall | 33m 34s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/branch-mvninstall-root.txt) | root in trunk failed. | | -1 :x: | compile | 0m 16s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | +1 :green_heart: | compile | 0m 20s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 23s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 41s | | trunk passed | | -1 :x: | shadedclient | 22m 8s | | branch has errors when building and testing our client artifacts. | | -0 :warning: | patch | 22m 33s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | -1 :x: | mvninstall | 0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | compile | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javac | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | compile | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | javac | 0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 20s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) | The patch fails to run checkstyle in hadoop-azure | | -1 :x: | mvnsite | 0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | -1 :x: | javadoc | 0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04. | | -1 :x: | javadoc | 0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) | hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09. | | -1 :x: | spotbugs | 0m 21s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +1 :green_heart: | shadedclient | 4m 17s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) | hadoop-azure in the patch failed. | | +0 :ok: | asflicense | 0m 23s | | ASF License check generated no output? | | | | 75m 16s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 88e24d886e71 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / ed6c78ee45d94c69e87c25c3b95b55effeed6923 | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/testReport/ | | Max. process+thread count | 556 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265728007 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 167 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 822, Failures: 0, Errors: 0, Skipped: 119 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 235 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 178 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 668, Failures: 0, Errors: 0, Skipped: 161 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 237 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 665, Failures: 0, Errors: 0, Skipped: 173 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12 [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 195 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 693, Failures: 0, Errors: 0, Skipped: 176 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 234 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265990736 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 27s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 21s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 23s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | trunk passed | | +1 :green_heart: | shadedclient | 21m 32s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 21m 45s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 18s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 18s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 24s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 78m 22s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 74a5f58b2863 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 443556cdb787d2c6be17e9317a9870812950506e | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/testReport/ | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3317397225 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 24s | | trunk passed | | +1 :green_heart: | compile | 0m 27s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | checkstyle | 0m 22s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 30s | | trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 49s | | trunk passed | | +1 :green_heart: | shadedclient | 24m 35s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 24m 49s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 1s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 14s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 23s | | the patch passed | | +1 :green_heart: | javadoc | 0m 16s | | the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 24m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 22s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 26s | | The patch does not generate ASF License warnings. | | | | 98m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c3b676228a26 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 6a0e5bcaabcdfcbbc3ba4dc1ab3cefc20362dfdf | | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/testReport/ | | Max. process+thread count | 561 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anujmodi2021 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2450656923 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); Review Comment: ll the log lines can be combined into a single log here. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); Review Comment: Setting both core pool size and max pool size to same value will make it a fixed size thread pool. We should only set max pool size and executor will spawn new threads only when needed. ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { Review Comment: Are we computing thread pool size based on available memory? Shouldn't it be available cpu? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( Review Comment: We are using a fixed Thread Pool executor service here. with initialThreadPoolSize Where are we setting the max threread pool size here? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); Review Comment: Have a single log line ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); Review Comment: This seems a bit misleading. Configuration says its the max count but t is used as a inital thread pool size, may be a better variable name like `configuredMaxPoolSize` ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -478,6 +478,57 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES) private int maxApacheHttpClientIoExceptionsRetries; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT, + DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT) + private boolean dynamicWriteThreadPoolEnablement; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME, + DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME) + private int writeThreadPoolKeepAliveTime; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL, + MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL, + MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL, + DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL) + private int writeCpuMonitoringInterval; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE, Review Comment: Is this the minimum thread pool size? Should we name it likewise then?"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454205775 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -321,16 +330,13 @@ public void close() throws IOException { try { Futures.allAsList(futures).get(); // shutdown the threadPool and set it to null. - HadoopExecutors.shutdown(boundedThreadPool, LOG, - 30, TimeUnit.SECONDS); - boundedThreadPool = null; } catch (InterruptedException e) { LOG.error(\"Interrupted freeing leases\", e); Thread.currentThread().interrupt(); } catch (ExecutionException e) { LOG.error(\"Error freeing leases\", e); } finally { - IOUtils.cleanupWithLogger(LOG, getClient()); + IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient()); Review Comment: That is taken care when the AzureBlobFileSystemStore is shutdown ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); Review Comment: Taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454234411 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,377 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + + /* Initialize the bounded thread pool executor */ + this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize); + + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); + LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize()); + LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount()); + } + } + + /** + * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly. + */ + synchronized void startCPUMonitoring() { + cpuMonitorExecutor.scheduleAtFixedRate(() -> { + double cpuUtilization = getCpuUtilization(); + LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization); + try { + adjustThreadPoolSizeBasedOnCPU(cpuUtilization); + } catch (InterruptedException e) { + throw new RuntimeException(String.format( + \"Thread pool size adjustment interrupted for filesystem %s\", + filesystemName), e); + } + }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS); + } + + /** + * Gets the current CPU utilization. + * + * @return the CPU utilization as a percentage (0.0 to 1.0). + */ + private double getCpuUtilization() { + OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean(); + if (osBean instanceof com.sun.management.OperatingSystemMXBean) { + com.sun.management.OperatingSystemMXBean sunOsBean + = (com.sun.management.OperatingSystemMXBean) osBean; + double cpuLoad = sunOsBean.getSystemCpuLoad(); + if (cpuLoad >= 0) { Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454238973 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -438,6 +438,10 @@ public class AbfsConfiguration{ FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION) private boolean isChecksumValidationEnabled; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = Review Comment: taken care by trunk merge"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454257729 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -478,6 +478,57 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES) private int maxApacheHttpClientIoExceptionsRetries; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT, + DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT) + private boolean dynamicWriteThreadPoolEnablement; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME, + DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME) + private int writeThreadPoolKeepAliveTime; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL, + MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL, + MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL, + DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL) + private int writeCpuMonitoringInterval; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE, Review Comment: This was used to determine the no. of threads to spawn for CPU monitoring and was kept default as 1 but it would be better to always have a single thread and hence removed this config,"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454304102 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); Review Comment: Taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454309973 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( Review Comment: So we are starting with the configured value of writeconcurrentrequestcount as the initial thread pool size which is also the max until it is scaled further, the logic for which is present in the adjustThreadPoolBasedOnCpu"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3435800307 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 31m 35s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 17m 44s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 17m 58s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 13s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 24s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 53 new + 1472 unchanged - 0 fixed = 1525 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 53 new + 1413 unchanged - 0 fixed = 1466 total (was 1413) | | -1 :x: | spotbugs | 0m 49s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) | hadoop-tools/hadoop-azure generated 1 new + 178 unchanged - 0 fixed = 179 total (was 178) | | +1 :green_heart: | shadedclient | 18m 36s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 11s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 18s | | The patch does not generate ASF License warnings. | | | | 77m 36s | | | | Reason | Tests | |-------:|:------| | SpotBugs | module:hadoop-tools/hadoop-azure | | | org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.getAbfsConfiguration() may expose internal representation by returning WriteThreadPoolSizeManager.abfsConfiguration At WriteThreadPoolSizeManager.java:by returning WriteThreadPoolSizeManager.abfsConfiguration At WriteThreadPoolSizeManager.java:[line 127] | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux e7d97cc8e101 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7708c26a556b7b84bd7edb6c70304f6f301eb0c3 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/testReport/ | | Max. process+thread count | 642 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454540355 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454542830 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { Review Comment: This is during initialization when CPU doesn't play a role, CPU is used for scaling and descaling only in the current design"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454547932 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); Review Comment: If we do on need basis, it doesn't serve our purpose of being aggressive. This was tried as well and even though max was set to higher number, available threads were not getting used."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2454550766 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,383 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.OperatingSystemMXBean; +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteMaxConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool( + abfsConfiguration.getWriteCorePoolSize()); + } + + public AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. + * @return Computed max thread pool size. + */ + private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) { + LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory); + LOG.debug(\"The number of available processors is {} \", availableProcessors); + int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors); + LOG.debug(\"The max thread pool size is {} \", maxpoolSize); + return maxpoolSize; + } + + /** + * Calculates the available heap memory in gigabytes. + * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory + * allowed for the JVM and subtracts the currently used memory (total - free) + * to determine how much heap memory is still available. + * The result is rounded up to the nearest gigabyte. + * + * @return the available heap memory in gigabytes + */ + private long getAvailableHeapMemory() { + Runtime runtime = Runtime.getRuntime(); + long maxMemory = runtime.maxMemory(); + long usedMemory = runtime.totalMemory() - runtime.freeMemory(); + long availableHeapBytes = maxMemory - usedMemory; + return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE; + } + + /** + * Returns aggressive thread count = CPU cores  multiplier based on heap tier. + */ + private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) { + int multiplier; + if (availableHeapGB = currentCorePoolSize) { + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + } else { + threadPoolExecutor.setCorePoolSize(newMaxPoolSize); + threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize); + } + + LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize); Review Comment: taken"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3436426336 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 22s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 24m 50s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 16m 11s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 16m 24s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 50 new + 1472 unchanged - 0 fixed = 1522 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 50 new + 1413 unchanged - 0 fixed = 1463 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 16m 50s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 23s | | The patch does not generate ASF License warnings. | | | | 67m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5c77e6ac2aef 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / fb58cf38644cae72dc0238fa354724d0af74d820 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/testReport/ | | Max. process+thread count | 612 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"bhattmanish98 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2464543406 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\"; /**Maximum number of thread per blob-delete orchestration: {@value}*/ public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; + /** + * Configuration key for the keep-alive time for the write thread pool. + * This value specifies the amount of time that threads in the write thread pool + * will remain idle before being terminated. + * Value: {@value}. + */ + public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\"; Review Comment: Is this time in millis? If yes, can we put millis at the end in the name and in the config key? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\"; /**Maximum number of thread per blob-delete orchestration: {@value}*/ public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; + /** + * Configuration key for the keep-alive time for the write thread pool. + * This value specifies the amount of time that threads in the write thread pool + * will remain idle before being terminated. + * Value: {@value}. + */ + public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\"; + + public static final String FS_AZURE_WRITE_CPU_MONITORING_INTERVAL = \"fs.azure.write.cpu.monitoring.interval\"; + + public static final String FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = \"fs.azure.write.dynamic.threadpool.enablement\"; + + public static final String FS_AZURE_WRITE_HIGH_CPU_THRESHOLD = \"fs.azure.write.high.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_MEDIUM_CPU_THRESHOLD = \"fs.azure.write.medium.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_LOW_CPU_THRESHOLD = \"fs.azure.write.low.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_LOW_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.low.tier.memory.multiplier\"; + + public static final String FS_AZURE_WRITE_MEDIUM_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.medium.tier.memory.multiplier\"; + + public static final String FS_AZURE_WRITE_HIGH_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.high.tier.memory.multiplier\"; + + + /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/ public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\"; /**Flag to enable/disable create idempotency during create operation: {@value}*/ public static final String FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = \"fs.azure.enable.create.blob.idempotency\"; + /** Review Comment: Since this config is related to the above newly added ones, should we group them together? ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,388 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.MemoryMXBean; +import java.lang.management.MemoryUsage; + +import com.sun.management.OperatingSystemMXBean; + +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool(1); + } + + /** Returns the internal {@link AbfsConfiguration}. */ + private AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. Review Comment: @param initialAvailableHeapMemory is missing ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -277,11 +278,19 @@ public AzureBlobFileSystemStore( } this.blockFactory = abfsStoreBuilder.blockFactory; this.blockOutputActiveBlocks = abfsStoreBuilder.blockOutputActiveBlocks; - this.boundedThreadPool = BlockingThreadPoolExecutorService.newInstance( - abfsConfiguration.getWriteMaxConcurrentRequestCount(), - abfsConfiguration.getMaxWriteRequestsToQueue(), - 10L, TimeUnit.SECONDS, - \"abfs-bounded\"); + if (abfsConfiguration.isDynamicWriteThreadPoolEnablement()) { + this.poolSizeManager = WriteThreadPoolSizeManager.getInstance( + getClient().getFileSystem() + \"-\" + UUID.randomUUID(), + abfsConfiguration); + poolSizeManager.startCPUMonitoring(); Review Comment: WriteThreadPoolSizeManager.getInstance(...) can return null and this line can throw null pointer exception."},{"author":"ASF GitHub Bot","body":"anujmodi2021 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465547159 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java: ########## @@ -478,6 +478,57 @@ public class AbfsConfiguration{ DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES) private int maxApacheHttpClientIoExceptionsRetries; + @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT, + DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT) + private boolean dynamicWriteThreadPoolEnablement; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME, + DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME) + private int writeThreadPoolKeepAliveTime; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL, + MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL, + MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL, + DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL) + private int writeCpuMonitoringInterval; + + @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE, Review Comment: Makes sense"},{"author":"ASF GitHub Bot","body":"anujmodi2021 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465584624 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -262,6 +281,48 @@ public final class FileSystemConfigurations { public static final int DEFAULT_FS_AZURE_BLOB_DELETE_THREAD = DEFAULT_FS_AZURE_LISTING_ACTION_THREADS; + public static final boolean DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = true; Review Comment: By default should be disabled ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestWriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,770 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.assertj.core.api.Assertions; +import org.junit.jupiter.api.BeforeEach; +import org.junit.jupiter.api.Test; + +import java.io.IOException; +import java.util.ArrayList; +import java.util.List; +import java.util.Random; +import java.util.concurrent.CountDownLatch; +import java.util.concurrent.CyclicBarrier; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.RejectedExecutionException; +import java.util.concurrent.ScheduledThreadPoolExecutor; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.atomic.AtomicIntegerArray; + +import org.apache.hadoop.fs.FSDataOutputStream; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ZERO; +import static org.mockito.Mockito.mock; +import static org.mockito.Mockito.when; + +class TestWriteThreadPoolSizeManager extends AbstractAbfsIntegrationTest { + + private AbfsConfiguration mockConfig; + private static final double HIGH_CPU_UTILIZATION_THRESHOLD = 0.95; + private static final double LOW_CPU_UTILIZATION_THRESHOLD = 0.05; + private static final int THREAD_SLEEP_DURATION_MS = 200; + private static final String TEST_FILE_PATH = \"testFilePath\"; + private static final String TEST_DIR_PATH = \"testDirPath\"; + private static final int TEST_FILE_LENGTH = 1024 * 1024 * 8; + private static final int CONCURRENT_REQUEST_COUNT = 15; + private static final int THREAD_POOL_KEEP_ALIVE_TIME = 10; + private static final int LOW_TIER_MEMORY_MULTIPLIER = 4; + private static final int MEDIUM_TIER_MEMORY_MULTIPLIER = 6; + private static final int HIGH_TIER_MEMORY_MULTIPLIER = 8; + private static final int HIGH_CPU_THRESHOLD = 15; + private static final int MEDIUM_CPU_THRESHOLD = 10; + private static final int LOW_CPU_THRESHOLD = 5; + private static final int CPU_MONITORING_INTERVAL = 15; + private static final int WAIT_DURATION_MS = 3000; + private static final int LATCH_TIMEOUT_SECONDS = 60; + private static final int RESIZE_WAIT_TIME_MS = 6_000; + private static final double HIGH_CPU_USAGE_RATIO = 0.95; + private static final double LOW_CPU_USAGE_RATIO = 0.05; + private static final int SLEEP_DURATION_MS = 150; + private static final int AWAIT_TIMEOUT_SECONDS = 45; + private static final int RESIZER_JOIN_TIMEOUT_MS = 2_000; + private static final int WAIT_TIMEOUT_MS = 5000; + private static final int SLEEP_DURATION_30S_MS = 30000; + private static final int SMALL_PAUSE_MS = 50; + private static final int BURST_LOAD = 50; + private static final long LOAD_SLEEP_DURATION_MS = 2000; + + TestWriteThreadPoolSizeManager() throws Exception { + super.setup(); + } + + /** + * Common setup to prepare a mock configuration for each test. + */ + @BeforeEach + public void setUp() { + mockConfig = mock(AbfsConfiguration.class); + when(mockConfig.getWriteConcurrentRequestCount()).thenReturn(CONCURRENT_REQUEST_COUNT); + when(mockConfig.getWriteThreadPoolKeepAliveTime()).thenReturn(THREAD_POOL_KEEP_ALIVE_TIME); + when(mockConfig.getLowTierMemoryMultiplier()).thenReturn(LOW_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getMediumTierMemoryMultiplier()).thenReturn(MEDIUM_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getHighTierMemoryMultiplier()).thenReturn(HIGH_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getWriteHighCpuThreshold()).thenReturn(HIGH_CPU_THRESHOLD); + when(mockConfig.getWriteMediumCpuThreshold()).thenReturn(MEDIUM_CPU_THRESHOLD); + when(mockConfig.getWriteLowCpuThreshold()).thenReturn(LOW_CPU_THRESHOLD); + when(mockConfig.getWriteCpuMonitoringInterval()).thenReturn(CPU_MONITORING_INTERVAL); + } + + /** + * Ensures that {@link WriteThreadPoolSizeManager#getInstance(String, AbfsConfiguration)} returns a singleton per key. + */ + @Test + void testGetInstanceReturnsSingleton() { Review Comment: Its no more a singleton right? May be we don't need this test anymore."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465654952 ########## hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestWriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,770 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.assertj.core.api.Assertions; +import org.junit.jupiter.api.BeforeEach; +import org.junit.jupiter.api.Test; + +import java.io.IOException; +import java.util.ArrayList; +import java.util.List; +import java.util.Random; +import java.util.concurrent.CountDownLatch; +import java.util.concurrent.CyclicBarrier; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.RejectedExecutionException; +import java.util.concurrent.ScheduledThreadPoolExecutor; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.atomic.AtomicIntegerArray; + +import org.apache.hadoop.fs.FSDataOutputStream; +import org.apache.hadoop.fs.FileSystem; +import org.apache.hadoop.fs.Path; + +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ZERO; +import static org.mockito.Mockito.mock; +import static org.mockito.Mockito.when; + +class TestWriteThreadPoolSizeManager extends AbstractAbfsIntegrationTest { + + private AbfsConfiguration mockConfig; + private static final double HIGH_CPU_UTILIZATION_THRESHOLD = 0.95; + private static final double LOW_CPU_UTILIZATION_THRESHOLD = 0.05; + private static final int THREAD_SLEEP_DURATION_MS = 200; + private static final String TEST_FILE_PATH = \"testFilePath\"; + private static final String TEST_DIR_PATH = \"testDirPath\"; + private static final int TEST_FILE_LENGTH = 1024 * 1024 * 8; + private static final int CONCURRENT_REQUEST_COUNT = 15; + private static final int THREAD_POOL_KEEP_ALIVE_TIME = 10; + private static final int LOW_TIER_MEMORY_MULTIPLIER = 4; + private static final int MEDIUM_TIER_MEMORY_MULTIPLIER = 6; + private static final int HIGH_TIER_MEMORY_MULTIPLIER = 8; + private static final int HIGH_CPU_THRESHOLD = 15; + private static final int MEDIUM_CPU_THRESHOLD = 10; + private static final int LOW_CPU_THRESHOLD = 5; + private static final int CPU_MONITORING_INTERVAL = 15; + private static final int WAIT_DURATION_MS = 3000; + private static final int LATCH_TIMEOUT_SECONDS = 60; + private static final int RESIZE_WAIT_TIME_MS = 6_000; + private static final double HIGH_CPU_USAGE_RATIO = 0.95; + private static final double LOW_CPU_USAGE_RATIO = 0.05; + private static final int SLEEP_DURATION_MS = 150; + private static final int AWAIT_TIMEOUT_SECONDS = 45; + private static final int RESIZER_JOIN_TIMEOUT_MS = 2_000; + private static final int WAIT_TIMEOUT_MS = 5000; + private static final int SLEEP_DURATION_30S_MS = 30000; + private static final int SMALL_PAUSE_MS = 50; + private static final int BURST_LOAD = 50; + private static final long LOAD_SLEEP_DURATION_MS = 2000; + + TestWriteThreadPoolSizeManager() throws Exception { + super.setup(); + } + + /** + * Common setup to prepare a mock configuration for each test. + */ + @BeforeEach + public void setUp() { + mockConfig = mock(AbfsConfiguration.class); + when(mockConfig.getWriteConcurrentRequestCount()).thenReturn(CONCURRENT_REQUEST_COUNT); + when(mockConfig.getWriteThreadPoolKeepAliveTime()).thenReturn(THREAD_POOL_KEEP_ALIVE_TIME); + when(mockConfig.getLowTierMemoryMultiplier()).thenReturn(LOW_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getMediumTierMemoryMultiplier()).thenReturn(MEDIUM_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getHighTierMemoryMultiplier()).thenReturn(HIGH_TIER_MEMORY_MULTIPLIER); + when(mockConfig.getWriteHighCpuThreshold()).thenReturn(HIGH_CPU_THRESHOLD); + when(mockConfig.getWriteMediumCpuThreshold()).thenReturn(MEDIUM_CPU_THRESHOLD); + when(mockConfig.getWriteLowCpuThreshold()).thenReturn(LOW_CPU_THRESHOLD); + when(mockConfig.getWriteCpuMonitoringInterval()).thenReturn(CPU_MONITORING_INTERVAL); + } + + /** + * Ensures that {@link WriteThreadPoolSizeManager#getInstance(String, AbfsConfiguration)} returns a singleton per key. + */ + @Test + void testGetInstanceReturnsSingleton() { Review Comment: Currently the design returns a singleton instance only_"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465656516 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java: ########## @@ -262,6 +281,48 @@ public final class FileSystemConfigurations { public static final int DEFAULT_FS_AZURE_BLOB_DELETE_THREAD = DEFAULT_FS_AZURE_LISTING_ACTION_THREADS; + public static final boolean DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = true; Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465671952 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java: ########## @@ -0,0 +1,388 @@ +/** + * Licensed to the Apache Software Foundation (ASF) under one + * or more contributor license agreements. See the NOTICE file + * distributed with this work for additional information + * regarding copyright ownership. The ASF licenses this file + * to you under the Apache License, Version 2.0 (the + * \"License\"); you may not use this file except in compliance + * with the License. You may obtain a copy of the License at + * + * http://www.apache.org/licenses/LICENSE-2.0 + * + * Unless required by applicable law or agreed to in writing, software + * distributed under the License is distributed on an \"AS IS\" BASIS, + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. + * See the License for the specific language governing permissions and + * limitations under the License. + */ + +package org.apache.hadoop.fs.azurebfs; + +import org.slf4j.Logger; +import org.slf4j.LoggerFactory; + +import java.io.Closeable; +import java.io.IOException; +import java.lang.management.ManagementFactory; +import java.lang.management.MemoryMXBean; +import java.lang.management.MemoryUsage; + +import com.sun.management.OperatingSystemMXBean; + +import java.util.concurrent.ConcurrentHashMap; +import java.util.concurrent.ExecutorService; +import java.util.concurrent.Executors; +import java.util.concurrent.ScheduledExecutorService; +import java.util.concurrent.ThreadPoolExecutor; +import java.util.concurrent.TimeUnit; +import java.util.concurrent.atomic.AtomicInteger; +import java.util.concurrent.locks.Lock; +import java.util.concurrent.locks.ReentrantLock; + +import org.apache.hadoop.util.concurrent.HadoopExecutors; + +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR; +import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS; + +/** + * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization. + */ +public final class WriteThreadPoolSizeManager implements Closeable { + + /* Maximum allowed size for the thread pool. */ + private final int maxThreadPoolSize; + /* Executor for periodically monitoring CPU usage. */ + private final ScheduledExecutorService cpuMonitorExecutor; + /* Thread pool whose size is dynamically managed. */ + private volatile ExecutorService boundedThreadPool; + /* Lock to ensure thread-safe updates to the thread pool. */ + private final Lock lock = new ReentrantLock(); + /* New computed max size for the thread pool after adjustment. */ + private volatile int newMaxPoolSize; + /* Logger instance for logging events from WriteThreadPoolSizeManager. */ + private static final Logger LOG = LoggerFactory.getLogger( + WriteThreadPoolSizeManager.class); + /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */ + private static final ConcurrentHashMap + POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>(); + /* Name of the filesystem associated with this manager. */ + private final String filesystemName; + /* Initial size for the thread pool when created. */ + private final int initialPoolSize; + /* Initially available heap memory. */ + private final long initialAvailableHeapMemory; + /* The configuration instance. */ + private final AbfsConfiguration abfsConfiguration; + + /** + * Private constructor to initialize the write thread pool and CPU monitor executor + * based on system resources and ABFS configuration. + * + * @param filesystemName Name of the ABFS filesystem. + * @param abfsConfiguration Configuration containing pool size parameters. + */ + private WriteThreadPoolSizeManager(String filesystemName, + AbfsConfiguration abfsConfiguration) { + this.filesystemName = filesystemName; + this.abfsConfiguration = abfsConfiguration; + int availableProcessors = Runtime.getRuntime().availableProcessors(); + /* Get the heap space available when the instance is created */ + this.initialAvailableHeapMemory = getAvailableHeapMemory(); + /* Compute the max pool size */ + int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory); + + /* Get the initial pool size from config, fallback to at least 1 */ + this.initialPoolSize = Math.max(1, + abfsConfiguration.getWriteConcurrentRequestCount()); + + /* Set the upper bound for the thread pool size */ + this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize); + AtomicInteger threadCount = new AtomicInteger(1); + this.boundedThreadPool = Executors.newFixedThreadPool( + initialPoolSize, + r -> { + Thread t = new Thread(r); + t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement()); + return t; + } + ); + ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool; + executor.setKeepAliveTime( + abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS); + executor.allowCoreThreadTimeOut(true); + /* Create a scheduled executor for CPU monitoring and pool adjustment */ + this.cpuMonitorExecutor = Executors.newScheduledThreadPool(1); + } + + /** Returns the internal {@link AbfsConfiguration}. */ + private AbfsConfiguration getAbfsConfiguration() { + return abfsConfiguration; + } + + /** + * Calculates the max thread pool size using a multiplier based on + * memory per core. Higher memory per core results in a larger multiplier. + * + * @param availableProcessors Number of CPU cores. Review Comment: taken"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465675885 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java: ########## @@ -277,11 +278,19 @@ public AzureBlobFileSystemStore( } this.blockFactory = abfsStoreBuilder.blockFactory; this.blockOutputActiveBlocks = abfsStoreBuilder.blockOutputActiveBlocks; - this.boundedThreadPool = BlockingThreadPoolExecutorService.newInstance( - abfsConfiguration.getWriteMaxConcurrentRequestCount(), - abfsConfiguration.getMaxWriteRequestsToQueue(), - 10L, TimeUnit.SECONDS, - \"abfs-bounded\"); + if (abfsConfiguration.isDynamicWriteThreadPoolEnablement()) { + this.poolSizeManager = WriteThreadPoolSizeManager.getInstance( + getClient().getFileSystem() + \"-\" + UUID.randomUUID(), + abfsConfiguration); + poolSizeManager.startCPUMonitoring(); Review Comment: It creates a new instance every time which is not null, can you please elaborate on your concern"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465682108 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\"; /**Maximum number of thread per blob-delete orchestration: {@value}*/ public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; + /** + * Configuration key for the keep-alive time for the write thread pool. + * This value specifies the amount of time that threads in the write thread pool + * will remain idle before being terminated. + * Value: {@value}. + */ + public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\"; + + public static final String FS_AZURE_WRITE_CPU_MONITORING_INTERVAL = \"fs.azure.write.cpu.monitoring.interval\"; + + public static final String FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = \"fs.azure.write.dynamic.threadpool.enablement\"; + + public static final String FS_AZURE_WRITE_HIGH_CPU_THRESHOLD = \"fs.azure.write.high.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_MEDIUM_CPU_THRESHOLD = \"fs.azure.write.medium.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_LOW_CPU_THRESHOLD = \"fs.azure.write.low.cpu.threshold\"; + + public static final String FS_AZURE_WRITE_LOW_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.low.tier.memory.multiplier\"; + + public static final String FS_AZURE_WRITE_MEDIUM_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.medium.tier.memory.multiplier\"; + + public static final String FS_AZURE_WRITE_HIGH_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.high.tier.memory.multiplier\"; + + + /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/ public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\"; /**Flag to enable/disable create idempotency during create operation: {@value}*/ public static final String FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = \"fs.azure.enable.create.blob.idempotency\"; + /** Review Comment: this config was no longer needed, hence removed it"},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465687329 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\"; /**Maximum number of thread per blob-delete orchestration: {@value}*/ public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; + /** + * Configuration key for the keep-alive time for the write thread pool. + * This value specifies the amount of time that threads in the write thread pool + * will remain idle before being terminated. + * Value: {@value}. + */ + public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\"; Review Comment: This is time in seconds"},{"author":"ASF GitHub Bot","body":"bhattmanish98 commented on code in PR #7669: URL: https://github.com/apache/hadoop/pull/7669#discussion_r2465902645 ########## hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java: ########## @@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\"; /**Maximum number of thread per blob-delete orchestration: {@value}*/ public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\"; + /** + * Configuration key for the keep-alive time for the write thread pool. + * This value specifies the amount of time that threads in the write thread pool + * will remain idle before being terminated. + * Value: {@value}. + */ + public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\"; Review Comment: It is good if we rename this variable to FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME_SECONDS = \"fs.azure.write.threadpool.keep.alive.time.seconds\". This will help us to know in future in which unit this time is."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3451664411 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 8m 28s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 32s | | trunk passed | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 28s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 7s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 19s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 50 new + 1472 unchanged - 0 fixed = 1522 total (was 1472) | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 50 new + 1413 unchanged - 0 fixed = 1463 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 15m 38s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 69m 8s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5e54e7b9a336 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 78f54686e9b34146f014af16b4cbf30945a659c9 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/testReport/ | | Max. process+thread count | 612 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3455405212 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 0s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 29s | | trunk passed | | +1 :green_heart: | javadoc | 0m 27s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 21s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 13m 58s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 11s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472) | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 44s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 6s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 58m 46s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 567ceba3cb8b 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a72afe4d40534d5cf403d52e90e6e990504d2967 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/testReport/ | | Max. process+thread count | 632 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3456331644 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 41s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 27s | | trunk passed | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 20s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 31s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 44s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3) | | +1 :green_heart: | mvnsite | 0m 20s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 43s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 0s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 59m 50s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux c2d6c89e9f53 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1cbce0190d046d02d388d1634e13c2398c68b243 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/testReport/ | | Max. process+thread count | 618 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anmolanmol1234 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3460085414 ============================================================ HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 217 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ HNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 5 [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 169 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10 ============================================================ NonHNS-SharedKey-DFS ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 282 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ AppendBlob-HNS-OAuth-DFS ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 228 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-SharedKey-Blob ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 140 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11 ============================================================ NonHNS-OAuth-DFS ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 284 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 718, Failures: 0, Errors: 0, Skipped: 152 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ AppendBlob-NonHNS-OAuth-Blob ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 198 [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24 ============================================================ HNS-Oauth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4 [WARNING] Tests run: 746, Failures: 0, Errors: 0, Skipped: 226 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23 ============================================================ NonHNS-OAuth-DFS-IngressBlob ============================================================ [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11 [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 281 [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9 [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3460789622 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 21s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 2s | | trunk passed | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 18s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 3s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 16s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) | hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3) | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472) | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 45s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 32s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 8s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 19s | | The patch does not generate ASF License warnings. | | | | 59m 11s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux dce0f60a12b6 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 30e66b4abd5a0c836980f9ab6baa8f7f0e6e1aae | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/testReport/ | | Max. process+thread count | 638 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3461151631 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 21m 6s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 17s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 28s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 30s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 43s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 20s | | the patch passed | | +1 :green_heart: | compile | 0m 18s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 20s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 20s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 11s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 42s | | the patch passed | | +1 :green_heart: | shadedclient | 14m 23s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 6s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 59m 39s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 84b747ef07c0 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2f9687561916ad049b94d710d3882545dd9b31c3 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/testReport/ | | Max. process+thread count | 613 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3466698904 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 20s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 22m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 25s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 26s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 19s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 22s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 14m 45s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 14m 58s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 21s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 22s | | the patch passed | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1472 unchanged - 0 fixed = 1493 total (was 1472) | | -1 :x: | javadoc | 0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 21 new + 1413 unchanged - 0 fixed = 1434 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 48s | | the patch passed | | +1 :green_heart: | shadedclient | 17m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 10s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 21s | | The patch does not generate ASF License warnings. | | | | 64m 27s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux ce684690898c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b3b2c8c29493adbdab9fb7886f5e74ba2dc31b69 | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/testReport/ | | Max. process+thread count | 640 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3467262942 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 24s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 25m 47s | | trunk passed | | +1 :green_heart: | compile | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 23s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | checkstyle | 0m 16s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 26s | | trunk passed | | +1 :green_heart: | javadoc | 0m 24s | | trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | -1 :x: | spotbugs | 0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) | hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings. | | +1 :green_heart: | shadedclient | 17m 47s | | branch has no errors when building and testing our client artifacts. | | -0 :warning: | patch | 18m 0s | | Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 19s | | the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 19s | | the patch passed | | +1 :green_heart: | compile | 0m 21s | | the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 21s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 10s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 24 new + 1440 unchanged - 32 fixed = 1464 total (was 1472) | | -1 :x: | javadoc | 0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) | hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 3 new + 1387 unchanged - 26 fixed = 1390 total (was 1413) | | +1 :green_heart: | spotbugs | 0m 46s | | the patch passed | | +1 :green_heart: | shadedclient | 18m 1s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 20s | | The patch does not generate ASF License warnings. | | | | 71m 20s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7669 | | JIRA Issue | HADOOP-19472 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 997ebef4e669 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a062843e2a6033d501196e66c17e5632d8a068bc | | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/testReport/ | | Max. process+thread count | 640 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/console | | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"anujmodi2021 commented on PR #7669: URL: https://github.com/apache/hadoop/pull/7669#issuecomment-3467277715 Thank you everyone who reviewed. The spotbugs and javadocs reported above are due to https://issues.apache.org/jira/browse/HADOOP-19731 Other Yetus checks looks good. Going ahead with the merge."},{"author":"ASF GitHub Bot","body":"anujmodi2021 merged PR #7669: URL: https://github.com/apache/hadoop/pull/7669"}]}
{"key":"HADOOP-18547","summary":"Check if config value is not empty string in AbfsConfiguration.getMandatoryPasswordString()","description":"The method `getMandatoryPasswordString` is called in `AbfsConfiguration.getTokenProvider()' to check if following configs are non-null (diff keys applicable for different implementation of AccessTokenProvider): 1. fs.azure.account.oauth2.client.endpoint: in ClientCredsTokenProvider 2. fs.azure.account.oauth2.client.id: in ClientCredsTokenProvider, MsiTokenProvider, RefreshTokenBasedTokenProvider 3. fs.azure.account.oauth2.client.secret: in ClientCredsTokenProvider 4. fs.azure.account.oauth2.client.endpoint: in UserPasswordTokenProvider 5. fs.azure.account.oauth2.user.name: in UserPasswordTokenProvider 6. fs.azure.account.oauth2.user.password: in UserPasswordTokenProvider 7. fs.azure.account.oauth2.msi.tenant: in MsiTokenProvider 8. fs.azure.account.oauth2.refresh.token: in RefreshTokenBasedTokenProvider Right now, this method checks if its non-null and not non-empty. This task needs to add check on non-empty config values.","status":"Open","priority":"Minor","reporter":"Pranav Saxena","assignee":"Pranav Saxena","labels":["pull-request-available"],"project":"HADOOP","created":"2022-12-01T06:22:10.000+0000","updated":"2025-10-31T00:22:59.000+0000","comments":[{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft opened a new pull request, #5177: URL: https://github.com/apache/hadoop/pull/5177 JIRA: https://issues.apache.org/jira/browse/HADOOP-18547 **Testing**"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-1333411119 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 43s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 40m 25s | | trunk passed | | +1 :green_heart: | compile | 0m 51s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 48s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 43s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 52s | | trunk passed | | -1 :x: | javadoc | 0m 47s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 23s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 49s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | compile | 0m 33s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 33s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 24s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 39s | | the patch passed | | -1 :x: | javadoc | 0m 31s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 28s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 13s | | the patch passed | | +1 :green_heart: | shadedclient | 22m 27s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 6s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 42s | | The patch does not generate ASF License warnings. | | | | 101m 41s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5177 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 16795c941670 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / b7b2633437f60d9edbb754afb85ce7ce29b57740 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/1/testReport/ | | Max. process+thread count | 728 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-1334963346 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 37s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 38m 57s | | trunk passed | | +1 :green_heart: | compile | 0m 59s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 55s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 52s | | trunk passed | | +1 :green_heart: | mvnsite | 1m 1s | | trunk passed | | -1 :x: | javadoc | 0m 59s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/2/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 50s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 34s | | trunk passed | | +1 :green_heart: | shadedclient | 22m 39s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 39s | | the patch passed | | +1 :green_heart: | compile | 0m 42s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 42s | | the patch passed | | +1 :green_heart: | compile | 0m 36s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 36s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 26s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 41s | | the patch passed | | -1 :x: | javadoc | 0m 31s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 31s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 15s | | the patch passed | | +1 :green_heart: | shadedclient | 21m 41s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 13s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 50s | | The patch does not generate ASF License warnings. | | | | 100m 45s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5177 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1969123ed4bd 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 2959a9cc354c625599532f47a611aca4df90b000 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/2/testReport/ | | Max. process+thread count | 671 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-1424685858 LGTM, if its ready for review"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-1425248320 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 7s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 47m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 35s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 30s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 40s | | trunk passed | | -1 :x: | javadoc | 0m 37s | [/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in trunk failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 14s | | trunk passed | | +1 :green_heart: | shadedclient | 26m 40s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 37s | | the patch passed | | +1 :green_heart: | compile | 0m 32s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 32s | | the patch passed | | +1 :green_heart: | compile | 0m 28s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 28s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | checkstyle | 0m 17s | | the patch passed | | +1 :green_heart: | mvnsite | 0m 31s | | the patch passed | | -1 :x: | javadoc | 0m 23s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04.txt) | hadoop-azure in the patch failed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04. | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 27m 9s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 2m 4s | | hadoop-azure in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 114m 48s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5177 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 12a4ab51a07b 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 1ae02b4786cc3297d2d799ccaa678707a6fb898c | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/3/testReport/ | | Max. process+thread count | 613 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5177/3/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pranavsaxena-microsoft commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-1427466685 > LGTM, if its ready for review @steveloughran , requesting you for your kind review. Thanks."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5177: URL: https://github.com/apache/hadoop/pull/5177#issuecomment-3465487727 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5177: HADOOP-18547. Check if config value is not empty string in AbfsConfiguration.getMandatoryPasswordString() URL: https://github.com/apache/hadoop/pull/5177"}]}
{"key":"HADOOP-18543","summary":"AliyunOSS: AliyunOSSFileSystem#open(Path path, int bufferSize) should use buffer size as its downloadPartSize","description":"In our application, different components have their own suitable buffer size to download. But currently, AliyunOSSFileSystem#open(Path path, int bufferSize) just get downloadPartSize from configuration. We cannnot use different value for different components in our programs. I think we should the method should use the buffer size from the paramater. AliyunOSSFileSystem#open(Path path) could have default value as current default downloadPartSize.","status":"Open","priority":"Major","reporter":"Hangxiang Yu","labels":["pull-request-available"],"project":"HADOOP","created":"2022-11-28T13:57:38.000+0000","updated":"2025-10-31T00:23:04.000+0000","comments":[{"author":"Hangxiang Yu","body":"[~wujinhu] WDYT? If right, I'd like to create a pr to fix it."},{"author":"ASF GitHub Bot","body":"masteryhx opened a new pull request, #5172: URL: https://github.com/apache/hadoop/pull/5172 ### Description of PR In our application, different components have their own suitable buffer size to download. But currently, AliyunOSSFileSystem#open(Path path, int bufferSize) just get downloadPartSize from configuration. We cannnot use different value for different components in our programs. I think we should the method should use the buffer size from the paramater. AliyunOSSFileSystem#open(Path path) could have default value as current default downloadPartSize. ### How was this patch tested? Added method TestAliyunOSSInputStream#testConfiguration to test. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1329502962 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 1m 4s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 41m 19s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 44s | | trunk passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | checkstyle | 0m 39s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 45s | | trunk passed | | +1 :green_heart: | javadoc | 0m 46s | | trunk passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 45s | | trunk passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | spotbugs | 1m 6s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 47s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 24s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 24s | | the patch passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | javac | 0m 24s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-aliyun.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aliyun.txt) | hadoop-tools/hadoop-aliyun: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 28s | | the patch passed | | +1 :green_heart: | javadoc | 0m 27s | | the patch passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 21s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 31s | | hadoop-aliyun in the patch passed. | | +1 :green_heart: | asflicense | 0m 45s | | The patch does not generate ASF License warnings. | | | | 96m 17s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5172 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 5bc2bfc76972 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7580a7cb63c066cc56d2277a5f1829e207b072a3 | | Default Java | Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/1/testReport/ | | Max. process+thread count | 731 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aliyun U: hadoop-tools/hadoop-aliyun | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5172: URL: https://github.com/apache/hadoop/pull/5172#discussion_r1034580639 ########## hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java: ########## @@ -57,18 +57,21 @@ public class AliyunOSSInputStream extends FSInputStream { private ExecutorService readAheadExecutorService; private Queue readBufferQueue = new ArrayDeque<>(); - public AliyunOSSInputStream(Configuration conf, - ExecutorService readAheadExecutorService, int maxReadAheadPartNumber, - AliyunOSSFileSystemStore store, String key, Long contentLength, - Statistics statistics) throws IOException { + public AliyunOSSInputStream( + long downloadPartSize, + ExecutorService readAheadExecutorService, + int maxReadAheadPartNumber, + AliyunOSSFileSystemStore store, + String key, + Long contentLength, + Statistics statistics) throws IOException { this.readAheadExecutorService = - MoreExecutors.listeningDecorator(readAheadExecutorService); + MoreExecutors.listeningDecorator(readAheadExecutorService); this.store = store; this.key = key; this.statistics = statistics; this.contentLength = contentLength; - downloadPartSize = conf.getLong(MULTIPART_DOWNLOAD_SIZE_KEY, - MULTIPART_DOWNLOAD_SIZE_DEFAULT); + this.downloadPartSize = downloadPartSize; Review Comment: you might want to have a minimum size for this"},{"author":"Steve Loughran","body":"often the buffer size passed in is small, say 16-32 kb. would that be too small this is a great time to implement openFile() as s3a and abfs does, which lets caller pass in a list of options, a file length/status (saves on the HEAD) and a read policy (random, whole file, sequential). all the open() calls in hadoop codebase now use this and pass in the read policy, length if known, and we do this internally in our own avro jars for for avro file reads so as to guarantee sequential reads of iceberg manifests even in clusters with the s3a read policy == random"},{"author":"ASF GitHub Bot","body":"masteryhx commented on code in PR #5172: URL: https://github.com/apache/hadoop/pull/5172#discussion_r1035582679 ########## hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java: ########## @@ -57,18 +57,21 @@ public class AliyunOSSInputStream extends FSInputStream { private ExecutorService readAheadExecutorService; private Queue readBufferQueue = new ArrayDeque<>(); - public AliyunOSSInputStream(Configuration conf, - ExecutorService readAheadExecutorService, int maxReadAheadPartNumber, - AliyunOSSFileSystemStore store, String key, Long contentLength, - Statistics statistics) throws IOException { + public AliyunOSSInputStream( + long downloadPartSize, + ExecutorService readAheadExecutorService, + int maxReadAheadPartNumber, + AliyunOSSFileSystemStore store, + String key, + Long contentLength, + Statistics statistics) throws IOException { this.readAheadExecutorService = - MoreExecutors.listeningDecorator(readAheadExecutorService); + MoreExecutors.listeningDecorator(readAheadExecutorService); this.store = store; this.key = key; this.statistics = statistics; this.contentLength = contentLength; - downloadPartSize = conf.getLong(MULTIPART_DOWNLOAD_SIZE_KEY, - MULTIPART_DOWNLOAD_SIZE_DEFAULT); + this.downloadPartSize = downloadPartSize; Review Comment: Good point. I'd like use IO_FILE_BUFFER_SIZE_DEFAULT(4KB) as its min size, WDYT?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1331799936 :confetti_ball: **+1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 38s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 1 new or modified test files. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 59s | | trunk passed | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 40s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 36s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 37s | | trunk passed | | +1 :green_heart: | javadoc | 0m 49s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 42s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 1m 0s | | trunk passed | | +1 :green_heart: | shadedclient | 20m 32s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 22s | [/results-checkstyle-hadoop-tools_hadoop-aliyun.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-aliyun.txt) | hadoop-tools/hadoop-aliyun: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) | | +1 :green_heart: | mvnsite | 0m 28s | | the patch passed | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 26s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 20m 11s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 31s | | hadoop-aliyun in the patch passed. | | +1 :green_heart: | asflicense | 0m 41s | | The patch does not generate ASF License warnings. | | | | 93m 23s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/2/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5172 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 2276ff2601e8 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 76ce3cb5dcd36949f916098aa8a58e29c6f7664a | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/2/testReport/ | | Max. process+thread count | 679 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-aliyun U: hadoop-tools/hadoop-aliyun | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5172/2/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5172: URL: https://github.com/apache/hadoop/pull/5172#discussion_r1035921694 ########## hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java: ########## @@ -57,18 +57,21 @@ public class AliyunOSSInputStream extends FSInputStream { private ExecutorService readAheadExecutorService; private Queue readBufferQueue = new ArrayDeque<>(); - public AliyunOSSInputStream(Configuration conf, - ExecutorService readAheadExecutorService, int maxReadAheadPartNumber, - AliyunOSSFileSystemStore store, String key, Long contentLength, - Statistics statistics) throws IOException { + public AliyunOSSInputStream( + long downloadPartSize, + ExecutorService readAheadExecutorService, + int maxReadAheadPartNumber, + AliyunOSSFileSystemStore store, + String key, + Long contentLength, + Statistics statistics) throws IOException { this.readAheadExecutorService = - MoreExecutors.listeningDecorator(readAheadExecutorService); + MoreExecutors.listeningDecorator(readAheadExecutorService); this.store = store; this.key = key; this.statistics = statistics; this.contentLength = contentLength; - downloadPartSize = conf.getLong(MULTIPART_DOWNLOAD_SIZE_KEY, - MULTIPART_DOWNLOAD_SIZE_DEFAULT); + this.downloadPartSize = downloadPartSize; Review Comment: i owrry that the downloadpart size of kb is way too small for efficient http GET requests; kilobytes to megabytes are better"},{"author":"ASF GitHub Bot","body":"masteryhx commented on code in PR #5172: URL: https://github.com/apache/hadoop/pull/5172#discussion_r1035962332 ########## hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java: ########## @@ -57,18 +57,21 @@ public class AliyunOSSInputStream extends FSInputStream { private ExecutorService readAheadExecutorService; private Queue readBufferQueue = new ArrayDeque<>(); - public AliyunOSSInputStream(Configuration conf, - ExecutorService readAheadExecutorService, int maxReadAheadPartNumber, - AliyunOSSFileSystemStore store, String key, Long contentLength, - Statistics statistics) throws IOException { + public AliyunOSSInputStream( + long downloadPartSize, + ExecutorService readAheadExecutorService, + int maxReadAheadPartNumber, + AliyunOSSFileSystemStore store, + String key, + Long contentLength, + Statistics statistics) throws IOException { this.readAheadExecutorService = - MoreExecutors.listeningDecorator(readAheadExecutorService); + MoreExecutors.listeningDecorator(readAheadExecutorService); this.store = store; this.key = key; this.statistics = statistics; this.contentLength = contentLength; - downloadPartSize = conf.getLong(MULTIPART_DOWNLOAD_SIZE_KEY, - MULTIPART_DOWNLOAD_SIZE_DEFAULT); + this.downloadPartSize = downloadPartSize; Review Comment: I think we could see different performance between uploading/requesting 4KB and 4MB ? In my some cases, some data are orgnazied with unit of ~16KB, and I will read them randomly. In this case, I am sure what I need is just these KB, more data will cost more time and bandwidth."},{"author":"ASF GitHub Bot","body":"steveloughran commented on code in PR #5172: URL: https://github.com/apache/hadoop/pull/5172#discussion_r1037378725 ########## hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java: ########## @@ -57,18 +57,21 @@ public class AliyunOSSInputStream extends FSInputStream { private ExecutorService readAheadExecutorService; private Queue readBufferQueue = new ArrayDeque<>(); - public AliyunOSSInputStream(Configuration conf, - ExecutorService readAheadExecutorService, int maxReadAheadPartNumber, - AliyunOSSFileSystemStore store, String key, Long contentLength, - Statistics statistics) throws IOException { + public AliyunOSSInputStream( + long downloadPartSize, + ExecutorService readAheadExecutorService, + int maxReadAheadPartNumber, + AliyunOSSFileSystemStore store, + String key, + Long contentLength, + Statistics statistics) throws IOException { this.readAheadExecutorService = - MoreExecutors.listeningDecorator(readAheadExecutorService); + MoreExecutors.listeningDecorator(readAheadExecutorService); this.store = store; this.key = key; this.statistics = statistics; this.contentLength = contentLength; - downloadPartSize = conf.getLong(MULTIPART_DOWNLOAD_SIZE_KEY, - MULTIPART_DOWNLOAD_SIZE_DEFAULT); + this.downloadPartSize = downloadPartSize; Review Comment: its about the efficiencies of a GET call, overhead of creating HTTPS connections etc. which comes down to * reading a whole file is the wrong strategy for random IO formats (ORC, parquet) * random IO/small ranged GETs wrong for whole files. * even with random io, KB is way too small. This is why there's a tendency for the stores to do adaptive \"first backwards/big forward seek means random IO\", and since HADOOP-16202 let caller declared read policy. If the existing code was changed to say \"we set GET range to be the buffer size you passed in on open()\", then everyone's existing code is going to suffer really badly on performance."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1334108437 sorry, but I'm going to say -1 to using the normal IO buffer size as the GET range. The default value of 4k is way too small even for parquet/orc reads, it will break all existing apps in performance terms: distcp, parquet library, avro, ORC, everything, as they all use the default value. 1. there is a configuration option for multipart download size, which is filesystem-wide. Not as flexible, but something everyone will expect to work. 2. If you want better control of read policy, buffer sizes etc, then this connector needs to implement openFile(), as s3a and abfs do. that will let you add a new option to specify the range for GET calls."},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1334109192 (note also that includes letting you declare read policy (whole-file, sequential, random, vectored....that can be used to change default block size too)"},{"author":"ASF GitHub Bot","body":"masteryhx commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1334751421 > sorry, but I'm going to say -1 to using the normal IO buffer size as the GET range. The default value of 4k is way too small even for parquet/orc reads, it will break all existing apps in performance terms: distcp, parquet library, avro, ORC, everything, as they all use the default value. > > 1. there is a configuration option for multipart download size, which is filesystem-wide. Not as flexible, but something everyone will expect to work. > 2. If you want better control of read policy, buffer sizes etc, then this connector needs to implement openFile(), as s3a and abfs do. that will let you add a new option to specify the range for GET calls. Thanks a lot for your detailed explaination and suggestion. I agree that it's better not to change the implementaion of current interface. I'd like to make oss also implement openFile() in this pr as s3a does which could also meet our needs. WDYT?"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-1335114105 > I'd like to make oss also implement openFile() in this pr as s3a does which could also meet our needs. This is exactly what the API was designed for -to let people provide extra hints/options. to the object stores. Do read the filesystem.md and related docs on the topic, and look at the abfs implementation as well as the s3a one. If your implementation takes a FileStatus or length option, it can then skip the HEAD request on opening and save time and money. All the hadoop internal uses of openFile() do this. My own copies of the parquet and avro readers also do it for better cloud reading performance"},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5172: URL: https://github.com/apache/hadoop/pull/5172#issuecomment-3465487975 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5172: HADOOP-18543. AliyunOSSFileSystem#open(Path path, int bufferSize) use buffer size as its downloadPartSize URL: https://github.com/apache/hadoop/pull/5172"}]}
{"key":"HADOOP-18541","summary":"Upgrade grizzly version to 2.4.4","description":"Upgrade grizzly version to 2.4.4 to resolve |[[sonatype-2016-0415] CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')|https://ossindex.sonatype.org/vulnerability/sonatype-2016-0415?component-type=maven&component-name=org.glassfish.grizzly/grizzly-http-server]| [CVE-2014-0099|https://nvd.nist.gov/vuln/detail/CVE-2014-0099], [CVE-2014-0075|https://nvd.nist.gov/vuln/detail/CVE-2014-0075], [CVE-2017-1000028|https://nvd.nist.gov/vuln/detail/CVE-2017-1000028]","status":"Open","priority":"Major","reporter":"D M Murali Krishna Reddy","assignee":"D M Murali Krishna Reddy","labels":["pull-request-available"],"project":"HADOOP","created":"2022-11-27T19:07:13.000+0000","updated":"2025-10-31T00:23:08.000+0000","comments":[{"author":"ASF GitHub Bot","body":"dmmkr opened a new pull request, #5167: URL: https://github.com/apache/hadoop/pull/5167 ### Description of PR Upgrade grizzly version to 2.4.4 ### How was this patch tested? Local compilation sucessful ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #5167: URL: https://github.com/apache/hadoop/pull/5167#issuecomment-1328561720 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 52s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 0s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 39m 58s | | trunk passed | | +1 :green_heart: | compile | 0m 39s | | trunk passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | compile | 0m 32s | | trunk passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | mvnsite | 0m 35s | | trunk passed | | +1 :green_heart: | javadoc | 0m 37s | | trunk passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 40s | | trunk passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | shadedclient | 63m 11s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 18s | | the patch passed | | +1 :green_heart: | compile | 0m 22s | | the patch passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javac | 0m 22s | | the patch passed | | +1 :green_heart: | compile | 0m 17s | | the patch passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | javac | 0m 17s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 21s | | the patch passed | | +1 :green_heart: | javadoc | 0m 22s | | the patch passed with JDK Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | +1 :green_heart: | shadedclient | 22m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 23s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 45s | | The patch does not generate ASF License warnings. | | | | 90m 18s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5167/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5167 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 84299e3ff4b1 4.15.0-191-generic #202-Ubuntu SMP Thu Aug 4 01:49:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / eb8344d4ce9ba5f4cc06c6a7f4256b6ca9e7e1fb | | Default Java | Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.16+8-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_342-8u342-b07-0ubuntu1~20.04-b07 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5167/1/testReport/ | | Max. process+thread count | 562 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5167/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"D M Murali Krishna Reddy","body":"[~brahmareddy] Can you add me to the contributors list Thanks"},{"author":"ASF GitHub Bot","body":"steveloughran commented on PR #5167: URL: https://github.com/apache/hadoop/pull/5167#issuecomment-1329463338 AFAIK, grizzly is only used in test, so this isn't something we should be distributing. how fussy is grizzly about jersey versions -as if it expects any changes there or nearby it can't be upgraded without production-time classpath incompatibilities?"},{"author":"Steve Loughran","body":"can you tag version and modules affected here, plus \"build\" for build related stuff. thanks"},{"author":"ASF GitHub Bot","body":"pjfanning commented on PR #5167: URL: https://github.com/apache/hadoop/pull/5167#issuecomment-1351730196 tests seem to have passed - maybe it is best to merge this since it only affects tests - probably not really a major candidate for backporting"},{"author":"Rohit Kumar","body":"[~dmmkr] , can we try to upgrade it to 4.0.0 ? One of the downstream Jira [CDPD-48195|https://jira.cloudera.com/browse/CDPD-48195] is asking for the same. It will be easy to sync since I just need to backport it then."},{"author":"ASF GitHub Bot","body":"github-actions[bot] commented on PR #5167: URL: https://github.com/apache/hadoop/pull/5167#issuecomment-3465488487 We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable. If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again. Thanks all for your contribution."},{"author":"ASF GitHub Bot","body":"github-actions[bot] closed pull request #5167: HADOOP-18541. Upgrade grizzly version to 2.4.4 URL: https://github.com/apache/hadoop/pull/5167"}]}
{"key":"HADOOP-19605","summary":"Upgrade Protobuf 3.25.5 for docker images","description":"HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.","status":"Open","priority":"Major","reporter":"Cheng Pan","labels":["pull-request-available"],"project":"HADOOP","created":"2025-07-07T02:22:39.000+0000","updated":"2025-10-31T02:16:58.000+0000","comments":[{"author":"ASF GitHub Bot","body":"GauthamBanasandra commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2196670293 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: Thanks for PR @pan3793. Please let me know once you're done with all the changes and I can verify it on Windows."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248461577 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/console in case of problems."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248556711 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/4/console in case of problems."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3249774993 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 12m 59s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 3s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 21m 20s | | trunk passed | | -1 :x: | compile | 1m 59s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/branch-compile-root.txt) | root in trunk failed. | | +1 :green_heart: | mvnsite | 15m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 72m 25s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 32s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 20m 41s | | the patch passed | | -1 :x: | compile | 2m 16s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | cc | 2m 16s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | -1 :x: | javac | 2m 16s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/patch-compile-root.txt) | root in the patch failed. | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 12m 2s | | the patch passed | | +1 :green_heart: | shadedclient | 36m 42s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 222m 8s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/patch-unit-root.txt) | root in the patch failed. | | +1 :green_heart: | asflicense | 1m 31s | | The patch does not generate ASF License warnings. | | | | 370m 34s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac | | uname | Linux 8e5a65a25879 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / eb4b43e28804ae48b229ac03a65db9f13ae327b6 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/testReport/ | | Max. process+thread count | 4390 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3252905469 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/console in case of problems."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3256642378 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 25s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 0s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 0s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 0s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 54s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 22m 44s | | trunk passed | | -1 :x: | compile | 2m 22s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/artifact/out/branch-compile-root.txt) | root in trunk failed. | | +1 :green_heart: | mvnsite | 15m 44s | | trunk passed | | +1 :green_heart: | shadedclient | 73m 20s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 22m 1s | | the patch passed | | +1 :green_heart: | compile | 7m 53s | | the patch passed | | -1 :x: | cc | 7m 53s | [/results-compile-cc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/artifact/out/results-compile-cc-root.txt) | root generated 138 new + 14 unchanged - 32 fixed = 152 total (was 46) | | +1 :green_heart: | golang | 7m 53s | | the patch passed | | +1 :green_heart: | javac | 7m 53s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 14m 10s | | the patch passed | | +1 :green_heart: | shadedclient | 40m 44s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 752m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 2m 23s | | The patch does not generate ASF License warnings. | | | | 901m 21s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.mapreduce.v2.app.webapp.TestAMWebApp | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | | hadoop.fs.compat.common.TestHdfsCompatShellCommand | | | hadoop.fs.compat.common.TestHdfsCompatDefaultSuites | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac golang | | uname | Linux 2a55a31923ae 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 8434cddca56fd45806321201aefefd63f228d5dc | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/testReport/ | | Max. process+thread count | 4507 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/5/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2324003089 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra Now it passes on all Linux containers. Due to my limited CPP experience, my change might be dirty ..."},{"author":"ASF GitHub Bot","body":"pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2324012372 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra do you remember why we should install `libprotobuf-dev` and `libprotoc-dev` through APT? seems we can remove it and always use the manually installed protobuf instead?"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3284186207 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/console in case of problems."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3286927808 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 19s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 1s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 1s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 1s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 8m 5s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 20m 5s | | trunk passed | | -1 :x: | compile | 2m 3s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/artifact/out/branch-compile-root.txt) | root in trunk failed. | | +1 :green_heart: | mvnsite | 14m 31s | | trunk passed | | +1 :green_heart: | shadedclient | 67m 10s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 35s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 19m 8s | | the patch passed | | +1 :green_heart: | compile | 7m 46s | | the patch passed | | -1 :x: | cc | 7m 46s | [/results-compile-cc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/artifact/out/results-compile-cc-root.txt) | root generated 138 new + 14 unchanged - 32 fixed = 152 total (was 46) | | +1 :green_heart: | golang | 7m 46s | | the patch passed | | +1 :green_heart: | javac | 7m 46s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 12m 4s | | the patch passed | | +1 :green_heart: | shadedclient | 34m 3s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 679m 21s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 2m 25s | | The patch does not generate ASF License warnings. | | | | 825m 26s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService | | | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.hdfs.server.federation.security.token.TestZKDelegationTokenSecretManagerImpl | | | hadoop.mapreduce.v2.TestUberAM | | | hadoop.mapreduce.v2.app.webapp.TestAMWebApp | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac golang | | uname | Linux 3b8eaff37aac 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / cf6a61b0a949f587d3234109b83c98aef9f445f6 | | Default Java | Red Hat, Inc.-1.8.0_462-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/testReport/ | | Max. process+thread count | 4094 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/6/console | | versions | git=2.43.7 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."},{"author":"ASF GitHub Bot","body":"pan3793 commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3388050172 @steveloughran protobuf upgrading is done here, I made it work at least on the Linux platform, but I'm not very familiar with cpp toolchains, the CMake files changes might need to be reviewed by a domain expert."},{"author":"ASF GitHub Bot","body":"GauthamBanasandra commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2441876283 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: These libraries are needed for C/C++ code that uses protobuf. Regarding installing these from `apt` v/s manual installation - I would prefer the former wherever possible as it keeps the environment clean. It's easy to remove or upgrade the libraries when installed through `apt`."},{"author":"ASF GitHub Bot","body":"pan3793 commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2443891194 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: @GauthamBanasandra, but it has already been installed `install-protobuf.sh`. I think the major advantage of manual installation is to make the protobuf version consistent with Java and across all Linux distributions. Anyway, duplicated installation is not a major issue. Could you please review this PR? I tested it in all current Dockerfiles, but my changes in CMake files might be dirty (sorry, I'm not familiar with cpp toolchains)"},{"author":"ASF GitHub Bot","body":"pan3793 commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3467197395 Rebased on trunk to trigger CI again (no code changes)"},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3467205357 (!) A patch to the testing environment has been detected. Re-executing against the patched versions to perform further tests. The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/console in case of problems."},{"author":"ASF GitHub Bot","body":"hadoop-yetus commented on PR #7780: URL: https://github.com/apache/hadoop/pull/7780#issuecomment-3471034141 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 13m 31s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 0s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 0s | | detect-secrets was not available. | | +0 :ok: | hadolint | 0m 0s | | hadolint was not available. | | +0 :ok: | shellcheck | 0m 1s | | Shellcheck was not available. | | +0 :ok: | shelldocs | 0m 1s | | Shelldocs was not available. | | +0 :ok: | jsonlint | 0m 1s | | jsonlint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | +1 :green_heart: | test4tests | 0m 0s | | The patch appears to include 3 new or modified test files. | |||| _ trunk Compile Tests _ | | +0 :ok: | mvndep | 7m 18s | | Maven dependency ordering for branch | | +1 :green_heart: | mvninstall | 23m 38s | | trunk passed | | -1 :x: | compile | 2m 1s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/branch-compile-root.txt) | root in trunk failed. | | -1 :x: | mvnsite | 11m 8s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/branch-mvnsite-root.txt) | root in trunk failed. | | +1 :green_heart: | shadedclient | 66m 2s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +0 :ok: | mvndep | 0m 36s | | Maven dependency ordering for patch | | +1 :green_heart: | mvninstall | 23m 15s | | the patch passed | | +1 :green_heart: | compile | 7m 48s | | the patch passed | | -1 :x: | cc | 7m 48s | [/results-compile-cc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/results-compile-cc-root.txt) | root generated 138 new + 14 unchanged - 3 fixed = 152 total (was 17) | | +1 :green_heart: | golang | 7m 48s | | the patch passed | | +1 :green_heart: | javac | 7m 48s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -1 :x: | mvnsite | 11m 49s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/patch-mvnsite-root.txt) | root in the patch failed. | | +1 :green_heart: | shadedclient | 33m 31s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 803m 58s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/patch-unit-root.txt) | root in the patch passed. | | +1 :green_heart: | asflicense | 2m 23s | | The patch does not generate ASF License warnings. | | | | 952m 47s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler | | | hadoop.yarn.server.router.webapp.TestFederationWebApp | | | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST | | | hadoop.mapreduce.v2.app.webapp.TestAMWebApp | | | hadoop.security.ssl.TestDelegatingSSLSocketFactory | | | hadoop.yarn.sls.appmaster.TestAMSimulator | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/7780 | | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs mvnsite unit jsonlint compile cc javac golang | | uname | Linux 1fe228f2ff8b 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / c4ffa0a889d95351da49731b6885f9e8d1a4fea4 | | Default Java | Red Hat, Inc.-1.8.0_472-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/testReport/ | | Max. process+thread count | 4015 (vs. ulimit of 5500) | | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/7/console | | versions | git=2.43.7 maven=3.9.11 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."}]}
