{"key":"KAFKA-19833","summary":"Refactor Nullable Types to Use a Unified Pattern","description":"see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676] Regarding the implementation of the nullable vs non-nullable types. We use 3 different approaches. # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES. # For array, we use one class ArraryOf, which takes a nullable param. # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA. We need to pick one approach to implement all nullable types in a consistent way.","status":"Open","priority":"Major","reporter":"Lan Ding","assignee":"Lan Ding","labels":[],"project":"KAFKA","created":"2025-10-25T03:21:54.000+0000","updated":"2025-10-25T03:21:54.000+0000","comments":[]}
{"key":"KAFKA-18679","summary":"KafkaRaftMetrics metrics are exposing doubles instead of integers","description":"The following metrics are being exposed as floating point doubles instead of ints/longs: * log-end-offset * log-end-epoch * number-unkown-voter-connections * current-leader * current-vote * current-epoch * high-watermark This issue extends to a lot of other metrics, which may be intending to report only integer/long values, but are instead reporting doubles. Link to GH discussion detailing issue further: https://github.com/apache/kafka/pull/18304#discussion_r1934364595","status":"Open","priority":"Major","reporter":"Kevin Wu","assignee":"TaiJuWu","labels":[],"project":"KAFKA","created":"2025-01-30T16:41:17.000+0000","updated":"2025-10-25T03:41:22.000+0000","comments":[{"author":"TaiJuWu","body":"Hi [~kevinwu2412] , may I pick this up?"}]}
{"key":"KAFKA-19638","summary":"NPE in `Processor#init()` accessing state store","description":"As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release. Stacktrace: {code:java} 15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now. org.apache.kafka.streams.errors.StreamsException: failed to initialize processor random-value-processor at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:132) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:141) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamTask.initializeTopology(StreamTask.java:1109) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:297) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:955) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:1417) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1219) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:934) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894) [kafka-streams-4.2.0-SNAPSHOT.jar:?] Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.streams.processor.internals.ProcessorRecordContext.timestamp()\" because the return value of \"org.apache.kafka.streams.processor.internals.InternalProcessorContext.recordContext()\" is null at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.put(ChangeLoggingKeyValueBytesStore.java:69) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.put(ChangeLoggingKeyValueBytesStore.java:32) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$put$6(MeteredKeyValueStore.java:303) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:901) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.put(MeteredKeyValueStore.java:303) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator$KeyValueStoreReadWriteDecorator.put(AbstractReadWriteDecorator.java:123) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at io.littlehorse.simulations.stateful.app.RandomValueProcessor.init(RandomValueProcessor.java:21) ~[kafka-streams-stateful-unspecified.jar:?] at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:124) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] ... 8 more {code} Thanks [~eduwerc] for reporting the issue. We clearly have a testing gap, not trying to use a state store within `Processor#init()`. – We need to close this gap. However, there is also the question if using zero as surrogate ts for this case (as the old code does), is a good solution or not? – We could try to use stream-time, but for the very first startup of an application, we also do not have stream-time established yet, so we kinda push the can down the road.","status":"Patch Available","priority":"Blocker","reporter":"Matthias J. Sax","assignee":"Eduwer Camacaro","labels":[],"project":"KAFKA","created":"2025-08-22T22:33:33.000+0000","updated":"2025-10-25T06:07:10.000+0000","comments":[{"author":"Jainil Rana","body":"Hi @mjsax, I’d like to work on this issue and submit a patch, can you please assign it to me?"}]}
{"key":"KAFKA-19634","summary":"document the encoding of nullable struct","description":"In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.","status":"Open","priority":"Major","reporter":"Jun Rao","assignee":"Lan Ding","labels":[],"project":"KAFKA","created":"2025-08-22T00:05:00.000+0000","updated":"2025-10-25T07:12:47.000+0000","comments":[{"author":"Lan Ding","body":"Hi [~junrao], if you are not working on this, may i take it over, thanks."},{"author":"Jun Rao","body":"[~isding_l] : Thanks for your interest. Feel free to take it."},{"author":"Jun Rao","body":"[~isding_l]: Any progress on this? Thanks."},{"author":"Lan Ding","body":"Hi [~junrao], I apologize, this slipped through the cracks. Thanks for the ping. I'll handle this within the next three days. Appreciate your patience."},{"author":"Lan Ding","body":"Hi [~junrao], I have a question regarding the nullability of Struct. Under what circumstances could a Struct potentially be null? Looking at the current implementation, a Struct holds a reference to a Schema, and the Schema's isNullable() method returns false by default. This suggests that Struct should not be nullable at all from a design perspective. What confuses me is how this aligns with handling nullable structs at the protocol level, especially when compared to other nullable types like NullableString where null handling is embedded within the type definition itself. Could you help clarify if I'm missing something here? Looking forward to your insights."},{"author":"Jun Rao","body":"[~isding_l]: Here is an example. ConsumerGroupHeartbeatResponse has a field Assignment, which is a nullable struct. {code:java} { \"name\": \"Assignment\", \"type\": \"Assignment\", \"versions\": \"0+\", \"nullableVersions\": \"0+\", \"default\": \"null\", \"about\": \"null if not provided; the assignment otherwise.\", \"fields\": [ { \"name\": \"TopicPartitions\", \"type\": \"[]TopicPartitions\", \"versions\": \"0+\", \"about\": \"The partitions assigned to the member that can be used immediately.\" } ]} {code} The generated class ConsumerGroupHeartbeatResponseData has the following code for serialization related to a null value. {code:java} if (assignment == null) { _writable.writeByte((byte) -1); } else { _writable.writeByte((byte) 1); assignment.write(_writable, _cache, _version); } {code}"},{"author":"Lan Ding","body":"Thanks for the helpful details! I've opened a PR with the nullable struct documentation. [KAFKA-19634|https://github.com/apache/kafka/pull/20614] PTAL when you get a chance, thanks in advince."}]}
{"key":"KAFKA-19340","summary":"Move DelayedRemoteFetch to the storage module","description":"Move DelayedRemoteFetch to storage module and rewrite it to java.","status":"Resolved","priority":"Minor","reporter":"Lan Ding","assignee":"Lan Ding","labels":[],"project":"KAFKA","created":"2025-05-27T12:08:39.000+0000","updated":"2025-10-25T08:43:10.000+0000","comments":[]}
{"key":"KAFKA-19802","summary":"Update ShareGroupCommand to use share partition lag information","description":"","status":"Open","priority":"Minor","reporter":"Chirag Wadhwa","assignee":"Andrew Schofield","labels":[],"project":"KAFKA","created":"2025-10-16T08:29:07.000+0000","updated":"2025-10-25T11:09:24.000+0000","comments":[]}
{"key":"KAFKA-19803","summary":"Relax state directory file system restrictions","description":"The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept. We could also make this configurable, which would require a KIP. KIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]","status":"Resolved","priority":"Minor","reporter":"Matthias J. Sax","assignee":"Nikita Shupletsov","labels":["kip"],"project":"KAFKA","created":"2025-10-16T18:36:27.000+0000","updated":"2025-10-25T16:14:42.000+0000","comments":[]}
{"key":"KAFKA-19834","summary":"Cleanup suppressions.xml","description":"Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.","status":"Open","priority":"Minor","reporter":"majialong","assignee":"majialong","labels":[],"project":"KAFKA","created":"2025-10-25T16:30:53.000+0000","updated":"2025-10-25T16:30:53.000+0000","comments":[]}
{"key":"KAFKA-19812","summary":"Unbound Error Thrown if some variables are not set for SASL/SSL configuration","description":"I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}. This issue is {*}just an enhancement to properly log the error details{*}.","status":"Open","priority":"Minor","reporter":"Manas Poddar","labels":[],"project":"KAFKA","created":"2025-10-19T14:13:48.000+0000","updated":"2025-10-25T19:40:27.000+0000","comments":[{"author":"crw31","body":"Hi [~scienmanas] , I’d like to take this issue and work on improving the error handling for missing SASL/SSL environment variables in the Docker setup. Could you please assign this to me?"},{"author":"Manas Poddar","body":"Hi [~crw31] , I guess I don't have access to assign the issues."},{"author":"crw31","body":"Hi [~scienmanas] , thanks for letting me know. Since I don’t have assign permissions, I’ll start working on this. If someone with commit rights wants to assign the ticket later, that would be great. Thanks!"}]}
{"key":"KAFKA-19810","summary":"Kafka streams with chained emitStrategy(onWindowClose) example does not work","description":"Hi, I got this example by using the following prompt in Google: # kafka streams unit testing with chained \"emitStrategy\" # Provide an example of testing chained suppress with different grace periods [https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d] Compiled and ran the example using latest kafka jars only to get [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694) at com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123) It appears that the test is not able to drive the kafka stream to emit the 2nd event. Could be a bug in test code/test driver/kafka streams? Thanks in advance Greg","status":"Resolved","priority":"Major","reporter":"Greg F","labels":[],"project":"KAFKA","created":"2025-10-19T06:01:45.000+0000","updated":"2025-10-25T22:06:00.000+0000","comments":[{"author":"Matthias J. Sax","body":"The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet."},{"author":"Greg F","body":"Thanks. is this a bug or a feature? I don't want several 1-minute windows for this test How do I fix the test so the end result (finalOutputTopic) contains a single aggregated value?"},{"author":"Matthias J. Sax","body":"It's not a bug, but behavior by design. For this particular test, you would need to send one more even, with ts => 10:08:30, to close the [10:07; 10:08) window; when this window gets closed the result goes into the second window operator, advancing the time there accordingly emitting the result of window [10:00; 10:05)"},{"author":"Greg F","body":"Hi you may close this issue as you wish. I have another one that I am about to open, hope to get your help with it. https://issues.apache.org/jira/browse/KAFKA-19828 Thanks"}]}
{"key":"KAFKA-19836","summary":"Decouple ConsumerConfig and ShareConsumerConfig","description":"ShareConsumerConfig and ConsumerConfig are inherent at the moment. The drawback is the config logic is mixed, for example: ShareAcknowledgementMode. We can decouple to prevent the logic is complicated in the future.","status":"Resolved","priority":"Major","reporter":"TaiJuWu","assignee":"TaiJuWu","labels":[],"project":"KAFKA","created":"2025-10-26T07:31:58.000+0000","updated":"2025-10-26T11:19:09.000+0000","comments":[{"author":"TaiJuWu","body":"Feel free to close if this is not necessary."}]}
{"key":"KAFKA-19777","summary":"Generator | Fix order of arguments to assertEquals in unit tests","description":"This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.","status":"Open","priority":"Trivial","reporter":"Ksolves India Limited","assignee":"Ksolves India Limited","labels":[],"project":"KAFKA","created":"2025-10-10T07:27:28.000+0000","updated":"2025-10-26T15:59:06.000+0000","comments":[]}
{"key":"KAFKA-19132","summary":"Move FetchSession and related classes to server module","description":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/FetchSession.scala","status":"Patch Available","priority":"Minor","reporter":"Dmitry Werner","assignee":"Dmitry Werner","labels":[],"project":"KAFKA","created":"2025-04-12T07:32:57.000+0000","updated":"2025-10-26T17:19:39.000+0000","comments":[]}
{"key":"KAFKA-19486","summary":"Always use the latest version of kafka-topics.sh to create topics in system tests","description":"Using \"old\" kafka-topics.sh to create topics on \"old\" brokers is stable, but it also has some disadvantages. 1. E2E does not cover the case of using \"new\" kafka-topics.sh on \"old\" brokers 2. it requires a bunch of conditions for \"zk\", since some old kafka-topics.sh require using zk connection In short, we should always use latest kafka-topics.sh to create topics on \"old\" brokers","status":"Open","priority":"Major","reporter":"Chia-Ping Tsai","assignee":"Chih-Yuan Chien","labels":[],"project":"KAFKA","created":"2025-07-09T13:44:51.000+0000","updated":"2025-10-27T03:33:04.000+0000","comments":[]}
{"key":"KAFKA-18379","summary":"Enforce resigned cannot transition to any other state in same epoch","description":"","status":"Open","priority":"Major","reporter":"Alyssa Huang","assignee":"TengYao Chi","labels":[],"project":"KAFKA","created":"2024-12-30T19:20:53.000+0000","updated":"2025-10-27T03:33:22.000+0000","comments":[{"author":"TengYao Chi","body":"Hi [~alyssahuang] I would take over this issue, thanks :)"},{"author":"Alyssa Huang","body":"Thanks [~frankvicky]! I can help review whenever it is ready. If it's alright with you though, perhaps this can wait for after KAFKA-17642 is complete (by code freeze)? That will change what transitions are possible from Resigned (i.e. Resigned must transition to Unattached with epoch + 1) and also re-organizes QuorumStateTest which will impact this ticket."},{"author":"TengYao Chi","body":"Hi [~alyssahuang] Thanks for information. I will start working on this ticket after KAFKA-17642 getting merged :)"}]}
{"key":"KAFKA-19756","summary":"Write down the steps for upgrading Gradle","description":"Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below. # upgrade gradle-wrapper.properties to the latest gradle # upgrade dependencies.gradle as well # use latest gradle to run command `gradle wrapper`to update gradlew # update wrapper.gradle to ensure the generated \"download command\" works well","status":"Open","priority":"Minor","reporter":"Chia-Ping Tsai","assignee":"Chih-Yuan Chien","labels":[],"project":"KAFKA","created":"2025-10-06T09:59:20.000+0000","updated":"2025-10-27T04:29:43.000+0000","comments":[]}
{"key":"KAFKA-19762","summary":"Turn on Gradle reproducible builds feature","description":"*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] *Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future) *Related Gradle issues and links:* * [https://github.com/gradle/gradle/issues/34643] * [https://github.com/gradle/gradle/issues/30871] * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives] * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps] *Definition of done (at the minimum):* * *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]","status":"Open","priority":"Minor","reporter":"Dejan Stojadinović","labels":["Gradle","build","gradle"],"project":"KAFKA","created":"2025-10-07T11:23:45.000+0000","updated":"2025-10-27T08:51:59.000+0000","comments":[{"author":"Naveen","body":"Hi [~dejan2609] Per the docs reproducible builds is enabled by deafult on gradle 9 {code:java} Starting with Gradle 9, archives are reproducible by default. {code} is this code block no longer required? {code:java} tasks.withType(AbstractArchiveTask).configureEach { reproducibleFileOrder = false preserveFileTimestamps = true useFileSystemPermissions() } {code} Sources: [Gradle Docks|https://docs.gradle.org/current/userguide/working_with_files.html#sec:reproducible_archives] [Gradle Reproducible Plugin|https://gradlex.org/reproducible-builds/]"},{"author":"Dejan Stojadinović","body":"Hi [~naveenthuvana] My suggestion for you is to try to build with *reproducibleFileOrder = true* (this is a default, so it can be left out) and *preserveFileTimestamps = true*"}]}
{"key":"KAFKA-19761","summary":"Reorder Gradle tasks (in order to bump Shadow plugin version)","description":"*Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027] *Scenario:* * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+ * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x) *Action points (what needs to be done):* * use `com.gradleup.shadow` recent version (9+) * reorder Gradle tasks so that Gradle command mentioned above can work *Definition of done (at the minimum):* * Gradle command mentioned above works as expected * also: *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]","status":"Open","priority":"Minor","reporter":"Dejan Stojadinović","assignee":"Nishanth","labels":["Gradle","build","gradle"],"project":"KAFKA","created":"2025-10-07T11:13:51.000+0000","updated":"2025-10-27T09:08:36.000+0000","comments":[{"author":"Nishanth","body":"Hi Dejan, Can I pick this up. I am new to open source. I think this could be a good one to start with."},{"author":"Dejan Stojadinović","body":"Hi [~nish4_nth] Although I don't actually decide who can work on this (because I'm not a maintainer but only a contributor) I guess you are free to start if you want to contribute :)"},{"author":"Nishanth","body":"Sure Thanks Dejan for updating the notes as well."}]}
{"key":"KAFKA-19837","summary":"Follow up for KIP-1188","description":"[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.","status":"Open","priority":"Major","reporter":"Mickael Maison","assignee":"Mickael Maison","labels":[],"project":"KAFKA","created":"2025-10-27T13:37:10.000+0000","updated":"2025-10-27T13:39:57.000+0000","comments":[]}
{"key":"KAFKA-19838","summary":"Follow up for KIP-1217","description":"[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.","status":"Open","priority":"Major","reporter":"Mickael Maison","assignee":"Mickael Maison","labels":[],"project":"KAFKA","created":"2025-10-27T13:38:37.000+0000","updated":"2025-10-27T13:41:39.000+0000","comments":[]}
{"key":"KAFKA-19784","summary":"Expose Rack ID in MemberDescription","description":"Currently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field. This causes users to be unable to retrieve member rack information through the Admin API. Rack information is crucial for: * Monitoring and visualization tools * Operational analysis of rack distribution * Diagnosing rack-aware assignment issues In addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent. The PR: [https://github.com/apache/kafka/pull/20691] The KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription","status":"Resolved","priority":"Major","reporter":"fujian","assignee":"fujian","labels":["needs-kip"],"project":"KAFKA","created":"2025-10-12T09:34:16.000+0000","updated":"2025-10-27T14:09:27.000+0000","comments":[]}
{"key":"KAFKA-19823","summary":"PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions","description":"There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than acquiredPartitionsSize","status":"Resolved","priority":"Major","reporter":"Abhinav Dixit","assignee":"Abhinav Dixit","labels":[],"project":"KAFKA","created":"2025-10-22T10:55:44.000+0000","updated":"2025-10-27T21:41:42.000+0000","comments":[]}
{"key":"KAFKA-19683","summary":"Clean up TaskManagerTest","description":"See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533","status":"In Progress","priority":"Blocker","reporter":"Shashank","assignee":"Shashank","labels":[],"project":"KAFKA","created":"2025-09-07T16:14:32.000+0000","updated":"2025-10-27T23:57:14.000+0000","comments":[{"author":"Shashank","body":"Hi [~lucasbru], since the tests in the cleanup of this file are many, I would like to propose to make incremental changes to this cleanup. - Removal of dead tests and address these 3 comments - [#1|https://github.com/apache/kafka/pull/19275#discussion_r2107811068], [#2|https://github.com/apache/kafka/pull/19275#discussion_r2107814832] and [#3|https://github.com/apache/kafka/pull/19275#discussion_r2107828813] made in the previous stale PR ([#19275|https://github.com/apache/kafka/pull/19275]). - Identify and replace tryToCompleteRestoration() with checkStateUpdater() in all tests that require no additional mocking - Modify tests that may require to be rewritten (I still am doing my analysis of this and may need some help) Steps 1 and 2 seem to be straightforward and also I think splitting would make it easier for you to review. Step 3 is where I may need some guidance and help. Do you think this is a good approach?"}]}
{"key":"KAFKA-7138","summary":"Kafka Connect - Make errors.deadletterqueue.topic.replication.factor default consistent","description":"{{errors.deadletterqueue.topic.replication.factor}} defaults to RF 3 The standard out of the box config files override the RF for {{offset.storage.replication.factor}} (and {{config}} and {{status}}) to 1 To make the experience consistent for users (especially new users, running a single-node dev environment), the default RF in effect for {{errors.deadletterqueue.topic.replication.factor}} should also be 1. It would make it easier for devs getting started on single-node setups. For prod people should be actively configuring this stuff anyway, this would get included in that.","status":"Open","priority":"Minor","reporter":"Robin Moffatt","labels":[],"project":"KAFKA","created":"2018-07-06T10:09:26.000+0000","updated":"2025-10-28T02:13:57.000+0000","comments":[{"author":"Randall Hauch","body":"Up to this point we've always made the defaults reflect a production value, since we have had people that don't change the defaults and get into trouble in production usage. We already have a number of replication factor settings, and they all default to '3', and IMO we need to be consistent. What we probably should do is to include more commented out properties w/ comments in the configurations or, as with the other replication factor properties, set them to 1 in the properties files and include a comment that they should be changed to at least '3' for production usage. Ideally, we might consider introducing separate development or quickstart oriented properties files that are good to use for minimal clusters that devs typically run locally, and then change the existing files to be more production-oriented -- tho the latter means we'd have to change system tests to use the quickstart-oriented config files."},{"author":"Robin Moffatt","body":"{quote}as with the other replication factor properties, set them to 1 in the properties files and include a comment that they should be changed to at least '3' for production usage. {quote} I would +1 this approach, as it retains consistency with what we currently do."}]}
{"key":"KAFKA-19439","summary":"OffsetFetch API does not return group level errors correctly with version 1 for 4.0 and 3.9","description":"We need to address https://issues.apache.org/jira/browse/KAFKA-19246 in 4.0 and 3.9. The [commit|https://github.com/apache/kafka/commit/f6a78c4c2b5656c77849573f04c55a1921f19de6] has merge conflicts.","status":"Open","priority":"Blocker","reporter":"David Jacot","assignee":"Kuan Po Tseng","labels":[],"project":"KAFKA","created":"2025-06-26T13:37:46.000+0000","updated":"2025-10-28T03:44:25.000+0000","comments":[{"author":"Kuan Po Tseng","body":"[~dajac], I can help fix conflicts if you don't mind"},{"author":"Kuan Po Tseng","body":"By the way, it would be much easier to resolve conflicts if we could also backport [https://github.com/apache/kafka/pull/19642]"},{"author":"Christo Lolov","body":"Moving the fix version from 4.0.1 to 4.0.2. Let me know if anybody is actively working on this and needs to be included in 4.0.1!"}]}
{"key":"KAFKA-19012","summary":"Messages ending up on the wrong topic","description":"We're experiencing messages very occasionally ending up on a different topic than what they were published to. That is, we publish a message to topicA and consumers of topicB see it and fail to parse it because the message contents are meant for topicA. This has happened for various topics. We've begun adding a header with the intended topic (which we get just by reading the topic from the record that we're about to pass to the OSS client) right before we call producer.send, this header shows the correct topic (which also matches up with the message contents itself). Similarly we're able to use this header and compare it to the actual topic to prevent consuming these misrouted messages, but this is still concerning. Some details: - This happens rarely: it happened approximately once per 10 trillion messages for a few months, though there was a period of a week or so where it happened more frequently (once per 1 trillion messages or so) - It often happens in a small burst, eg 2 or 3 messages very close in time (but from different hosts) will be misrouted - It often but not always coincides with some sort of event in the cluster (a broker restarting or being replaced, network issues causing errors, etc). Also these cluster events happen quite often with no misrouted messages - We run many clusters, it has happened for several of them - There is no pattern between intended and actual topic, other than the intended topic tends to be higher volume ones (but I'd attribute that to there being more messages published -> more occurrences affecting it rather than it being more likely per-message) - It only occurs with clients that are using a non-zero linger - Once it happened with two sequential messages, both were intended for topicA but both ended up on topicB, published by the same host (presumably within the same linger batch) - Most of our clients are 3.2.3 and it has only affected those, most of our brokers are 3.2.3 but it has also happened with a cluster that's running 3.8.1 (but I suspect a client rather than broker problem because of it never happening with clients that use 0 linger)","status":"In Progress","priority":"Blocker","reporter":"Donny Nadolny","assignee":"Kirk True","labels":[],"project":"KAFKA","created":"2025-03-19T12:53:13.000+0000","updated":"2025-10-28T03:44:27.000+0000","comments":[{"author":"Donny Nadolny","body":"Here's our client config: {code:java} partitioner.class = max.request.size = 50331648 request.timeout.ms = 500 delivery.timeout.ms = 10000 max.block.ms = 500 max.in.flight.requests.per.connection = 1 linger.ms = 10 batch.size = 524288 {code} Our custom partitioner extends {{org.apache.kafka.clients.producer.internals.DefaultPartitioner}} and for the {{partition}} and {{onNewBatch}} methods might change the {{Cluster}} object (well, create a copy) to artificially reduce the number of partitions available in certain situations. Here's our broker config: {code:java} listeners=CONTROLLER://:9091,PLAINTEXT://:9092,SSL://:9093,INTERNAL_PLAINTEXT://:9094,INTERNAL_SSL://:9095 listener.security.protocol.map=CONTROLLER:SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,INTERNAL_PLAINTEXT:PLAINTEXT,INTERNAL_SSL:SSL advertised.listeners=CONTROLLER://xx.xx.xx.xx:9091,PLAINTEXT://xx.xx.xx.xx:9092,SSL://xx.xx.xx.xx:9093,INTERNAL_PLAINTEXT://xx.xx.xx.xx:9094,INTERNAL_SSL://xx.xx.xx.xx:9095 control.plane.listener.name=CONTROLLER inter.broker.listener.name=INTERNAL_SSL host.name=xxx broker.rack=xxx num.network.threads=16 num.io.threads=16 socket.send.buffer.bytes=1048576 socket.receive.buffer.bytes=1048576 socket.request.max.bytes=104857600 queued.max.requests=500 log.dirs=xxx num.partitions=8 log.retention.hours=168 log.segment.bytes=536870912 log.retention.check.interval.ms=300000 log.cleaner.enable=true offsets.retention.minutes=10080 replica.fetch.wait.max.ms=100 replica.socket.timeout.ms=1000 replica.lag.time.max.ms=5000 replica.selector.class=org.apache.kafka.common.replica.RackAwareReplicaSelector num.replica.fetchers=18 default.replication.factor=3 offsets.topic.replication.factor=3 zookeeper.connect=xxx zookeeper.connection.timeout.ms=6000 zookeeper.sync.time.ms=2000 inter.broker.protocol.version=3.3 log.message.format.version=3.3 auto.create.topics.enable=false auto.leader.rebalance.enable=true leader.imbalance.per.broker.percentage=20 delete.topic.enable=true min.insync.replicas=2 max.connections.per.ip=2048 unclean.leader.election.enable=false quota.consumer.default=20971520 quota.producer.default=20971520 replica.fetch.max.bytes=16780000 log.message.timestamp.type=LogAppendTime transaction.max.timeout.ms=3600000 transaction.remove.expired.transaction.cleanup.interval.ms=86400000 metric.reporters=com.linkedin.kafka.cruisecontrol.metricsreporter.CruiseControlMetricsReporter cruise.control.metrics.topic=_cruise-control.metrics cruise.control.metrics.reporter.bootstrap.servers=localhost:9093 cruise.control.metrics.reporter.security.protocol=SSL cruise.control.metrics.reporter.ssl.keystore.location=xxx cruise.control.metrics.reporter.ssl.keystore.password=xxx cruise.control.metrics.reporter.ssl.keystore.type=JKS cruise.control.metrics.reporter.ssl.truststore.location=xxx cruise.control.metrics.reporter.ssl.truststore.password=xxx cruise.control.metrics.reporter.ssl.truststore.type=JKS cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm= ssl.keystore.location=xxx ssl.keystore.password=xxx ssl.keystore.type=JKS ssl.key.password=xxx ssl.truststore.location=xxx ssl.truststore.password=xxx ssl.truststore.type=JKS ssl.client.auth=required ssl.enabled.protocols=TLSv1.2 ssl.cipher.suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 authorizer.class.name=<custom authorizer class&gt; principal.builder.class=<custom principal builder class&gt; super.users=xxx ssl.endpoint.identification.algorithm= group.min.session.timeout.ms=500 {code} Some values redacted with x's"},{"author":"Lianet Magrans","body":"Hello [~dnadolny] , thanks for reporting! I'm assuming this is not a transactional producer given the info you shared, is that correct? Also, I guess getting client logs is tricky given that you mention it has happened rarely, but if you get some when the issue happens please share them."},{"author":"David Arthur","body":"[~dnadolny] I know it's a long shot, but could we perhaps see the application code as well?"},{"author":"Artem Livshits","body":"[~dnadolny] a few questions: * what are the versions of the clients that don't have this problem? * when you see misrouted messages, are they all in the same batch (i.e. the whole batch misrouted) or a batch contains both correct and misrouted messages? * when you see bursts, are they always between same topics or different messages in the same burst are misrouted between different topics? * could you check that you don't have \"interceptor.classes\" config defined in any of the clients? * any chance you can share the custom partitioner code?"},{"author":"Kirk True","body":"Taking this task for investigation."},{"author":"Kirk True","body":"[~dnadolny], gentle reminder about the outstanding questions. Thanks!"},{"author":"Donny Nadolny","body":"* what are the versions of the clients that don't have this problem? ** In our setup almost all traffic goes through our proxy, which only runs a single client version (right now 3.3.1). We previously used 2.5.1 for quite a long time and never experienced this, but also didn't experience it for many months after upgrading the client from 2.5.1 -> 3.3.1. There is another system which doesn't use our proxy and uses client 3.4.0 and hasn't experienced these misroutings, though I'm not sure how much to read in to the version difference because it's much lower volume than our proxy and also doesn't use linger.ms which is what my strong suspicion is. Also we run multiple variations of our proxy, the one which does not use linger.ms has not experienced any misrouting and runs the same kafka client version (and same proxy code as well), 3.3.1 currently. * when you see misrouted messages, are they all in the same batch (i.e. the whole batch misrouted) or a batch contains both correct and misrouted messages? ** We don't have enough logging to tell for sure, but what I can say is that we did have one occurrence where two messages to the same topic were both misrouted at the same time (the same proxy instance published them, and they were sequential in the same partition i.e. offset difference of 1), so presumably from the same batch ** We haven't seen multiple messages misrouted at the exact same time (except for the occurrence above). ** These taken together lead me to believe that within a batch of messages only messages destined for the same topic are misrouted when this bug occurs, not all messages published within the linger time * when you see bursts, are they always between same topics or different messages in the same burst are misrouted between different topics? ** It's the latter, different messages to seemingly random topics. eg one occurrence might be a message from high volume topicA -> medium volume topicB, as well as one from topicC -> topicD. then another occurrence could be a single misrouted message from topicA -> topicE * could you check that you don't have \"interceptor.classes\" config defined in any of the clients? ** We do have a rarely used API path which uses a client that is configured with interceptor.classes (a single class which records metrics in {{onAcknowledgement}} and it also adds a header to the event as well as recording metrics in {{{}onSend{}}}), however it is never used in the variant of our proxy that has experienced these misroutings. * any chance you can share the custom partitioner code? ** Possibly - let me check. I'll also see if I can share the interceptor class above"},{"author":"Donny Nadolny","body":"> I know it's a long shot, but could we perhaps see the application code as well? [~davidarthur] Unfortunately I can't share that, though I'll see if I can share the partitioner that we run. > I'm assuming this is not a transactional producer given the info you shared, is that correct? Also, I guess getting client logs is tricky given that you mention it has happened rarely, but if you get some when the issue happens please share them. [~lianetm] correct it's not a transactional producer. On this service logging is turned down a fair bit, I don't believe we had any log lines from the kafka client in these occurrences."},{"author":"Donny Nadolny","body":"Here is the code for the custom partitioner we use. An explanation of what it does: Normally when you repartition a topic, there is a race condition that results in what is effectively data loss if you have consumers that are using offset reset = latest. Say you go from 4 to 8 partitions, and the producer sees the new partitions and publishes to them before the consumer group has rebalanced and begun consuming from the new partitions. Shortly after that, the consumer group rebalances and begins consuming but because it uses offset reset = latest it will skip over those messages that were already published. To avoid that (and for some other reasons too) we have this code which comes in to effect when we're in the process of repartitioning a topic. First we create the new partitions (in the example above readPartitions would go from 4 to 8, but writePartitions would remain 4) and we allow consumers to begin consuming and that's when this code takes effect, even though there are 8 partitions for the purposes of producing we'll only publish to the original 4 partitions. Once all consumers have rebalanced, we can update writePartitions to 8 and allow traffic to flow through to all partitions. Many (maybe all?) of these misroutings have occurred when we are not in the process of repartitioning a topic, i.e. readPartitions == writePartitions for both the original and misrouted topic so {{effectiveCluster}} doesn't change the cluster at all. {code:java} package com.stripe.kafkatools.kproxy.partitioner import java.util.concurrent.atomic.AtomicReference import java.util.{Map => JMap} import com.stripe.kafkatools.config.{ClusterName, Reloading} import com.stripe.kafkatools.kproxy.config.KproxyConfig import org.apache.kafka.clients.producer.ProducerConfig import org.apache.kafka.clients.producer.internals.DefaultPartitioner import org.apache.kafka.common.Cluster import org.slf4j.LoggerFactory import scala.collection.JavaConverters._ import scala.collection._ import scala.reflect.ClassTag object KproxyPartitioner { private val ClusterName = \"CLUSTER_NAME\" private val ReloadingConfig = \"RELOADING_KAFKA_CONFIG\" def config(cluster: ClusterName, kafkaConfig: Reloading[KproxyConfig]): Map[String, Any] = Map( KproxyPartitioner.ClusterName -> cluster.name, KproxyPartitioner.ReloadingConfig -> kafkaConfig, ProducerConfig.PARTITIONER_CLASS_CONFIG -> classOf[KproxyPartitioner].getName ) // only for testing def apply(cluster: ClusterName, config: Reloading[KproxyConfig]): KproxyPartitioner = { val partitioner = new KproxyPartitioner() val partitionerConfig = KproxyPartitioner.config(cluster, config) partitioner.configure(partitionerConfig.asJava) partitioner } } /** * For records with unspecified partition writes it to partitions in range [0, `partitions`) * where `partitions` is read from `KproxyConfig`. This ensures no data loss at consumer during topic repartition. * * * Partitioning Strategy: * * If a partition is specified in a record, use it * If no partition but a key is present choose a partition in range [0, `partitions`) based on a has of the key * If no partition or key is present choose the sticky partition that changes when the batch is full. * */ class KproxyPartitioner extends DefaultPartitioner { private[this] val logger = LoggerFactory.getLogger(getClass) // 2.15. SHOULD NOT use \"var\" as shared state private val kafkaConfigRef = new AtomicReference[Reloading[KproxyConfig]]() private val kafkaClusterNameRef = new AtomicReference[ClusterName]() private def effectiveCluster(topic: String, cluster: Cluster): Cluster = { val writePartitions = kafkaConfigRef .get() .current() .topics(topic) .partitions(kafkaClusterNameRef.get()) .writable val effectiveCluster = if (writePartitions value case _ => sys.error(s\"No value for $key\") } kafkaConfigRef.set(get[Reloading[KproxyConfig]](KproxyPartitioner.ReloadingConfig)) kafkaClusterNameRef.set(ClusterName(get[String](KproxyPartitioner.ClusterName))) logger.info(s\"Configs passed to KproxyPartitioner: ${kafkaConfigRef.get()}\") } /** * Applicable in case of batched writing without a specified key * @param topic * @param cluster * @param prevPartition */ override def onNewBatch(topic: String, cluster: Cluster, prevPartition: Int): Unit = { val newCluster = effectiveCluster(topic, cluster) super.onNewBatch(topic, newCluster, prevPartition) () } } {code} Here's the interceptor class which is very rarely used, it's pretty self explanatory: {code:java} package com.stripe.kafkatools.clients import github.gphat.censorinus.DogStatsDClient import org.apache.kafka.clients.producer.{ProducerInterceptor, ProducerRecord, RecordMetadata} import java.util.{Map => JMap} class ProducerMetricsInterceptor[K, V] extends ProducerInterceptor[K, V] { var metricsTags: List[String] = Nil private val successful = \"success:true\" private val unsuccessful = \"success:false\" private val stats: DogStatsDClient = statsClient private val totalMetric = \"produce.interceptor.total\" override def configure(configs: JMap[String, _]): Unit = { val clientId = produceClientId(configs).getOrElse(\"no-client-id\") metricsTags = s\"client_id:$clientId\" :: Nil } override def onSend(record: ProducerRecord[K, V]): ProducerRecord[K, V] = { stats.increment(totalMetric, tags = metricsTags) try { record.headers.add( TimestampHeaders.Key, TimestampHeaders.fromDouble(TimestampHeaders.getCurrentTimestamp()) ) } catch { case _: IllegalStateException => // Records may be in a read-only state if we've already attempted sending once // If we don't catch this, it'll log a failure (but not fail the send) () } record } override def onAcknowledgement(metadata: RecordMetadata, exception: Exception): Unit = if (exception == null) stats.increment(totalMetric, tags = successful :: metricsTags) else stats.increment(totalMetric, tags = unsuccessful :: metricsTags) override def close(): Unit = () } {code}"},{"author":"Nat Wilson","body":"We've been observing behaviour similar to what [~dnadolny] reported. We use Kafka 3.7.1 clients in a Java-based proxy service that inserts a header with the destination topic at the time that the ProducerRecord is constructed. The brokers are also running Kafka 3.7.1. We're seeing that some very small fraction of events (very roughly 1 in 10^11) are then observed by consumers reading from different topics. We've confirmed that the actual topics these messages are read from don't match topic named in the header. We do not use a custom partitioner or any interceptors. For comparison with what's been shared above, here's a subset of our producer configuration {{batch.size = 786432}} {{buffer.memory = 268435456}} {{delivery.timeout.ms = 10200}} {{enable.idempotence = false}} {{interceptor.classes = []}} {{linger.ms = 10}} {{max.block.ms = 15000}} {{max.in.flight.requests.per.connection = 10}} {{max.request.size = 52428800}} {{partitioner.adaptive.partitioning.enable = true}} {{partitioner.class = null}} {{receive.buffer.bytes = -1}} {{request.timeout.ms = 5000}} {{retries = 1}} {{send.buffer.bytes = -1}} {{transactional.id = null}} We're still collecting information to better characterise when this happens, but I'm happy to share any other data that I can."},{"author":"Jun Rao","body":"[~dnadolny] : Thanks for the reporting this issue. Could you confirm the earliest version of the producer where you see this problem? The description of the jira says 3.2.3, but in the comment, you mentioned that the client is upgraded from 2.5.1 -> 3.3.1. I am asking because we introduced a relatively significant change for the producer in 3.3.0 (https://issues.apache.org/jira/browse/KAFKA-10888)."},{"author":"Donny Nadolny","body":"[~junrao] the earliest producer where we saw this was 3.3.1 (never with 2.5.1 which we ran for a few years), however we had been running 3.3.1 clients for approximately 1 year without experiencing this. The broker version is 3.2.3 for most of the clusters we were running and we saw it, though we upgraded the broker to 3.8.1 and still see it."},{"author":"Jun Rao","body":"Thanks for the info, [~dnadolny]."},{"author":"Kirk True","body":"Quick update: I’m working on instrumenting the {{KafkaProducer}} (and its internals) so that we will emit logs and metrics when this happens. Ideally this instrumentation could be done at a few different points to help narrow down where the mismatch happens."},{"author":"Jun Rao","body":"[~dnadolny] : This PR [https://github.com/apache/kafka/pull/20146] has some instrumentation that could help confirm if there is indeed an issue in the producer code. Would you be willing to try the produce patched with the PR in your environment? The following is a summary on how to use the patched producer. * Set up alerts on the new metrics (topic level) in the producer. If the alerts fire, obtain the warning message in the log. Also, verify if there are mis-routed messages around that time. * If the alerts on the new metrics don't fire, but mis-routed messages still occur, check if the newly added internal header contains the correct topic. If not, it indicates an application issue (i.e. the applications send the message to the wrong topic). Otherwise, it's potentially a producer issue and we need to investigate further. * The PR increases the CPU by about 1%."},{"author":"Donny Nadolny","body":"[~junrao] I'm on vacation now but will be back next week. We can certainly run that in our QA environment which does occasionally experience this issue though much more rarely than prod. If we don't get a hit in a few weeks we can consider running in production but would need to do some more testing."},{"author":"Krzysztof Barczynski","body":"Today we experienced the same issue (ver. 3.9.1). On topic A we found {*}records from 4 different topics{*}. On multiple servers producer metrics revealed *explosion of \"buffer-exhausted-rate\"* during turbulences - might be a hint. !image-2025-08-06-13-34-30-830.png!"},{"author":"Jun Rao","body":"[~dnadolny] : Thanks for your help. Please let us know what you find. [~krisso-b] : Thanks for reporting this issue. A quick look around the logic when the buffer is exhausted doesn't lead to anything suspicious. How often does this occur? If we provide an instrumentation PR for 3.9, would you be willing to deploy the patched producer in your environment to help debugging this issue?"},{"author":"Krzysztof Barczynski","body":"[~junrao] We experienced the issue just once. If we found it's a recurring problem we would probably put more effort for debugging."},{"author":"Jun Rao","body":"[~dnadolny] : Have you gotten a chance to try the patch yet? Thanks."}]}
{"key":"KAFKA-19839","summary":"Native-image (dockerimage) does not work with compression.type=zstd","description":"When sending records to a topic created like this: {quote} admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1) .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\")))); {quote} With a producer configured with: {quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\"); {quote} The server fails with: {quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager) java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] {quote} The file: {{docker/native/native-image-configs/jni-config.json}} seems to have it referred: {quote}{ \"name\":\"com.github.luben.zstd.ZstdInputStreamNoFinalizer\", \"fields\":[\\{ \"name\":\"dstPos\"}, \\{ \"name\":\"srcPos\"}] },{quote}","status":"Open","priority":"Minor","reporter":"Morten Bøgeskov","labels":[],"project":"KAFKA","created":"2025-10-28T07:59:52.000+0000","updated":"2025-10-28T08:27:04.000+0000","comments":[]}
{"key":"KAFKA-19449","summary":"Unexpected UNREVOKED_PARTITIONS to UNRELEASED_PARTITIONS transition in consumer member reconciliation","description":"We transition to `UNRELEASED_PARTITIONS` when advancing to a new target assignment and partitions in the assignment are \"owned\" by another member. Partitions are considered \"owned\" by another member if they are in a member's assignment or partitions pending revocation. Ownership is only updated after reconciliation has finished. When the latest target assignment includes partitions from the partitions pending revocation list, we mistakenly think the partitions are owned by another member and can transition from `UNREVOKED_PARTITIONS` to `UNRELEASED_PARTITIONS` instead.","status":"Open","priority":"Minor","reporter":"Sean Quah","assignee":"Sean Quah","labels":[],"project":"KAFKA","created":"2025-06-30T12:21:09.000+0000","updated":"2025-10-28T10:45:13.000+0000","comments":[]}
{"key":"KAFKA-19042","summary":"Move kafka.api test cases to clients-integration-tests module","description":"This is an umbrella Jira about moving integration tests from kafka.api module to clients-integration-tests module and rewrite them with ClusterTestExtensions. h2. AbstractConsumerTest * ConsumerBounceTest [~taijuwu] * ConsumerWithLegacyMessageFormatIntegrationTest [~lansg] * PlaintextConsumerAssignTest [~taijuwu] * PlaintextConsumerAssignorsTest [~taijuwu] * PlaintextConsumerCallbackTest [~m1a2st] * PlaintextConsumerCommitTest [~m1a2st] * PlaintextConsumerFetchTest [~m1a2st] * PlaintextConsumerPollTest [~m1a2st] * PlaintextConsumerSubscriptionTest [~m1a2st] h3. BaseConsumerTest [~m1a2st] * PlaintextConsumerTest [~m1a2st] * SaslMultiMechanismConsumerTest * SaslPlainPlaintextConsumerTest [~m1a2st] * SaslPlaintextConsumerTest * SaslSslConsumerTest * SslConsumerTest h3. RebootstrapTest * ConsumerRebootstrapTest ---- h2. AbstractSaslTest * SaslClientsWithInvalidCredentialsTest ---- h2. AbstractAuthorizerIntegrationTest * kafka.api.AuthorizerIntegrationTest * org.apache.kafka.tools.consumer.group.AuthorizerIntegrationTest ---- h2. BaseAdminIntegrationTest * PlaintextAdminIntegrationTest * SaslSslAdminIntegrationTest * SslAdminIntegrationTest ---- h2. BaseProducerSendTest * PlaintextProducerSendTest * SslProducerSendTest ---- h2. BaseQuotaTest * ClientIdQuotaTest [~jimmywang611] * UserClientIdQuotaTest [~jimmywang611] * UserQuotaTest [~jimmywang611] ---- h2. EndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationWithOwnerTest * PlaintextEndToEndAuthorizationTest * SslEndToEndAuthorizationTest h3. SaslEndToEndAuthorizationTest * GroupEndToEndAuthorizationTest * SaslGssapiSslEndToEndAuthorizationTest * SaslOAuthBearerSslEndToEndAuthorizationTest * SaslScramSslEndToEndAuthorizationTest ---- h2. Others * AdminClientWithPoliciesIntegrationTest [~yung] * ConsumerTopicCreationTest [~frankvicky] * CustomQuotaCallbackTest * GroupAuthorizerIntegrationTest [~lansg] * GroupCoordinatorIntegrationTest [~yung] * MetricsTest [~yung] * ProducerCompressionTest [~gongxuanzhang] * ProducerFailureHandlingTest [~gongxuanzhang] * ProducerIdExpirationTest [~gongxuanzhang] * ProducerSendWhileDeletionTest [~m1a2st] * TransactionsBounceTest [~yangpoan] * TransactionsExpirationTest [~yangpoan] * TransactionsTest [~m1a2st] * TransactionsWithMaxInFlightOneTest [~yangpoan]","status":"Open","priority":"Major","reporter":"PoAn Yang","assignee":"PoAn Yang","labels":[],"project":"KAFKA","created":"2025-03-25T10:45:00.000+0000","updated":"2025-10-28T12:27:47.000+0000","comments":[{"author":"Jhen-Yung Hsu","body":"Nit: client-integration-tests -> clients-integration-tests I noticed that both the Jira and the current PR title misspell it."},{"author":"PoAn Yang","body":"Thanks. Updated it."},{"author":"Lianet Magrans","body":"Hi [~yangpoan], long time no see! :) Is it still the intention to migrate the remaining consumer/producer/admin client integration tests from core to clients-integration-tests and to the new test infra? And if so, is this the right Jira to follow for updates? (Just trying to know where we are on this). Thanks!"},{"author":"PoAn Yang","body":"Hi [~lianetm], yes, this Jira lists all test cases which will be migrated to clients-integration-tests module. I think most of cases in AbstractConsumerTest and Others sections are finished, because they don't have inherited test cases. For other test cases like BaseConsumerTest, BaseAdminIntegrationTest, etc., we need to find a way to handle inherited test cases. I think we can follow what [~m1a2st] did, using a static function like testSimpleConsumption to reuse the test case in different security protocols. cc [~chia7712]"}]}
{"key":"KAFKA-19827","summary":"Call acknowledgement commit callback at end of waiting calls","description":"The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.","status":"Resolved","priority":"Major","reporter":"Andrew Schofield","assignee":"Andrew Schofield","labels":[],"project":"KAFKA","created":"2025-10-22T14:53:09.000+0000","updated":"2025-10-28T14:45:21.000+0000","comments":[]}
