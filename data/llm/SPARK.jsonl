{"task":"summarization","id":"SPARK-52598::summ","input":"Reorganize docs for Spark Connect\n","metadata":{"project":"SPARK","priority":"Minor","status":"Open"}}
{"task":"classification","id":"SPARK-52598::class","input":"","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"SPARK-52598::qna","question":"Reorganize docs for Spark Connect","answer":"No answer found"}
{"task":"summarization","id":"SPARK-53335::summ","input":"Support `spark.kubernetes.driver.annotateExitException`\nFor jobs which run on kubernetes there is no native concept of diagnostics (like there is in YARN), which means that for debugging and triaging errors users _must_ go to logs. For many jobs which run on YARN this is often not necessary, since the diagnostics contains the root cause reason for failure. Additionally, for platforms which provide automation of failure insights, or make decisions based on failures, there must be a custom solution or deciding why the application failed (e.g. log and stack trace parsing). We should provide a way for yarn-equivalent diagnostics for spark jobs on kubernetes.","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53335::class","input":"For jobs which run on kubernetes there is no native concept of diagnostics (like there is in YARN), which means that for debugging and triaging errors users _must_ go to logs. For many jobs which run on YARN this is often not necessary, since the diagnostics contains the root cause reason for failure. Additionally, for platforms which provide automation of failure insights, or make decisions based on failures, there must be a custom solution or deciding why the application failed (e.g. log and s","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53335::qna","question":"Support `spark.kubernetes.driver.annotateExitException`","answer":"https://github.com/apache/spark/pull/52068"}
{"task":"summarization","id":"SPARK-53880::summ","input":"Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants\nThis goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53880::class","input":"This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53880::qna","question":"Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants","answer":"Issue resolved by pull request 52578 [https://github.com/apache/spark/pull/52578]"}
{"task":"summarization","id":"SPARK-52509::summ","input":"Fallback storage accumulates removed shuffle data\nThe fallback storage removes migrated shuffle data on Spark context shutdown. Ideally, it should remove individual shuffles once they get removed from the Spark context. Otherwise, all shuffle data ever migrated to the fallback storage accumulate on the remote storage until the Spark context shuts down. For long running jobs with a lot of decommissioning, this can be orders of magnitude more data than what is actively been used / usable / referenced.","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-52509::class","input":"The fallback storage removes migrated shuffle data on Spark context shutdown. Ideally, it should remove individual shuffles once they get removed from the Spark context. Otherwise, all shuffle data ever migrated to the fallback storage accumulate on the remote storage until the Spark context shuts down. For long running jobs with a lot of decommissioning, this can be orders of magnitude more data than what is actively been used / usable / referenced.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-52509::qna","question":"Fallback storage accumulates removed shuffle data","answer":"[~enricomi] Do we have to create a custom Spark event(UnregisterShuffle) and add a listener which will remove shuffleId from fallback storage? I am willing to contribte but need guidance."}
{"task":"summarization","id":"SPARK-53942::summ","input":"Support changing stateless shuffle partitions upon restart of streaming query\nWe have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this. The main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53942::class","input":"We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this. The main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the n","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53942::qna","question":"Support changing stateless shuffle partitions upon restart of streaming query","answer":"Issue resolved by pull request 52645 [https://github.com/apache/spark/pull/52645]"}
{"task":"summarization","id":"SPARK-28098::summ","input":"Native ORC reader doesn't support subdirectories with Hive tables\nThe Hive ORC reader supports recursive directory reads from S3. Spark's native ORC reader supports recursive directory reads, but not when used with Hive. {code:java} val testData = List(1,2,3,4,5) val dataFrame = testData.toDF() dataFrame .coalesce(1) .write .mode(SaveMode.Overwrite) .format(\"orc\") .option(\"compression\", \"zlib\") .save(\"s3://ddrinka.sparkbug/dirTest/dir1/dir2/\") spark.sql(\"DROP TABLE IF EXISTS ddrinka_sparkbug.dirTest\") spark.sql(\"CREATE EXTERNAL TABLE ddrinka_sparkbug.dirTest (val INT) STORED AS ORC LOCATION 's3://ddrinka.sparkbug/dirTest/'\") spark.conf.set(\"hive.mapred.supports.subdirectories\",\"true\") spark.conf.set(\"mapred.input.dir.recursive\",\"true\") spark.conf.set(\"mapreduce.input.fileinputformat.input.dir.recursive\",\"true\") spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"true\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //0 spark.conf.set(\"spark.sql.hive.convertMetastoreOrc\", \"false\") println(spark.sql(\"SELECT * FROM ddrinka_sparkbug.dirTest\").count) //5{code}","metadata":{"project":"SPARK","priority":"Major","status":"In Progress"}}
{"task":"classification","id":"SPARK-28098::class","input":"The Hive ORC reader supports recursive directory reads from S3. Spark's native ORC reader supports recursive directory reads, but not when used with Hive. {code:java} val testData = List(1,2,3,4,5) val dataFrame = testData.toDF() dataFrame .coalesce(1) .write .mode(SaveMode.Overwrite) .format(\"orc\") .option(\"compression\", \"zlib\") .save(\"s3://ddrinka.sparkbug/dirTest/dir1/dir2/\") spark.sql(\"DROP TABLE IF EXISTS ddrinka_sparkbug.dirTest\") spark.sql(\"CREATE EXTERNAL TABLE ddrinka_sparkbug.dirTest (","label":{"priority":"Major","status":"In Progress"}}
{"task":"qna","id":"SPARK-28098::qna","question":"Native ORC reader doesn't support subdirectories with Hive tables","answer":"[~ddrinka], do you mind if I ask to check similar stuff as said in https://issues.apache.org/jira/browse/SPARK-28099?focusedCommentId=16868249&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16868249?"}
{"task":"summarization","id":"SPARK-54015::summ","input":"Relex Py4J requirement to 0.10.9.7+\nJVM are compatible with 0.10.9.7+ and above versions have some correctness fixes","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54015::class","input":"JVM are compatible with 0.10.9.7+ and above versions have some correctness fixes","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54015::qna","question":"Relex Py4J requirement to 0.10.9.7+","answer":"Issue resolved by pull request 52721 [https://github.com/apache/spark/pull/52721]"}
{"task":"summarization","id":"SPARK-53002::summ","input":"RocksDB Bounded Memory Fix\nCurrently, RocksDB metrics show 0 bytes used if bounded memory is enabled. This PR will provide an approximation of memory used by fetching the memory used by RocksDB per executor, then dividing it by the open RocksDB instances per executor.","metadata":{"project":"SPARK","priority":"Major","status":"Closed"}}
{"task":"classification","id":"SPARK-53002::class","input":"Currently, RocksDB metrics show 0 bytes used if bounded memory is enabled. This PR will provide an approximation of memory used by fetching the memory used by RocksDB per executor, then dividing it by the open RocksDB instances per executor.","label":{"priority":"Major","status":"Closed"}}
{"task":"qna","id":"SPARK-53002::qna","question":"RocksDB Bounded Memory Fix","answer":"PR merged here - https://github.com/apache/spark/pull/51709"}
{"task":"summarization","id":"SPARK-53406::summ","input":"Support Shuffle Spec in Direct Partition ID Pass Through\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53406::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53406::qna","question":"Support Shuffle Spec in Direct Partition ID Pass Through","answer":"Issue resolved by pull request 52443 [https://github.com/apache/spark/pull/52443]"}
{"task":"summarization","id":"SPARK-54012::summ","input":"Improve Netty usage patterns\n","metadata":{"project":"SPARK","priority":"Critical","status":"Resolved"}}
{"task":"classification","id":"SPARK-54012::class","input":"","label":{"priority":"Critical","status":"Resolved"}}
{"task":"qna","id":"SPARK-54012::qna","question":"Improve Netty usage patterns","answer":"Hi, [~yao]. As a recognition of your contribution, I made this umbrella JIRA issue with `releasenotes` label. Although I have more ideas for this topic and will add some, I believe we are able to mark this as a *resolved* item for Apache Spark 4.1.0 by containing only what we did and what we want to add until the feature freeze. WDYT?"}
{"task":"summarization","id":"SPARK-54018::summ","input":"Upgrade Volcano to 1.13.0\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54018::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54018::qna","question":"Upgrade Volcano to 1.13.0","answer":"Issue resolved by pull request 52722 [https://github.com/apache/spark/pull/52722]"}
{"task":"summarization","id":"SPARK-52790::summ","input":"Introduce new grid testing method which provides better naming\nCurrently, gridTest accepts test name prefix and sequence of parameters. Final test name is made like this `testNamePrefix + s\" ($param)\"`. Which is not good since developers often don't know how final test name would look like and pass here sequence of map, or sequence of booleans, which results in unintuitive test case names. E.g. {code:java} gridTest(\"Select with limit\")(Seq(true, false)) { pushdownEnabled => ... } {code} Will result in registering of next test cases: * Select with limit (true) * Select with limit (false) Instead of that, developers should provide descriptive name suffix: {code:java} gridTest(\"Select with limit\")(Seq( GridTestCase(params = true, suffix = \"pushdown enabled\"), GridTestCase(params = false, suffix = \"pushdown disabled\"), )) { pushdownEnabled => ... } {code} Instead of relying on developers to look for base implementation and make some case class for parameters with overriden `toString` implementation, we should enforce engineers to provide suffix. (Even with proper `toString` implementation, intent of test case may be unknown).","metadata":{"project":"SPARK","priority":"Major","status":"Open"}}
{"task":"classification","id":"SPARK-52790::class","input":"Currently, gridTest accepts test name prefix and sequence of parameters. Final test name is made like this `testNamePrefix + s\" ($param)\"`. Which is not good since developers often don't know how final test name would look like and pass here sequence of map, or sequence of booleans, which results in unintuitive test case names. E.g. {code:java} gridTest(\"Select with limit\")(Seq(true, false)) { pushdownEnabled => ... } {code} Will result in registering of next test cases: * Select with limit (tru","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"SPARK-52790::qna","question":"Introduce new grid testing method which provides better naming","answer":"Currently, gridTest accepts test name prefix and sequence of parameters"}
{"task":"summarization","id":"SPARK-54035::summ","input":"Construct FileStatus from the executor side directly\nhttps://github.com/apache/spark/pull/50765#discussion_r2357607758","metadata":{"project":"SPARK","priority":"Major","status":"Open"}}
{"task":"classification","id":"SPARK-54035::class","input":"https://github.com/apache/spark/pull/50765#discussion_r2357607758","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"SPARK-54035::qna","question":"Construct FileStatus from the executor side directly","answer":"https://github"}
{"task":"summarization","id":"SPARK-52011::summ","input":"Reduce HDFS NameNode RPC on vectorized Parquet reader\n","metadata":{"project":"SPARK","priority":"Major","status":"Open"}}
{"task":"classification","id":"SPARK-52011::class","input":"","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"SPARK-52011::qna","question":"Reduce HDFS NameNode RPC on vectorized Parquet reader","answer":"Hi, [~chengpan]. Do you have more subtasks for this umbrella JIRA issue?"}
{"task":"summarization","id":"SPARK-54032::summ","input":"Prefer to use native Netty transports by default\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54032::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54032::qna","question":"Prefer to use native Netty transports by default","answer":"Issue resolved by pull request 52736 [https://github.com/apache/spark/pull/52736]"}
{"task":"summarization","id":"SPARK-54023::summ","input":"Support `AUTO` Netty IO Mode\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54023::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54023::qna","question":"Support `AUTO` Netty IO Mode","answer":"Issue resolved by pull request 52724 [https://github.com/apache/spark/pull/52724]"}
{"task":"summarization","id":"SPARK-53980::summ","input":"Add `SparkConf.getAllWithPrefix(String, String => K)` API\nWe need to set some config related to S3 for our inner Spark. The implementation of the function show below. {code:java} private def setS3Configs(conf: SparkConf): Unit = { val S3A_PREFIX = \"spark.fs.s3a\" val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\" val s3aConf = conf.getAllWithPrefix(S3A_PREFIX) s3aConf .foreach( confPair => { val keyWithoutPrefix = confPair._1 val oldKey = S3A_PREFIX + keyWithoutPrefix val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix val value = confPair._2 (newKey, value) }) } {code} These code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53980::class","input":"We need to set some config related to S3 for our inner Spark. The implementation of the function show below. {code:java} private def setS3Configs(conf: SparkConf): Unit = { val S3A_PREFIX = \"spark.fs.s3a\" val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\" val s3aConf = conf.getAllWithPrefix(S3A_PREFIX) s3aConf .foreach( confPair => { val keyWithoutPrefix = confPair._1 val oldKey = S3A_PREFIX + keyWithoutPrefix val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix val value = confPair._2 (newK","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53980::qna","question":"Add `SparkConf.getAllWithPrefix(String, String => K)` API","answer":"Issue resolved by pull request 52693 [https://github.com/apache/spark/pull/52693]"}
{"task":"summarization","id":"SPARK-54036::summ","input":"Add `build_python_3.11_macos26.yml` GitHub Action job\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54036::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54036::qna","question":"Add `build_python_3.11_macos26.yml` GitHub Action job","answer":"Issue resolved by pull request 52740 [https://github.com/apache/spark/pull/52740]"}
{"task":"summarization","id":"SPARK-53659::summ","input":"Infer Variant shredding schema in parquet writer\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53659::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53659::qna","question":"Infer Variant shredding schema in parquet writer","answer":"Issue resolved by pull request 52406 [https://github.com/apache/spark/pull/52406]"}
{"task":"summarization","id":"SPARK-53962::summ","input":"Upgrade ASM to 9.9\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53962::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53962::qna","question":"Upgrade ASM to 9.9","answer":"Issue resolved by pull request 52672 [https://github.com/apache/spark/pull/52672]"}
{"task":"summarization","id":"SPARK-54037::summ","input":"Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0\nMy team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database. I have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only. In case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5. I have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines \"Running task x in stage y\" and \"Finished task x in stage y\". Is this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ? (I'll also mention that we are using checkpointing (in case it might be important here))","metadata":{"project":"SPARK","priority":"Major","status":"Open"}}
{"task":"classification","id":"SPARK-54037::class","input":"My team recently updated spark dependency version from 3.5.5 to 4.0.0 This included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic). After this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database. I have performed comparison between application versions w","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"SPARK-54037::qna","question":"Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0","answer":"My team recently updated spark dependency version from 3"}
{"task":"summarization","id":"SPARK-54034::summ","input":"Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly\n","metadata":{"project":"SPARK","priority":"Critical","status":"Resolved"}}
{"task":"classification","id":"SPARK-54034::class","input":"","label":{"priority":"Critical","status":"Resolved"}}
{"task":"qna","id":"SPARK-54034::qna","question":"Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly","answer":"Issue resolved by pull request 52738 [https://github.com/apache/spark/pull/52738]"}
{"task":"summarization","id":"SPARK-54040::summ","input":"Remove unused commons-collections 3.x\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54040::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54040::qna","question":"Remove unused commons-collections 3.x","answer":"Issue resolved by pull request 52743 [https://github.com/apache/spark/pull/52743]"}
{"task":"summarization","id":"SPARK-54006::summ","input":"WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above\nRunning {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with {code} WithAggregationKinesisBackedBlockRDDSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56) at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893) at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45) at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60) at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23) at scala.collection.immutable.Range.foreach(Range.scala:158) at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57) at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134) ... Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56) at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893) at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52) at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45) at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60) at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23) ... {code}","metadata":{"project":"SPARK","priority":"Minor","status":"Open"}}
{"task":"classification","id":"SPARK-54006::class","input":"Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with {code} WithAggregationKinesisBackedBlockRDDSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56) at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893) at com.amazonaws.services.kinesis.producer.Kin","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"SPARK-54006::qna","question":"WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above","answer":"Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with {code} WithAggregationKinesisBackedBlockRDDSuite: *** RUN ABORTED *** java"}
{"task":"summarization","id":"SPARK-54043::summ","input":"Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54043::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54043::qna","question":"Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1","answer":"Issue resolved by pull request 252 [https://github.com/apache/spark-connect-swift/pull/252]"}
{"task":"summarization","id":"SPARK-54042::summ","input":"Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54042::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54042::qna","question":"Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`","answer":"Issue resolved by pull request 251 [https://github.com/apache/spark-connect-swift/pull/251]"}
{"task":"summarization","id":"SPARK-54044::summ","input":"Upgrade `gRPC Swift NIO Transport` to 2.2.0\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-54044::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-54044::qna","question":"Upgrade `gRPC Swift NIO Transport` to 2.2.0","answer":"Issue resolved by pull request 253 [https://github.com/apache/spark-connect-swift/pull/253]"}
{"task":"summarization","id":"SPARK-51127::summ","input":"Kill the Python worker on idle timeout.\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-51127::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-51127::qna","question":"Kill the Python worker on idle timeout.","answer":"Issue resolved by pull request 49843 [https://github.com/apache/spark/pull/49843]"}
{"task":"summarization","id":"SPARK-54024::summ","input":"add sbt-dependency-graph to SBT plugins\nThe plugin adds few useful commands to browser dependencies tree in SBT.","metadata":{"project":"SPARK","priority":"Minor","status":"Open"}}
{"task":"classification","id":"SPARK-54024::class","input":"The plugin adds few useful commands to browser dependencies tree in SBT.","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"SPARK-54024::qna","question":"add sbt-dependency-graph to SBT plugins","answer":"The plugin adds few useful commands to browser dependencies tree in SBT"}
{"task":"summarization","id":"SPARK-53732::summ","input":"Add TimeTravelSpec to DataSourceV2Relation\n","metadata":{"project":"SPARK","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"SPARK-53732::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"SPARK-53732::qna","question":"Add TimeTravelSpec to DataSourceV2Relation","answer":"Issue resolved by pull request 52599 [https://github.com/apache/spark/pull/52599]"}
