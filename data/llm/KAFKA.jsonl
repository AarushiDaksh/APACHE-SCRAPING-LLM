{"task":"summarization","id":"KAFKA-19833::summ","input":"Refactor Nullable Types to Use a Unified Pattern\nsee [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676] Regarding the implementation of the nullable vs non-nullable types. We use 3 different approaches. # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES. # For array, we use one class ArraryOf, which takes a nullable param. # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA. We need to pick one approach to implement all nullable types in a consistent way.","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19833::class","input":"see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676] Regarding the implementation of the nullable vs non-nullable types. We use 3 different approaches. # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES. # For array, we use one class ArraryOf, which takes a nullable param. # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA. We need to pick one approach to implement all nullable types in a consistent way.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19833::qna","question":"Refactor Nullable Types to Use a Unified Pattern","answer":"see [https://github"}
{"task":"summarization","id":"KAFKA-18679::summ","input":"KafkaRaftMetrics metrics are exposing doubles instead of integers\nThe following metrics are being exposed as floating point doubles instead of ints/longs: * log-end-offset * log-end-epoch * number-unkown-voter-connections * current-leader * current-vote * current-epoch * high-watermark This issue extends to a lot of other metrics, which may be intending to report only integer/long values, but are instead reporting doubles. Link to GH discussion detailing issue further: https://github.com/apache/kafka/pull/18304#discussion_r1934364595","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-18679::class","input":"The following metrics are being exposed as floating point doubles instead of ints/longs: * log-end-offset * log-end-epoch * number-unkown-voter-connections * current-leader * current-vote * current-epoch * high-watermark This issue extends to a lot of other metrics, which may be intending to report only integer/long values, but are instead reporting doubles. Link to GH discussion detailing issue further: https://github.com/apache/kafka/pull/18304#discussion_r1934364595","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-18679::qna","question":"KafkaRaftMetrics metrics are exposing doubles instead of integers","answer":"Hi [~kevinwu2412] , may I pick this up?"}
{"task":"summarization","id":"KAFKA-19638::summ","input":"NPE in `Processor#init()` accessing state store\nAs reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release. Stacktrace: {code:java} 15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now. org.apache.kafka.streams.errors.StreamsException: failed to initialize processor random-value-processor at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:132) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:141) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamTask.initializeTopology(StreamTask.java:1109) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:297) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:955) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:1417) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1219) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:934) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894) [kafka-streams-4.2.0-SNAPSHOT.jar:?] Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.streams.processor.internals.ProcessorRecordContext.timestamp()\" because the return value of \"org.apache.kafka.streams.processor.internals.InternalProcessorContext.recordContext()\" is null at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.put(ChangeLoggingKeyValueBytesStore.java:69) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.put(ChangeLoggingKeyValueBytesStore.java:32) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$put$6(MeteredKeyValueStore.java:303) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:901) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.put(MeteredKeyValueStore.java:303) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at org.apache.kafka.streams.processor.internals.AbstractReadWriteDecorator$KeyValueStoreReadWriteDecorator.put(AbstractReadWriteDecorator.java:123) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] at io.littlehorse.simulations.stateful.app.RandomValueProcessor.init(RandomValueProcessor.java:21) ~[kafka-streams-stateful-unspecified.jar:?] at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:124) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?] ... 8 more {code} Thanks [~eduwerc] for reporting the issue. We clearly have a testing gap, not trying to use a state store within `Processor#init()`. – We need to close this gap. However, there is also the question if using zero as surrogate ts for this case (as the old code does), is a good solution or not? – We could try to use stream-time, but for the very first startup of an application, we also do not have stream-time established yet, so we kinda push the can down the road.","metadata":{"project":"KAFKA","priority":"Blocker","status":"Patch Available"}}
{"task":"classification","id":"KAFKA-19638::class","input":"As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release. Stacktrace: {code:java} 15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CL","label":{"priority":"Blocker","status":"Patch Available"}}
{"task":"qna","id":"KAFKA-19638::qna","question":"NPE in `Processor#init()` accessing state store","answer":"Hi @mjsax, I’d like to work on this issue and submit a patch, can you please assign it to me?"}
{"task":"summarization","id":"KAFKA-19634::summ","input":"document the encoding of nullable struct\nIn [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19634::class","input":"In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19634::qna","question":"document the encoding of nullable struct","answer":"Hi [~junrao], if you are not working on this, may i take it over, thanks."}
{"task":"summarization","id":"KAFKA-19340::summ","input":"Move DelayedRemoteFetch to the storage module\nMove DelayedRemoteFetch to storage module and rewrite it to java.","metadata":{"project":"KAFKA","priority":"Minor","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19340::class","input":"Move DelayedRemoteFetch to storage module and rewrite it to java.","label":{"priority":"Minor","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19340::qna","question":"Move DelayedRemoteFetch to the storage module","answer":"Move DelayedRemoteFetch to storage module and rewrite it to java"}
{"task":"summarization","id":"KAFKA-19802::summ","input":"Update ShareGroupCommand to use share partition lag information\n","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19802::class","input":"","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19802::qna","question":"Update ShareGroupCommand to use share partition lag information","answer":"No answer found"}
{"task":"summarization","id":"KAFKA-19803::summ","input":"Relax state directory file system restrictions\nThe implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept. We could also make this configurable, which would require a KIP. KIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]","metadata":{"project":"KAFKA","priority":"Minor","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19803::class","input":"The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept. We could also make this configurable, which would require a KIP. KIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]","label":{"priority":"Minor","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19803::qna","question":"Relax state directory file system restrictions","answer":"The implementation of permission restriction by https://issues"}
{"task":"summarization","id":"KAFKA-19834::summ","input":"Cleanup suppressions.xml\nCurrently the rules in suppressions.xml are a bit messy and need to be cleaned up.","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19834::class","input":"Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19834::qna","question":"Cleanup suppressions.xml","answer":"Currently the rules in suppressions"}
{"task":"summarization","id":"KAFKA-19812::summ","input":"Unbound Error Thrown if some variables are not set for SASL/SSL configuration\nI was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}. This issue is {*}just an enhancement to properly log the error details{*}.","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19812::class","input":"I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19812::qna","question":"Unbound Error Thrown if some variables are not set for SASL/SSL configuration","answer":"Hi [~scienmanas] , I’d like to take this issue and work on improving the error handling for missing SASL/SSL environment variables in the Docker setup. Could you please assign this to me?"}
{"task":"summarization","id":"KAFKA-19810::summ","input":"Kafka streams with chained emitStrategy(onWindowClose) example does not work\nHi, I got this example by using the following prompt in Google: # kafka streams unit testing with chained \"emitStrategy\" # Provide an example of testing chained suppress with different grace periods [https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d] Compiled and ran the example using latest kafka jars only to get [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158) at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139) at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694) at com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123) It appears that the test is not able to drive the kafka stream to emit the 2nd event. Could be a bug in test code/test driver/kafka streams? Thanks in advance Greg","metadata":{"project":"KAFKA","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19810::class","input":"Hi, I got this example by using the following prompt in Google: # kafka streams unit testing with chained \"emitStrategy\" # Provide an example of testing chained suppress with different grace periods [https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d] Compiled and ran the example using latest kafka jars only to get [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s expected: but was: at org.junit.jupiter.api.AssertionFailureBuilder.buil","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19810::qna","question":"Kafka streams with chained emitStrategy(onWindowClose) example does not work","answer":"The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet."}
{"task":"summarization","id":"KAFKA-19836::summ","input":"Decouple ConsumerConfig and ShareConsumerConfig\nShareConsumerConfig and ConsumerConfig are inherent at the moment. The drawback is the config logic is mixed, for example: ShareAcknowledgementMode. We can decouple to prevent the logic is complicated in the future.","metadata":{"project":"KAFKA","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19836::class","input":"ShareConsumerConfig and ConsumerConfig are inherent at the moment. The drawback is the config logic is mixed, for example: ShareAcknowledgementMode. We can decouple to prevent the logic is complicated in the future.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19836::qna","question":"Decouple ConsumerConfig and ShareConsumerConfig","answer":"Feel free to close if this is not necessary."}
{"task":"summarization","id":"KAFKA-19777::summ","input":"Generator | Fix order of arguments to assertEquals in unit tests\nThis sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.","metadata":{"project":"KAFKA","priority":"Trivial","status":"Open"}}
{"task":"classification","id":"KAFKA-19777::class","input":"This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.","label":{"priority":"Trivial","status":"Open"}}
{"task":"qna","id":"KAFKA-19777::qna","question":"Generator | Fix order of arguments to assertEquals in unit tests","answer":"This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package"}
{"task":"summarization","id":"KAFKA-19132::summ","input":"Move FetchSession and related classes to server module\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/FetchSession.scala","metadata":{"project":"KAFKA","priority":"Minor","status":"Patch Available"}}
{"task":"classification","id":"KAFKA-19132::class","input":"https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/FetchSession.scala","label":{"priority":"Minor","status":"Patch Available"}}
{"task":"qna","id":"KAFKA-19132::qna","question":"Move FetchSession and related classes to server module","answer":"https://github"}
{"task":"summarization","id":"KAFKA-19486::summ","input":"Always use the latest version of kafka-topics.sh to create topics in system tests\nUsing \"old\" kafka-topics.sh to create topics on \"old\" brokers is stable, but it also has some disadvantages. 1. E2E does not cover the case of using \"new\" kafka-topics.sh on \"old\" brokers 2. it requires a bunch of conditions for \"zk\", since some old kafka-topics.sh require using zk connection In short, we should always use latest kafka-topics.sh to create topics on \"old\" brokers","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19486::class","input":"Using \"old\" kafka-topics.sh to create topics on \"old\" brokers is stable, but it also has some disadvantages. 1. E2E does not cover the case of using \"new\" kafka-topics.sh on \"old\" brokers 2. it requires a bunch of conditions for \"zk\", since some old kafka-topics.sh require using zk connection In short, we should always use latest kafka-topics.sh to create topics on \"old\" brokers","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19486::qna","question":"Always use the latest version of kafka-topics.sh to create topics in system tests","answer":"Using \"old\" kafka-topics"}
{"task":"summarization","id":"KAFKA-18379::summ","input":"Enforce resigned cannot transition to any other state in same epoch\n","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-18379::class","input":"","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-18379::qna","question":"Enforce resigned cannot transition to any other state in same epoch","answer":"Hi [~alyssahuang] I would take over this issue, thanks :)"}
{"task":"summarization","id":"KAFKA-19756::summ","input":"Write down the steps for upgrading Gradle\nNormally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below. # upgrade gradle-wrapper.properties to the latest gradle # upgrade dependencies.gradle as well # use latest gradle to run command `gradle wrapper`to update gradlew # update wrapper.gradle to ensure the generated \"download command\" works well","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19756::class","input":"Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below. # upgrade gradle-wrapper.properties to the latest gradle # upgrade dependencies.gradle as well # use latest gradle to run command `gradle wrapper`to update gradlew # update wrapper.gradle to ensure the generated \"download command\" works well","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19756::qna","question":"Write down the steps for upgrading Gradle","answer":"Normally, we only update `gradle-wrapper"}
{"task":"summarization","id":"KAFKA-19762::summ","input":"Turn on Gradle reproducible builds feature\n*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] *Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future) *Related Gradle issues and links:* * [https://github.com/gradle/gradle/issues/34643] * [https://github.com/gradle/gradle/issues/30871] * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives] * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps] *Definition of done (at the minimum):* * *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19762::class","input":"*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] *Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future) *Related Gradle issues and links:* * [https://github.com/gradle/gradle/issues/34643] * [https://github.com/gradle/gradle/issues/30871] * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives] * [https://docs.g","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19762::qna","question":"Turn on Gradle reproducible builds feature","answer":"Hi [~dejan2609] Per the docs reproducible builds is enabled by deafult on gradle 9 {code:java} Starting with Gradle 9, archives are reproducible by default. {code} is this code block no longer required? {code:java} tasks.withType(AbstractArchiveTask).configureEach { reproducibleFileOrder = false preserveFileTimestamps = true useFileSystemPermissions() } {code} Sources: [Gradle Docks|https://docs.gradle.org/current/userguide/working_with_files.html#sec:reproducible_archives] [Gradle Reproducible Plugin|https://gradlex.org/reproducible-builds/]"}
{"task":"summarization","id":"KAFKA-19761::summ","input":"Reorder Gradle tasks (in order to bump Shadow plugin version)\n*Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027] *Scenario:* * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+ * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x) *Action points (what needs to be done):* * use `com.gradleup.shadow` recent version (9+) * reorder Gradle tasks so that Gradle command mentioned above can work *Definition of done (at the minimum):* * Gradle command mentioned above works as expected * also: *./gradlew releaseTarGz* works as expected * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19761::class","input":"*Prologue:* * JIRA ticket: KAFKA-19174 * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027] *Scenario:* * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+ * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x) *Action points (what needs to be done):* * use `com.gradleup.shadow` recent version (9+) * reorder Gradle tasks so that Gradle command mentioned above can work *Definition of done (at the minimum):* * Gradle command ment","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19761::qna","question":"Reorder Gradle tasks (in order to bump Shadow plugin version)","answer":"Hi Dejan, Can I pick this up. I am new to open source. I think this could be a good one to start with."}
{"task":"summarization","id":"KAFKA-19837::summ","input":"Follow up for KIP-1188\n[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19837::class","input":"[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19837::qna","question":"Follow up for KIP-1188","answer":"[KIP-1188|https://cwiki"}
{"task":"summarization","id":"KAFKA-19838::summ","input":"Follow up for KIP-1217\n[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19838::class","input":"[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19838::qna","question":"Follow up for KIP-1217","answer":"[KIP-1217|https://cwiki"}
{"task":"summarization","id":"KAFKA-19784::summ","input":"Expose Rack ID in MemberDescription\nCurrently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field. This causes users to be unable to retrieve member rack information through the Admin API. Rack information is crucial for: * Monitoring and visualization tools * Operational analysis of rack distribution * Diagnosing rack-aware assignment issues In addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent. The PR: [https://github.com/apache/kafka/pull/20691] The KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription","metadata":{"project":"KAFKA","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19784::class","input":"Currently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field. This causes users to be unable to retrieve member rack information through the Admin API. Rack information is crucial for: * Monitoring and visualization tools * Operational analysis of rack distribution * Diagnosing rack-aware assignment issues In ad","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19784::qna","question":"Expose Rack ID in MemberDescription","answer":"Currently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field"}
{"task":"summarization","id":"KAFKA-19823::summ","input":"PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions\nThere is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than acquiredPartitionsSize","metadata":{"project":"KAFKA","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19823::class","input":"There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than acquiredPartitionsSize","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19823::qna","question":"PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions","answer":"There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy"}
{"task":"summarization","id":"KAFKA-19683::summ","input":"Clean up TaskManagerTest\nSee https://github.com/apache/kafka/pull/20392#issuecomment-3241457533","metadata":{"project":"KAFKA","priority":"Blocker","status":"In Progress"}}
{"task":"classification","id":"KAFKA-19683::class","input":"See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533","label":{"priority":"Blocker","status":"In Progress"}}
{"task":"qna","id":"KAFKA-19683::qna","question":"Clean up TaskManagerTest","answer":"Hi [~lucasbru], since the tests in the cleanup of this file are many, I would like to propose to make incremental changes to this cleanup. - Removal of dead tests and address these 3 comments - [#1|https://github.com/apache/kafka/pull/19275#discussion_r2107811068], [#2|https://github.com/apache/kafka/pull/19275#discussion_r2107814832] and [#3|https://github.com/apache/kafka/pull/19275#discussion_r2107828813] made in the previous stale PR ([#19275|https://github.com/apache/kafka/pull/19275]). - Identify and replace tryToCompleteRestoration() with checkStateUpdater() in all tests that require no additional mocking - Modify tests that may require to be rewritten (I still am doing my analysis of this and may need some help) Steps 1 and 2 seem to be straightforward and also I think splitting would make it easier for you to review. Step 3 is where I may need some guidance and help. Do you think this is a good approach?"}
{"task":"summarization","id":"KAFKA-7138::summ","input":"Kafka Connect - Make errors.deadletterqueue.topic.replication.factor default consistent\n{{errors.deadletterqueue.topic.replication.factor}} defaults to RF 3 The standard out of the box config files override the RF for {{offset.storage.replication.factor}} (and {{config}} and {{status}}) to 1 To make the experience consistent for users (especially new users, running a single-node dev environment), the default RF in effect for {{errors.deadletterqueue.topic.replication.factor}} should also be 1. It would make it easier for devs getting started on single-node setups. For prod people should be actively configuring this stuff anyway, this would get included in that.","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-7138::class","input":"{{errors.deadletterqueue.topic.replication.factor}} defaults to RF 3 The standard out of the box config files override the RF for {{offset.storage.replication.factor}} (and {{config}} and {{status}}) to 1 To make the experience consistent for users (especially new users, running a single-node dev environment), the default RF in effect for {{errors.deadletterqueue.topic.replication.factor}} should also be 1. It would make it easier for devs getting started on single-node setups. For prod people s","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-7138::qna","question":"Kafka Connect - Make errors.deadletterqueue.topic.replication.factor default consistent","answer":"Up to this point we've always made the defaults reflect a production value, since we have had people that don't change the defaults and get into trouble in production usage. We already have a number of replication factor settings, and they all default to '3', and IMO we need to be consistent. What we probably should do is to include more commented out properties w/ comments in the configurations or, as with the other replication factor properties, set them to 1 in the properties files and include a comment that they should be changed to at least '3' for production usage. Ideally, we might consider introducing separate development or quickstart oriented properties files that are good to use for minimal clusters that devs typically run locally, and then change the existing files to be more production-oriented -- tho the latter means we'd have to change system tests to use the quickstart-oriented config files."}
{"task":"summarization","id":"KAFKA-19439::summ","input":"OffsetFetch API does not return group level errors correctly with version 1 for 4.0 and 3.9\nWe need to address https://issues.apache.org/jira/browse/KAFKA-19246 in 4.0 and 3.9. The [commit|https://github.com/apache/kafka/commit/f6a78c4c2b5656c77849573f04c55a1921f19de6] has merge conflicts.","metadata":{"project":"KAFKA","priority":"Blocker","status":"Open"}}
{"task":"classification","id":"KAFKA-19439::class","input":"We need to address https://issues.apache.org/jira/browse/KAFKA-19246 in 4.0 and 3.9. The [commit|https://github.com/apache/kafka/commit/f6a78c4c2b5656c77849573f04c55a1921f19de6] has merge conflicts.","label":{"priority":"Blocker","status":"Open"}}
{"task":"qna","id":"KAFKA-19439::qna","question":"OffsetFetch API does not return group level errors correctly with version 1 for 4.0 and 3.9","answer":"[~dajac], I can help fix conflicts if you don't mind"}
{"task":"summarization","id":"KAFKA-19012::summ","input":"Messages ending up on the wrong topic\nWe're experiencing messages very occasionally ending up on a different topic than what they were published to. That is, we publish a message to topicA and consumers of topicB see it and fail to parse it because the message contents are meant for topicA. This has happened for various topics. We've begun adding a header with the intended topic (which we get just by reading the topic from the record that we're about to pass to the OSS client) right before we call producer.send, this header shows the correct topic (which also matches up with the message contents itself). Similarly we're able to use this header and compare it to the actual topic to prevent consuming these misrouted messages, but this is still concerning. Some details: - This happens rarely: it happened approximately once per 10 trillion messages for a few months, though there was a period of a week or so where it happened more frequently (once per 1 trillion messages or so) - It often happens in a small burst, eg 2 or 3 messages very close in time (but from different hosts) will be misrouted - It often but not always coincides with some sort of event in the cluster (a broker restarting or being replaced, network issues causing errors, etc). Also these cluster events happen quite often with no misrouted messages - We run many clusters, it has happened for several of them - There is no pattern between intended and actual topic, other than the intended topic tends to be higher volume ones (but I'd attribute that to there being more messages published -> more occurrences affecting it rather than it being more likely per-message) - It only occurs with clients that are using a non-zero linger - Once it happened with two sequential messages, both were intended for topicA but both ended up on topicB, published by the same host (presumably within the same linger batch) - Most of our clients are 3.2.3 and it has only affected those, most of our brokers are 3.2.3 but it has also happened with a cluster that's running 3.8.1 (but I suspect a client rather than broker problem because of it never happening with clients that use 0 linger)","metadata":{"project":"KAFKA","priority":"Blocker","status":"In Progress"}}
{"task":"classification","id":"KAFKA-19012::class","input":"We're experiencing messages very occasionally ending up on a different topic than what they were published to. That is, we publish a message to topicA and consumers of topicB see it and fail to parse it because the message contents are meant for topicA. This has happened for various topics. We've begun adding a header with the intended topic (which we get just by reading the topic from the record that we're about to pass to the OSS client) right before we call producer.send, this header shows th","label":{"priority":"Blocker","status":"In Progress"}}
{"task":"qna","id":"KAFKA-19012::qna","question":"Messages ending up on the wrong topic","answer":"Here's our client config: {code:java} partitioner.class = max.request.size = 50331648 request.timeout.ms = 500 delivery.timeout.ms = 10000 max.block.ms = 500 max.in.flight.requests.per.connection = 1 linger.ms = 10 batch.size = 524288 {code} Our custom partitioner extends {{org.apache.kafka.clients.producer.internals.DefaultPartitioner}} and for the {{partition}} and {{onNewBatch}} methods might change the {{Cluster}} object (well, create a copy) to artificially reduce the number of partitions available in certain situations. Here's our broker config: {code:java} listeners=CONTROLLER://:9091,PLAINTEXT://:9092,SSL://:9093,INTERNAL_PLAINTEXT://:9094,INTERNAL_SSL://:9095 listener.security.protocol.map=CONTROLLER:SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,INTERNAL_PLAINTEXT:PLAINTEXT,INTERNAL_SSL:SSL advertised.listeners=CONTROLLER://xx.xx.xx.xx:9091,PLAINTEXT://xx.xx.xx.xx:9092,SSL://xx.xx.xx.xx:9093,INTERNAL_PLAINTEXT://xx.xx.xx.xx:9094,INTERNAL_SSL://xx.xx.xx.xx:9095 control.plane.listener.name=CONTROLLER inter.broker.listener.name=INTERNAL_SSL host.name=xxx broker.rack=xxx num.network.threads=16 num.io.threads=16 socket.send.buffer.bytes=1048576 socket.receive.buffer.bytes=1048576 socket.request.max.bytes=104857600 queued.max.requests=500 log.dirs=xxx num.partitions=8 log.retention.hours=168 log.segment.bytes=536870912 log.retention.check.interval.ms=300000 log.cleaner.enable=true offsets.retention.minutes=10080 replica.fetch.wait.max.ms=100 replica.socket.timeout.ms=1000 replica.lag.time.max.ms=5000 replica.selector.class=org.apache.kafka.common.replica.RackAwareReplicaSelector num.replica.fetchers=18 default.replication.factor=3 offsets.topic.replication.factor=3 zookeeper.connect=xxx zookeeper.connection.timeout.ms=6000 zookeeper.sync.time.ms=2000 inter.broker.protocol.version=3.3 log.message.format.version=3.3 auto.create.topics.enable=false auto.leader.rebalance.enable=true leader.imbalance.per.broker.percentage=20 delete.topic.enable=true min.insync.replicas=2 max.connections.per.ip=2048 unclean.leader.election.enable=false quota.consumer.default=20971520 quota.producer.default=20971520 replica.fetch.max.bytes=16780000 log.message.timestamp.type=LogAppendTime transaction.max.timeout.ms=3600000 transaction.remove.expired.transaction.cleanup.interval.ms=86400000 metric.reporters=com.linkedin.kafka.cruisecontrol.metricsreporter.CruiseControlMetricsReporter cruise.control.metrics.topic=_cruise-control.metrics cruise.control.metrics.reporter.bootstrap.servers=localhost:9093 cruise.control.metrics.reporter.security.protocol=SSL cruise.control.metrics.reporter.ssl.keystore.location=xxx cruise.control.metrics.reporter.ssl.keystore.password=xxx cruise.control.metrics.reporter.ssl.keystore.type=JKS cruise.control.metrics.reporter.ssl.truststore.location=xxx cruise.control.metrics.reporter.ssl.truststore.password=xxx cruise.control.metrics.reporter.ssl.truststore.type=JKS cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm= ssl.keystore.location=xxx ssl.keystore.password=xxx ssl.keystore.type=JKS ssl.key.password=xxx ssl.truststore.location=xxx ssl.truststore.password=xxx ssl.truststore.type=JKS ssl.client.auth=required ssl.enabled.protocols=TLSv1.2 ssl.cipher.suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 authorizer.class.name=<custom authorizer class&gt; principal.builder.class=<custom principal builder class&gt; super.users=xxx ssl.endpoint.identification.algorithm= group.min.session.timeout.ms=500 {code} Some values redacted with x's"}
{"task":"summarization","id":"KAFKA-19839::summ","input":"Native-image (dockerimage) does not work with compression.type=zstd\nWhen sending records to a topic created like this: {quote} admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1) .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\")))); {quote} With a producer configured with: {quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\"); {quote} The server fails with: {quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager) java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] ... at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?] {quote} The file: {{docker/native/native-image-configs/jni-config.json}} seems to have it referred: {quote}{ \"name\":\"com.github.luben.zstd.ZstdInputStreamNoFinalizer\", \"fields\":[\\{ \"name\":\"dstPos\"}, \\{ \"name\":\"srcPos\"}] },{quote}","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19839::class","input":"When sending records to a topic created like this: {quote} admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1) .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\")))); {quote} With a producer configured with: {quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\"); {quote} The server fails with: {quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager) java.lang.No","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19839::qna","question":"Native-image (dockerimage) does not work with compression.type=zstd","answer":"When sending records to a topic created like this: {quote} admin"}
{"task":"summarization","id":"KAFKA-19449::summ","input":"Unexpected UNREVOKED_PARTITIONS to UNRELEASED_PARTITIONS transition in consumer member reconciliation\nWe transition to `UNRELEASED_PARTITIONS` when advancing to a new target assignment and partitions in the assignment are \"owned\" by another member. Partitions are considered \"owned\" by another member if they are in a member's assignment or partitions pending revocation. Ownership is only updated after reconciliation has finished. When the latest target assignment includes partitions from the partitions pending revocation list, we mistakenly think the partitions are owned by another member and can transition from `UNREVOKED_PARTITIONS` to `UNRELEASED_PARTITIONS` instead.","metadata":{"project":"KAFKA","priority":"Minor","status":"Open"}}
{"task":"classification","id":"KAFKA-19449::class","input":"We transition to `UNRELEASED_PARTITIONS` when advancing to a new target assignment and partitions in the assignment are \"owned\" by another member. Partitions are considered \"owned\" by another member if they are in a member's assignment or partitions pending revocation. Ownership is only updated after reconciliation has finished. When the latest target assignment includes partitions from the partitions pending revocation list, we mistakenly think the partitions are owned by another member and can","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"KAFKA-19449::qna","question":"Unexpected UNREVOKED_PARTITIONS to UNRELEASED_PARTITIONS transition in consumer member reconciliation","answer":"We transition to `UNRELEASED_PARTITIONS` when advancing to a new target assignment and partitions in the assignment are \"owned\" by another member"}
{"task":"summarization","id":"KAFKA-19042::summ","input":"Move kafka.api test cases to clients-integration-tests module\nThis is an umbrella Jira about moving integration tests from kafka.api module to clients-integration-tests module and rewrite them with ClusterTestExtensions. h2. AbstractConsumerTest * ConsumerBounceTest [~taijuwu] * ConsumerWithLegacyMessageFormatIntegrationTest [~lansg] * PlaintextConsumerAssignTest [~taijuwu] * PlaintextConsumerAssignorsTest [~taijuwu] * PlaintextConsumerCallbackTest [~m1a2st] * PlaintextConsumerCommitTest [~m1a2st] * PlaintextConsumerFetchTest [~m1a2st] * PlaintextConsumerPollTest [~m1a2st] * PlaintextConsumerSubscriptionTest [~m1a2st] h3. BaseConsumerTest [~m1a2st] * PlaintextConsumerTest [~m1a2st] * SaslMultiMechanismConsumerTest * SaslPlainPlaintextConsumerTest [~m1a2st] * SaslPlaintextConsumerTest * SaslSslConsumerTest * SslConsumerTest h3. RebootstrapTest * ConsumerRebootstrapTest ---- h2. AbstractSaslTest * SaslClientsWithInvalidCredentialsTest ---- h2. AbstractAuthorizerIntegrationTest * kafka.api.AuthorizerIntegrationTest * org.apache.kafka.tools.consumer.group.AuthorizerIntegrationTest ---- h2. BaseAdminIntegrationTest * PlaintextAdminIntegrationTest * SaslSslAdminIntegrationTest * SslAdminIntegrationTest ---- h2. BaseProducerSendTest * PlaintextProducerSendTest * SslProducerSendTest ---- h2. BaseQuotaTest * ClientIdQuotaTest [~jimmywang611] * UserClientIdQuotaTest [~jimmywang611] * UserQuotaTest [~jimmywang611] ---- h2. EndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationTest * DelegationTokenEndToEndAuthorizationWithOwnerTest * PlaintextEndToEndAuthorizationTest * SslEndToEndAuthorizationTest h3. SaslEndToEndAuthorizationTest * GroupEndToEndAuthorizationTest * SaslGssapiSslEndToEndAuthorizationTest * SaslOAuthBearerSslEndToEndAuthorizationTest * SaslScramSslEndToEndAuthorizationTest ---- h2. Others * AdminClientWithPoliciesIntegrationTest [~yung] * ConsumerTopicCreationTest [~frankvicky] * CustomQuotaCallbackTest * GroupAuthorizerIntegrationTest [~lansg] * GroupCoordinatorIntegrationTest [~yung] * MetricsTest [~yung] * ProducerCompressionTest [~gongxuanzhang] * ProducerFailureHandlingTest [~gongxuanzhang] * ProducerIdExpirationTest [~gongxuanzhang] * ProducerSendWhileDeletionTest [~m1a2st] * TransactionsBounceTest [~yangpoan] * TransactionsExpirationTest [~yangpoan] * TransactionsTest [~m1a2st] * TransactionsWithMaxInFlightOneTest [~yangpoan]","metadata":{"project":"KAFKA","priority":"Major","status":"Open"}}
{"task":"classification","id":"KAFKA-19042::class","input":"This is an umbrella Jira about moving integration tests from kafka.api module to clients-integration-tests module and rewrite them with ClusterTestExtensions. h2. AbstractConsumerTest * ConsumerBounceTest [~taijuwu] * ConsumerWithLegacyMessageFormatIntegrationTest [~lansg] * PlaintextConsumerAssignTest [~taijuwu] * PlaintextConsumerAssignorsTest [~taijuwu] * PlaintextConsumerCallbackTest [~m1a2st] * PlaintextConsumerCommitTest [~m1a2st] * PlaintextConsumerFetchTest [~m1a2st] * PlaintextConsumerP","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"KAFKA-19042::qna","question":"Move kafka.api test cases to clients-integration-tests module","answer":"Nit: client-integration-tests -> clients-integration-tests I noticed that both the Jira and the current PR title misspell it."}
{"task":"summarization","id":"KAFKA-19827::summ","input":"Call acknowledgement commit callback at end of waiting calls\nThe acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.","metadata":{"project":"KAFKA","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"KAFKA-19827::class","input":"The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"KAFKA-19827::qna","question":"Call acknowledgement commit callback at end of waiting calls","answer":"The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods"}
