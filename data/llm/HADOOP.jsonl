{"task":"summarization","id":"HADOOP-18637::summ","input":"S3A to support upload of files greater than 2 GB using DiskBlocks\nUse S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. cc: [~mthakur] [~stevel@apache.org] [~mehakmeet]","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-18637::class","input":"Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. cc: [~mthakur] [~stevel@apache.org] [~mehakmeet]","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-18637::qna","question":"S3A to support upload of files greater than 2 GB using DiskBlocks","answer":"S3ABlockOutputStream creates three different types of DataBlock depending upon the {{fs.s3a.fast.upload.buffer}} which defaults to disk, we can create an empty file for the same size and limit the Buffer size to {{Integer.MAX_VALUE}} . *For other buffer types should we deny uploads larger than 2 Gigs or should we add the support there as well?* like for {{ByteArrayBlock}} which writes directly to the {{S3AByteArrayOutputStream}} which will be again initialized with {{Integer.MAX_Value}} .The same goes for {{ByteBufferBlock}} as well. One thing to make sure of here is that it's never gonna write something larger than {{Integer.MAX_VALUE}} as the calling function to write has the signature {{public synchronized void write(byte[] source, int offset, int len)}} (S3ABlockOutputStream). *This is just for compatibility with non-AWS s3 stores.*"}
{"task":"summarization","id":"HADOOP-18657::summ","input":"Tune ABFS create() retry logic\nBased on experience trying to debug this happening # add debug statements when create() fails # generated exception text to reference string shared with tests, path and error code # generated exception to include inner exception for full stack trace Currently the retry logic is # create(overwrite=false) # if HTTP_CONFLICT/409 raised; call HEAD # use etag in create(path, overwrite=true, etag) # special handling of error HTTP_PRECON_FAILED = 412 There's a race condition here, which is if between 1 and 2 the file which exists is deleted. The retry should succeed, but currently a 404 from the head is escalated to a failure proposed changes # if HEAD is 404, leave etag == null and continue # special handling of 412 also to handle 409","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18657::class","input":"Based on experience trying to debug this happening # add debug statements when create() fails # generated exception text to reference string shared with tests, path and error code # generated exception to include inner exception for full stack trace Currently the retry logic is # create(overwrite=false) # if HTTP_CONFLICT/409 raised; call HEAD # use etag in create(path, overwrite=true, etag) # special handling of error HTTP_PRECON_FAILED = 412 There's a race condition here, which is if between 1","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18657::qna","question":"Tune ABFS create() retry logic","answer":"steveloughran opened a new pull request, #5462: URL: https://github.com/apache/hadoop/pull/5462 ### Description of PR Tunes how abfs handles a failure during create which may be due to concurrency *or* load-related retries happening in the store. * better logging * happy with the confict being resolved by the file being deleted * more diagnostics in failure raised ### How was this patch tested? lease test run already; doing full hadoop-azure test run ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18450::summ","input":"JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail\nAttempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: {noformat} renameOrFail(Path src, Path dest) throws IOException {noformat} ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: {noformat} $ hadoop key create sample java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691'{noformat} Update the implementation to check for the file, throwing a FileNotFoundException.","metadata":{"project":"HADOOP","priority":"Critical","status":"Open"}}
{"task":"classification","id":"HADOOP-18450::class","input":"Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: {noformat} renameOrFail(Path src, Path dest) throws IOException {noformat} ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: {noformat} $ hadoop key create sample java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_N","label":{"priority":"Critical","status":"Open"}}
{"task":"qna","id":"HADOOP-18450::qna","question":"JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail","answer":"snmvaughan opened a new pull request, #5452: URL: https://github.com/apache/hadoop/pull/5452 … in renameOrFail ### Description of PR Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store. The calls to: ``` renameOrFail(Path src, Path dest) throws IOException ``` ... fails with an IOException when it attempts to rename a file. The calling code catches FileNotFoundException since the src file may not exist. Example: ``` $ hadoop key create sample java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691' ``` Update the implementation to check for the file, throwing a FileNotFoundException. ### How was this patch tested? Running in an Hadoop development environment docker image. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-17725::summ","input":"Improve error message for token providers in ABFS\nIt would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider. For example, when running the following code: {code:java} import org.apache.hadoop.conf._ import org.apache.hadoop.fs._ val conf = new Configuration() conf.set(\"fs.azure.account.auth.type\", \"OAuth\") conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") conf.set(\"fs.azure.account.oauth2.client.id\", \"my-client-id\") // conf.set(\"fs.azure.account.oauth2.client.secret.my-account.dfs.core.windows.net\", \"my-secret\") conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"my-endpoint\") val path = new Path(\"abfss://container@my-account.dfs.core.windows.net/\") val fs = path.getFileSystem(conf) fs.getFileStatus(path){code} The following exception is thrown: {code:java} TokenAccessProviderException: Unable to load OAuth token provider class. ... Caused by: UncheckedExecutionException: java.lang.NullPointerException: clientSecret ... Caused by: NullPointerException: clientSecret {code} which does not tell what configuration key was not loaded. IMHO, it would be good if the exception was something like this: {code:java} TokenAccessProviderException: Unable to load OAuth token provider class. ... Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found. {code}","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-17725::class","input":"It would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider. For example, when running the following code: {code:java} import org.apache.hadoop.conf._ import org.apache.hadoop.fs._ val conf = new Configuration() conf.set(\"fs.azure.account.auth.type\", \"OA","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-17725::qna","question":"Improve error message for token providers in ABFS","answer":"[~stevel@apache.org] Would you like to take a look at https://github.com/apache/hadoop/pull/3041? Thanks"}
{"task":"summarization","id":"HADOOP-18033::summ","input":"Upgrade fasterxml Jackson to 2.13.0\nSpark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper. h2. this has been reverted. we had to revert this as it broke tez.","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-18033::class","input":"Spark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper. h2. this has been reverted. we had to revert this as it broke tez.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-18033::qna","question":"Upgrade fasterxml Jackson to 2.13.0","answer":"Let's upgrade to 2.13.0? It already seems to have good number of usages [here|https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core/2.13.0/usages]."}
{"task":"summarization","id":"HADOOP-18142::summ","input":"Increase precommit job timeout from 24 hr to 30 hr\nAs per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.","metadata":{"project":"HADOOP","priority":"Minor","status":"Patch Available"}}
{"task":"classification","id":"HADOOP-18142::class","input":"As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.","label":{"priority":"Minor","status":"Patch Available"}}
{"task":"qna","id":"HADOOP-18142::qna","question":"Increase precommit job timeout from 24 hr to 30 hr","answer":"[~vjasani] I've noticed this ticket by chance and would like to mention a few things I've done in Hive to avoid kinda like the same issues: * [use rateLimit|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L21] to avoid building the same PR multiple time a day ; this naturally adds a 6 hour wait before the next would start * use a global lock to [limit the number|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L150] of concurrently running * [disable concurrent builds|https://github.com/apache/hive/blob/af013246100be85675d18e6dcfcea7f202bc8d2c/Jenkinsfile#L23] as there is no point running the tests for someone who pushed new changes while it was still running => the contributor most likely will push more commits anyway which could launch even more builds...not starting a new build means it could pick up multiple trigger events while the one executing is still running * auto-kill the build in case the PR was updated while it waiting/running ; by calling [this method|https://github.com/apache/hive/blob/master/Jenkinsfile#L30-L45] at a few key points in the build"}
{"task":"summarization","id":"HADOOP-19654::summ","input":"Upgrade AWS SDK to 2.35.4\nUpgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.","metadata":{"project":"HADOOP","priority":"Major","status":"In Progress"}}
{"task":"classification","id":"HADOOP-19654::class","input":"Upgrade to a recent version of 2.33.x or later while off the critical path of things. HADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.","label":{"priority":"Major","status":"In Progress"}}
{"task":"qna","id":"HADOOP-19654::qna","question":"Upgrade AWS SDK to 2.35.4","answer":"steveloughran opened a new pull request, #7882: URL: https://github.com/apache/hadoop/pull/7882 ### How was this patch tested? Testing in progress; still trying to get the ITests working. JUnit5 update complicates things here, as it highlights that minicluster tests aren't working. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18640::summ","input":"ABFS: Enabling Client-side Backoff only for new requests\nEnabling backoff only for new requests that happen, and disabling for retried requests.","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-18640::class","input":"Enabling backoff only for new requests that happen, and disabling for retried requests.","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-18640::qna","question":"ABFS: Enabling Client-side Backoff only for new requests","answer":"sreeb-msft opened a new pull request, #5446: URL: https://github.com/apache/hadoop/pull/5446 This PR introduces two changes that allows client-side throttling and backoff only for new requests, and increases the level of control through new configs in AbfsClientThrottlingAnalyzer. For the first change, it checks for whether the current rest operation corresponds to a new or retried request. In case of a new request, it calls the necessary methods to apply throttling and backoff at the client side if necessary. In case of a retried request, these methods are skipped and no backoff is applied, or even checked if necessary. This, however, does not affect any other flow such as updating metrics, which happens for each request, irrespective of whether retried or new. In code, the check for whether it is a retried request or not, and the subsequent call for CST is moved to a separate new method, which directly takes input of the current retry count, and makes the decision based on that. #### Tests Added: Two separate tests have been added as part of this change, one for the read and the other for write/append requests - which are the only cases where client-side throttling is applied. Each test does the following - 1. Validates for a new request: - The method for CST (sendingRequest) is called. - The counters for throttling (read/append) are incremented by 1. 2. Validates for a retried request: - The CST method (sendingRequest) call is skipped. - The counters for throttling (read/append) are not incremented and are the same before and after the apply throttling backoff call happens. For the second change, new configs are introduced for the following - 1. `fs.azure.min.acceptable.error.percentage` 2. `fs.azure.max.equilibrium.error.percentage` 3. `fs.azure.rapid.sleep.decrease.factor` 4. `fs.azure.rapid.sleep.decrease.transition.ms` 5. `fs.azure.sleep.decrease.factor` 6. `fs.azure.sleep.increase.factor` All of these are meant to replace the static values that were in place, by allowing the user more control to configure these directly. The static values for these variables, earlier present in code itself, now would be used as default values for the configs."}
{"task":"summarization","id":"HADOOP-18639::summ","input":"DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed.\nYARN NodeManager's deletion service has two types of deletion tasks: the FileDeletionTask for deleting log, usercache, appcache files and the DockerContainerDeletionTask for deleting Docker containers. The FileDeletionTask is removed from the statestore when the task is completed, but the DockerContainerDeletionTask is not. Therefore, the DockerContainerDeletionTask accumulates continuously in the statestore. This causes the NodeManager's deletion service to run the accumulated DockerContainerDeletionTask in the statestore when the NodeManager restarts. As a result, the FileDeletionTask and DockerContainerDeletionTask are delayed unnecessarily while processing accumulated tasks, which can cause disk full issues in environments where a large number of containers are allocated and released. I will attach a patch soon","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18639::class","input":"YARN NodeManager's deletion service has two types of deletion tasks: the FileDeletionTask for deleting log, usercache, appcache files and the DockerContainerDeletionTask for deleting Docker containers. The FileDeletionTask is removed from the statestore when the task is completed, but the DockerContainerDeletionTask is not. Therefore, the DockerContainerDeletionTask accumulates continuously in the statestore. This causes the NodeManager's deletion service to run the accumulated DockerContainerDe","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18639::qna","question":"DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed.","answer":"Sejin-Hwang opened a new pull request, #5425: URL: https://github.com/apache/hadoop/pull/5425 ### Description of PR When the DockerContainerDeletionTask is completed, it should be deleted from the NodeManager's statestore, just like the FileDeletionTask. JIRA : https://issues.apache.org/jira/browse/HADOOP-18639 ### How was this patch tested? Unit test added"}
{"task":"summarization","id":"HADOOP-19666::summ","input":"Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension\nThis PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-19666::class","input":"This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-19666::qna","question":"Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension","answer":"leiwen2025 opened a new pull request, #7912: URL: https://github.com/apache/hadoop/pull/7912 This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc instruction sets, with full functional verification and performance testing completed. The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction. Key Features: 1. Runtime Hardware Detection The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime. 2. Performance Improvement Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation. ### Description of PR ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-19599::summ","input":"Fix file permission errors as per the platform\nThe file permission denial error message in Linux systems end with \"(Permission denied)\" particularly. However, an error message in the same scenario on Windows ends with \"(Access is denied)\" error. This results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*. Thus, we need to make the appropriate check in accordance with the platform.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-19599::class","input":"The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly. However, an error message in the same scenario on Windows ends with \"(Access is denied)\" error. This results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*. Thus, we need to make the appropriate check in accordance with the platform.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-19599::qna","question":"Fix file permission errors as per the platform","answer":"slfan1989 commented on PR #7767: URL: https://github.com/apache/hadoop/pull/7767#issuecomment-3081984805 @GauthamBanasandra Thank you for your contribution — I believe this PR is ready to be merged."}
{"task":"summarization","id":"HADOOP-18635::summ","input":"Expose distcp counters to user via config parameter and distcp contants\nCurrently users or application such as Hive cannot access directly the distcp counters such as total number of bytes copied by distcp operation. This Jira is to enable this functionality in distcp tool.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18635::class","input":"Currently users or application such as Hive cannot access directly the distcp counters such as total number of bytes copied by distcp operation. This Jira is to enable this functionality in distcp tool.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18635::qna","question":"Expose distcp counters to user via config parameter and distcp contants","answer":"hadoop-yetus commented on PR #5402: URL: https://github.com/apache/hadoop/pull/5402#issuecomment-1431585282 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 58s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 43m 8s | | trunk passed | | +1 :green_heart: | compile | 0m 34s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 31s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | checkstyle | 0m 33s | | trunk passed | | +1 :green_heart: | mvnsite | 0m 36s | | trunk passed | | +1 :green_heart: | javadoc | 0m 36s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 29s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 58s | | trunk passed | | +1 :green_heart: | shadedclient | 23m 50s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 31s | | the patch passed | | +1 :green_heart: | compile | 0m 25s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 25s | | the patch passed | | +1 :green_heart: | compile | 0m 23s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 23s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | -0 :warning: | checkstyle | 0m 17s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) | hadoop-tools/hadoop-distcp: The patch generated 3 new + 18 unchanged - 0 fixed = 21 total (was 18) | | +1 :green_heart: | mvnsite | 0m 25s | | the patch passed | | +1 :green_heart: | javadoc | 0m 21s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | spotbugs | 0m 50s | | the patch passed | | +1 :green_heart: | shadedclient | 23m 28s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | -1 :x: | unit | 15m 58s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) | hadoop-distcp in the patch passed. | | +1 :green_heart: | asflicense | 0m 36s | | The patch does not generate ASF License warnings. | | | | 117m 35s | | | | Reason | Tests | |-------:|:------| | Failed junit tests | hadoop.tools.TestExternalCall | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5402 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets | | uname | Linux 1c80ac3f44b1 4.15.0-200-generic #211-Ubuntu SMP Thu Nov 24 18:16:04 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / a3dcbd611b0d06f06431e4128c96bcfacf9c3866 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/testReport/ | | Max. process+thread count | 704 (vs. ulimit of 5500) | | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5402/1/console | | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."}
{"task":"summarization","id":"HADOOP-18632::summ","input":"ABFS: Customize and optimize timeouts made based on each separate request\nIn present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18632::class","input":"In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent ","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18632::qna","question":"ABFS: Customize and optimize timeouts made based on each separate request","answer":"sreeb-msft opened a new pull request, #5399: URL: https://github.com/apache/hadoop/pull/5399 ### Description of PR In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker. For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18629::summ","input":"Hadoop DistCp supports specifying favoredNodes for data copying\nWhen importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp. Could I submit a pull request to optimize it?","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18629::class","input":"When importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp. Could I submit a pull request to optimize it?","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18629::qna","question":"Hadoop DistCp supports specifying favoredNodes for data copying","answer":"zhuyaogai opened a new pull request, #5391: URL: https://github.com/apache/hadoop/pull/5391 ### Description of PR Hadoop DistCp supports specifying favoredNodes for data copying. ### How was this patch tested? Add new UT. ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18624::summ","input":"Leaked calls may cause ObserverNameNode OOM.\nLeaked calls may cause ObserverNameNode OOM. During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM.","metadata":{"project":"HADOOP","priority":"Major","status":"Reopened"}}
{"task":"classification","id":"HADOOP-18624::class","input":"Leaked calls may cause ObserverNameNode OOM. During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM.","label":{"priority":"Major","status":"Reopened"}}
{"task":"qna","id":"HADOOP-18624::qna","question":"Leaked calls may cause ObserverNameNode OOM.","answer":"ZanderXu opened a new pull request, #5367: URL: https://github.com/apache/hadoop/pull/5367 ### Description of PR Jira: [HADOOP-18624](https://issues.apache.org/jira/browse/HADOOP-18624) Leaked calls may cause ObserverNameNode OOM. During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM."}
{"task":"summarization","id":"HADOOP-19730::summ","input":"upgrade bouncycastle to 1.82 due to CVE-2025-8916\nhttps://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-19730::class","input":"https://github.com/advisories/GHSA-4cx2-fc23-5wg6 Thought it was tidier to upgrade to latest version even if the fix was a while ago.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-19730::qna","question":"upgrade bouncycastle to 1.82 due to CVE-2025-8916","answer":"pjfanning opened a new pull request, #8039: URL: https://github.com/apache/hadoop/pull/8039 ### Description of PR HADOOP-19730 ### How was this patch tested? ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-19621::summ","input":"hadoop-client-api exclude webapps/static front-end resources\n","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-19621::class","input":"","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-19621::qna","question":"hadoop-client-api exclude webapps/static front-end resources","answer":"cxzl25 opened a new pull request, #7804: URL: https://github.com/apache/hadoop/pull/7804 ### Description of PR `hadoop-client-api` contains some front-end resources in the webapps/static path. ### How was this patch tested? ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18623::summ","input":"S3A delegation token implementations to be able to update tokens from the user credentials\nSpark never renews tokens, instead it can create new ones and attach them to the current users credentials. This means long-running S3A instances which can pick up new tokens/credentials need a way to look for new tokens in the credential chain. Proposed * class AbstractDelegationTokenBinding adds a CallableRaisingIOE field which can be updated with a callback * S3ADelegationTokens to add method boolean maybeUpdateTokenFromOwner() to look for any new token and switch to it if new * S3ADelegationTokens serviceInit() to pass the method down to the instantiated DT binding as the callback It is up to the token binding implementation to decide what to do about it; the standard implementations will do: nothing.","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-18623::class","input":"Spark never renews tokens, instead it can create new ones and attach them to the current users credentials. This means long-running S3A instances which can pick up new tokens/credentials need a way to look for new tokens in the credential chain. Proposed * class AbstractDelegationTokenBinding adds a CallableRaisingIOE field which can be updated with a callback * S3ADelegationTokens to add method boolean maybeUpdateTokenFromOwner() to look for any new token and switch to it if new * S3ADelegation","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-18623::qna","question":"S3A delegation token implementations to be able to update tokens from the user credentials","answer":"steveloughran opened a new pull request, #5365: URL: https://github.com/apache/hadoop/pull/5365 ### Description of PR Adds the production side changes; no tests ### How was this patch tested? will let yetus do that ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18544::summ","input":"S3A: add option to disable probe for dir marker recreation on delete/rename.\nIn applications which do many single-file deletions on the same dir, a lot of time is wasted in {{maybeCreateFakeParentDirectory()}}. Proposed: add an option to disable the probe, for use by applications which are happy for parent dirs to sometimes disappear after a cleanup. file by file delete is still woefully inefficient because of the HEAD request on every file, but there's no need to amplify the damage.","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-18544::class","input":"In applications which do many single-file deletions on the same dir, a lot of time is wasted in {{maybeCreateFakeParentDirectory()}}. Proposed: add an option to disable the probe, for use by applications which are happy for parent dirs to sometimes disappear after a cleanup. file by file delete is still woefully inefficient because of the HEAD request on every file, but there's no need to amplify the damage.","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-18544::qna","question":"S3A: add option to disable probe for dir marker recreation on delete/rename.","answer":"[~harshit.gupta] assigning to you this'll need a new s3a option (Constants.java) read to a field in s3afs. initialize(), then checked in {{maybeCreateFakeParentDirectory()}} to skip the delete. then need a test (similar to ITestS3ARenameCost/ITestS3ADeleteCost) which asserts that no HEAD request is made on rename and delete. those existing suites will need to set the new option to false to stop all their existing tests failing; see their parent class's createConfiguration() to see what to do there."}
{"task":"summarization","id":"HADOOP-18613::summ","input":"Upgrade ZooKeeper to version 3.8.3\n","metadata":{"project":"HADOOP","priority":"Major","status":"Resolved"}}
{"task":"classification","id":"HADOOP-18613::class","input":"","label":{"priority":"Major","status":"Resolved"}}
{"task":"qna","id":"HADOOP-18613::qna","question":"Upgrade ZooKeeper to version 3.8.3","answer":"hadoop-yetus commented on PR #5345: URL: https://github.com/apache/hadoop/pull/5345#issuecomment-1415919530 :broken_heart: **-1 overall** | Vote | Subsystem | Runtime | Logfile | Comment | |:----:|----------:|--------:|:--------:|:-------:| | +0 :ok: | reexec | 0m 49s | | Docker mode activated. | |||| _ Prechecks _ | | +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. | | +0 :ok: | codespell | 0m 1s | | codespell was not available. | | +0 :ok: | detsecrets | 0m 1s | | detect-secrets was not available. | | +0 :ok: | xmllint | 0m 1s | | xmllint was not available. | | +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. | | -1 :x: | test4tests | 0m 0s | | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | |||| _ trunk Compile Tests _ | | +1 :green_heart: | mvninstall | 45m 44s | | trunk passed | | +1 :green_heart: | compile | 0m 16s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | compile | 0m 18s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | mvnsite | 0m 22s | | trunk passed | | +1 :green_heart: | javadoc | 0m 25s | | trunk passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 19s | | trunk passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 72m 52s | | branch has no errors when building and testing our client artifacts. | |||| _ Patch Compile Tests _ | | +1 :green_heart: | mvninstall | 0m 14s | | the patch passed | | +1 :green_heart: | compile | 0m 12s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javac | 0m 12s | | the patch passed | | +1 :green_heart: | compile | 0m 11s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | javac | 0m 11s | | the patch passed | | +1 :green_heart: | blanks | 0m 0s | | The patch has no blanks issues. | | +1 :green_heart: | mvnsite | 0m 13s | | the patch passed | | +1 :green_heart: | javadoc | 0m 12s | | the patch passed with JDK Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 | | +1 :green_heart: | javadoc | 0m 11s | | the patch passed with JDK Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | +1 :green_heart: | shadedclient | 27m 14s | | patch has no errors when building and testing our client artifacts. | |||| _ Other Tests _ | | +1 :green_heart: | unit | 0m 14s | | hadoop-project in the patch passed. | | +1 :green_heart: | asflicense | 0m 33s | | The patch does not generate ASF License warnings. | | | | 104m 4s | | | | Subsystem | Report/Notes | |----------:|:-------------| | Docker | ClientAPI=1.42 ServerAPI=1.42 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/artifact/out/Dockerfile | | GITHUB PR | https://github.com/apache/hadoop/pull/5345 | | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint | | uname | Linux 97bfd0150be1 4.15.0-197-generic #208-Ubuntu SMP Tue Nov 1 17:23:37 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | dev-support/bin/hadoop.sh | | git revision | trunk / 7ed7f25f252f90c2e2580e849e44a6e70fa3e242 | | Default Java | Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.17+8-post-Ubuntu-1ubuntu220.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_352-8u352-ga-1~20.04-b08 | | Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/testReport/ | | Max. process+thread count | 540 (vs. ulimit of 5500) | | modules | C: hadoop-project U: hadoop-project | | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5345/1/console | | versions | git=2.25.1 maven=3.6.3 | | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org | This message was automatically generated."}
{"task":"summarization","id":"HADOOP-18614::summ","input":"Ensure that the config writers are closed\nUse AutoCloseable to ensure that the config writers are closed between tests.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18614::class","input":"Use AutoCloseable to ensure that the config writers are closed between tests.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18614::qna","question":"Ensure that the config writers are closed","answer":"snmvaughan opened a new pull request, #5341: URL: https://github.com/apache/hadoop/pull/5341 ### Description of PR Use AutoCloseable to ensure that the config writers are closed between tests. ### How was this patch tested? Tested using the Hadoop development environment docker image. ### For code changes: - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-18618::summ","input":"Support custom property for credential provider path\nHadoop allows the configuration of a credential provider path through the property \"{*}hadoop.security.credential.provider.path{*}\", and the {{Configuration#getPassword()}} method retrieves the credentials from this provider. However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site.xml property file. To resolve this, all components should share a common credential provider path and add all their credentials. Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property \"{*}fs.azure.account.credential.provider.path..blob.core.windows.net{*}\". To accommodate this, the {{Configuration#getPassword()}} method should accept a custom property for the credential provider path and retrieve its value. The current default property can be overridden to achieve this. {code:java} public char[] getPassword(String name) throws IOException { ...... ...... } public char[] getPassword(String name, String providerKey) throws IOException { ...... ...... }{code} One Example is, Ambari [CustomServiceOrchestrator|https://github.com/apache/ambari/blob/trunk/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py#L312] service override the core-site.xml value for other component. This fix is very much needed for Ambari.","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-18618::class","input":"Hadoop allows the configuration of a credential provider path through the property \"{*}hadoop.security.credential.provider.path{*}\", and the {{Configuration#getPassword()}} method retrieves the credentials from this provider. However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-18618::qna","question":"Support custom property for credential provider path","answer":"surendralilhore opened a new pull request, #5352: URL: https://github.com/apache/hadoop/pull/5352 Hadoop allows the configuration of a credential provider path through the property \"hadoop.security.credential.provider.path\", and the Configuration#getPassword() method retrieves the credentials from this provider. However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site.xml property file. To resolve this, all components should share a common credential provider path and add all their credentials. Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property \"fs.azure.account.credential.provider.path..blob.core.windows.net\". To accommodate this, the Configuration#getPassword() method should accept a custom property for the credential provider path and retrieve its value. The current default property can be overridden to achieve this. public char[] getPassword(String name) throws IOException { ...... ...... } public char[] getPassword(String name, String providerKey) throws IOException { ...... ...... }"}
{"task":"summarization","id":"HADOOP-17377::summ","input":"ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service\n*Summary* The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling] In particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability when running multiple processes on the same host. *Environment* * Spark talking to an ABFS store * Hadoop 3.2.1 * Running on an Azure VM with user-assigned identity, ABFS configured to use MsiTokenProvider * 6 executor processes on each VM *Example* Here's an example error message and stack trace. It's always the same stack trace. This appears in logs a few hundred to low thousands of times a day. It's luckily skating by since the download operation is wrapped in 3 retries. {noformat} AADToken: HTTP connection failed for getting token from AzureAD. Http response: 429 null Content-Type: application/json; charset=utf-8 Content-Length: 90 Request ID: Proxies: none First 1K of Body: {\"error\":\"invalid_request\",\"error_description\":\"Temporarily throttled, too many requests\"} at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437) at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717) at org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:747) at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:724) at org.apache.spark.util.Utils$.fetchFile(Utils.scala:496) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7(Executor.scala:812) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7$adapted(Executor.scala:803) at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792) at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149) at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237) at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44) at scala.collection.mutable.HashMap.foreach(HashMap.scala:149) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791) at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748){noformat} CC [~mackrorysd], [~stevel@apache.org]","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-17377::class","input":"*Summary* The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling] In particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability whe","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-17377::qna","question":"ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service","answer":"[~snvijaya]"}
{"task":"summarization","id":"HADOOP-18594::summ","input":"ProxyUserAuthenticationFilter add properties 'hadoop.security.impersonation.provider.class'  to enable  load custom ImpersonationProvider class when start namenode\nh3. h3. the phenomenon I made a custom ImpersonationProvider class and configured in core-site.xml {code:none} hadoop.security.impersonation.provider.class org.apache.hadoop.security.authorize.MyImpersonationProvider {code} {color:#ff0000}However, when start namenode, MyImpersonationProvider could't be load automatically, but DefaultImpersonationProvider is loaded.{color} When execute the following command, custom ImpersonationProvider could be load. {code:java} bin/hdfs dfsadmin -refreshSuperUserGroupsConfiguration{code} h3. h3. what I see else custom ImpersonationProvider was load in org.apache.hadoop.security.authorize.ProxyUsers#refreshSuperUserGroupsConfiguration through the property \"hadoop.security.impersonation.provider.class\" [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/ProxyUsers.java#L70] {code:java} public static void refreshSuperUserGroupsConfiguration(Configuration conf, String proxyUserPrefix) { Preconditions.checkArgument(proxyUserPrefix != null && !proxyUserPrefix.isEmpty(), \"prefix cannot be NULL or empty\"); // sip is volatile. Any assignment to it as well as the object's state // will be visible to all the other threads. ImpersonationProvider ip = getInstance(conf); ip.init(proxyUserPrefix); sip = ip; ProxyServers.refresh(conf); } private static ImpersonationProvider getInstance(Configuration conf) { Class clazz = conf.getClass( CommonConfigurationKeysPublic.HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS, DefaultImpersonationProvider.class, ImpersonationProvider.class); return ReflectionUtils.newInstance(clazz, conf); }{code} when namenode start, refreshSuperUserGroupsConfiguration was called in ProxyUserAuthenticationFilter, [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java#L56] {code:java} public void init(FilterConfig filterConfig) throws ServletException { Configuration conf = getProxyuserConfiguration(filterConfig); ProxyUsers.refreshSuperUserGroupsConfiguration(conf, PROXYUSER_PREFIX); super.init(filterConfig); } {code} here is the stack trace {code:none} init:70, DefaultImpersonationProvider (org.apache.hadoop.security.authorize) refreshSuperUserGroupsConfiguration:77, ProxyUsers (org.apache.hadoop.security.authorize) init:56, ProxyUserAuthenticationFilter (org.apache.hadoop.security.authentication.server) initialize:140, FilterHolder (org.eclipse.jetty.servlet) lambda$initialize$0:731, ServletHandler (org.eclipse.jetty.servlet) accept:-1, 1541075662 (org.eclipse.jetty.servlet.ServletHandler$$Lambda$36) forEachRemaining:948, Spliterators$ArraySpliterator (java.util) forEachRemaining:742, Streams$ConcatSpliterator (java.util.stream) forEach:580, ReferencePipeline$Head (java.util.stream) initialize:755, ServletHandler (org.eclipse.jetty.servlet) startContext:379, ServletContextHandler (org.eclipse.jetty.servlet) doStart:910, ContextHandler (org.eclipse.jetty.server.handler) doStart:288, ServletContextHandler (org.eclipse.jetty.servlet) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:169, ContainerLifeCycle (org.eclipse.jetty.util.component) start:423, Server (org.eclipse.jetty.server) doStart:110, ContainerLifeCycle (org.eclipse.jetty.util.component) doStart:97, AbstractHandler (org.eclipse.jetty.server.handler) doStart:387, Server (org.eclipse.jetty.server) start:73, AbstractLifeCycle (org.eclipse.jetty.util.component) start:1276, HttpServer2 (org.apache.hadoop.http) start:170, NameNodeHttpServer (org.apache.hadoop.hdfs.server.namenode) startHttpServer:954, NameNode (org.apache.hadoop.hdfs.server.namenode) initialize:765, NameNode (org.apache.hadoop.hdfs.server.namenode) :1020, NameNode (org.apache.hadoop.hdfs.server.namenode) :995, NameNode (org.apache.hadoop.hdfs.server.namenode) createNameNode:1769, NameNode (org.apache.hadoop.hdfs.server.namenode) main:1834, NameNode (org.apache.hadoop.hdfs.server.namenode) {code} {color:#ff0000}but the filterConfig in ProxyUserAuthenticationFilter did't contains properties ''hadoop.security.impersonation.provider.class''{color} filterConfig in ProxyUserAuthenticationFilter is controled by ProxyUserAuthenticationFilterInitializer or AuthFilterInitializer filterConfig only put property which start with \"hadoop.proxyuser\", but not put \"hadoop.security.impersonation.provider.class\" {code:java} protected Map createFilterConfig(Configuration conf) { Map filterConfig = AuthenticationFilterInitializer .getFilterConfigMap(conf, configPrefix); //Add proxy user configs for (Map.Entry entry : conf.getPropsWithPrefix( ProxyUsers.CONF_HADOOP_PROXYUSER).entrySet()) { filterConfig.put(\"proxyuser\" + entry.getKey(), entry.getValue()); } return filterConfig; } {code} [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java#L46] [https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/AuthFilterInitializer.java#L46] it leads to custome ImpersonationProvider can't be load during namenode start.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18594::class","input":"h3. h3. the phenomenon I made a custom ImpersonationProvider class and configured in core-site.xml {code:none} hadoop.security.impersonation.provider.class org.apache.hadoop.security.authorize.MyImpersonationProvider {code} {color:#ff0000}However, when start namenode, MyImpersonationProvider could't be load automatically, but DefaultImpersonationProvider is loaded.{color} When execute the following command, custom ImpersonationProvider could be load. {code:java} bin/hdfs dfsadmin -refreshSuperUs","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18594::qna","question":"ProxyUserAuthenticationFilter add properties 'hadoop.security.impersonation.provider.class'  to enable  load custom ImpersonationProvider class when start namenode","answer":"Could we add the property \"hadoop.security.impersonation.provider.class\" in ProxyUserAuthenticationFilterInitializer#createFilterConfig and AuthFilterInitializer#createFilterConfig"}
{"task":"summarization","id":"HADOOP-18585::summ","input":"DataNode's internal infoserver redirects with http scheme, not https when https enabled.\nAfter HADOOP-16314, WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme. (Hostname is modified.) {code:java} $ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location < Location: http://dn1.example.com:50475/index.html {code} I can't ensure that which solution is the best among: - Use DefaultServlet instead of WebServlet. DataNode can answer with index.html when accessed in '/'. - According to \"dfs.http.policy\" in hdfs-site.xml, run internal infoserver as https or http server each. - Make redirection on URLDispatcher.java","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-18585::class","input":"After HADOOP-16314, WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme. (Hostname is modified.) {code:java} $ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location < Location: http://dn1.example.com:50475/index.html {code} I can't ensure that which solution is the best among: - Use DefaultServlet instead of WebSe","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-18585::qna","question":"DataNode's internal infoserver redirects with http scheme, not https when https enabled.","answer":"eubnara opened a new pull request, #5259: URL: https://github.com/apache/hadoop/pull/5259 ### Description of PR After [HADOOP-16314](https://issues.apache.org/jira/browse/HADOOP-16314), WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme. (Hostname is modified.) ``` $ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location < Location: http://dn1.example.com:50475/index.html ``` I can't ensure that which solution is the best between: - Use DefaultServlet instead of WebServlet. DataNode can answer with index.html when accessed in '/'. - According to \"dfs.http.policy\" in hdfs-site.xml, run internal infoserver as https or http server each. ### How was this patch tested? Manually tested with internal cluster. ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?"}
{"task":"summarization","id":"HADOOP-19472::summ","input":"ABFS: Enhance performance of ABFS driver for write-heavy workloads\nThe goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-19472::class","input":"The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-19472::qna","question":"ABFS: Enhance performance of ABFS driver for write-heavy workloads","answer":"anmolanmol1234 opened a new pull request, #7669: URL: https://github.com/apache/hadoop/pull/7669 Enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes. ![{05B55BCA-EF1F-496D-B1ED-17DCD394DDA1}](https://github.com/user-attachments/assets/5ebd5ad7-51db-4028-812f-ce9da9266984) The proposed design advocates for a centralized `WriteThreadPoolSizeManager` class to handle the collective thread allocation required for all write operations across the system, replacing the current CachedThreadPool in AzureBlobFileSystemStore. This centralized approach ensures that the initial thread pool size is set at `4 * number of available processors` and dynamically adjusts the pool size based on the system's current CPU utilization. This adaptive scaling and descaling mechanism optimizes resource usage and responsiveness. Moreover, this shared thread pool is accessible and utilized by all output streams, streamlining resource management and promoting efficient concurrency across write operations."}
{"task":"summarization","id":"HADOOP-18547::summ","input":"Check if config value is not empty string in AbfsConfiguration.getMandatoryPasswordString()\nThe method `getMandatoryPasswordString` is called in `AbfsConfiguration.getTokenProvider()' to check if following configs are non-null (diff keys applicable for different implementation of AccessTokenProvider): 1. fs.azure.account.oauth2.client.endpoint: in ClientCredsTokenProvider 2. fs.azure.account.oauth2.client.id: in ClientCredsTokenProvider, MsiTokenProvider, RefreshTokenBasedTokenProvider 3. fs.azure.account.oauth2.client.secret: in ClientCredsTokenProvider 4. fs.azure.account.oauth2.client.endpoint: in UserPasswordTokenProvider 5. fs.azure.account.oauth2.user.name: in UserPasswordTokenProvider 6. fs.azure.account.oauth2.user.password: in UserPasswordTokenProvider 7. fs.azure.account.oauth2.msi.tenant: in MsiTokenProvider 8. fs.azure.account.oauth2.refresh.token: in RefreshTokenBasedTokenProvider Right now, this method checks if its non-null and not non-empty. This task needs to add check on non-empty config values.","metadata":{"project":"HADOOP","priority":"Minor","status":"Open"}}
{"task":"classification","id":"HADOOP-18547::class","input":"The method `getMandatoryPasswordString` is called in `AbfsConfiguration.getTokenProvider()' to check if following configs are non-null (diff keys applicable for different implementation of AccessTokenProvider): 1. fs.azure.account.oauth2.client.endpoint: in ClientCredsTokenProvider 2. fs.azure.account.oauth2.client.id: in ClientCredsTokenProvider, MsiTokenProvider, RefreshTokenBasedTokenProvider 3. fs.azure.account.oauth2.client.secret: in ClientCredsTokenProvider 4. fs.azure.account.oauth2.clie","label":{"priority":"Minor","status":"Open"}}
{"task":"qna","id":"HADOOP-18547::qna","question":"Check if config value is not empty string in AbfsConfiguration.getMandatoryPasswordString()","answer":"pranavsaxena-microsoft opened a new pull request, #5177: URL: https://github.com/apache/hadoop/pull/5177 JIRA: https://issues.apache.org/jira/browse/HADOOP-18547 **Testing**"}
{"task":"summarization","id":"HADOOP-18543::summ","input":"AliyunOSS: AliyunOSSFileSystem#open(Path path, int bufferSize) should use buffer size as its downloadPartSize\nIn our application, different components have their own suitable buffer size to download. But currently, AliyunOSSFileSystem#open(Path path, int bufferSize) just get downloadPartSize from configuration. We cannnot use different value for different components in our programs. I think we should the method should use the buffer size from the paramater. AliyunOSSFileSystem#open(Path path) could have default value as current default downloadPartSize.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18543::class","input":"In our application, different components have their own suitable buffer size to download. But currently, AliyunOSSFileSystem#open(Path path, int bufferSize) just get downloadPartSize from configuration. We cannnot use different value for different components in our programs. I think we should the method should use the buffer size from the paramater. AliyunOSSFileSystem#open(Path path) could have default value as current default downloadPartSize.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18543::qna","question":"AliyunOSS: AliyunOSSFileSystem#open(Path path, int bufferSize) should use buffer size as its downloadPartSize","answer":"[~wujinhu] WDYT? If right, I'd like to create a pr to fix it."}
{"task":"summarization","id":"HADOOP-18541::summ","input":"Upgrade grizzly version to 2.4.4\nUpgrade grizzly version to 2.4.4 to resolve |[[sonatype-2016-0415] CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')|https://ossindex.sonatype.org/vulnerability/sonatype-2016-0415?component-type=maven&component-name=org.glassfish.grizzly/grizzly-http-server]| [CVE-2014-0099|https://nvd.nist.gov/vuln/detail/CVE-2014-0099], [CVE-2014-0075|https://nvd.nist.gov/vuln/detail/CVE-2014-0075], [CVE-2017-1000028|https://nvd.nist.gov/vuln/detail/CVE-2017-1000028]","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-18541::class","input":"Upgrade grizzly version to 2.4.4 to resolve |[[sonatype-2016-0415] CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')|https://ossindex.sonatype.org/vulnerability/sonatype-2016-0415?component-type=maven&component-name=org.glassfish.grizzly/grizzly-http-server]| [CVE-2014-0099|https://nvd.nist.gov/vuln/detail/CVE-2014-0099], [CVE-2014-0075|https://nvd.nist.gov/vuln/detail/CVE-2014-0075], [CVE-2017-1000028|https://nvd.nist.gov/vuln/detail/CVE-2017-1000028]","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-18541::qna","question":"Upgrade grizzly version to 2.4.4","answer":"dmmkr opened a new pull request, #5167: URL: https://github.com/apache/hadoop/pull/5167 ### Description of PR Upgrade grizzly version to 2.4.4 ### How was this patch tested? Local compilation sucessful ### For code changes: - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"task":"summarization","id":"HADOOP-19605::summ","input":"Upgrade Protobuf 3.25.5 for docker images\nHADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.","metadata":{"project":"HADOOP","priority":"Major","status":"Open"}}
{"task":"classification","id":"HADOOP-19605::class","input":"HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.","label":{"priority":"Major","status":"Open"}}
{"task":"qna","id":"HADOOP-19605::qna","question":"Upgrade Protobuf 3.25.5 for docker images","answer":"GauthamBanasandra commented on code in PR #7780: URL: https://github.com/apache/hadoop/pull/7780#discussion_r2196670293 ########## dev-support/docker/vcpkg/vcpkg.json: ########## @@ -10,7 +10,7 @@ \"overrides\": [ { \"name\": \"protobuf\", - \"version\": \"3.21.12\" + \"version\": \"3.25.5\" Review Comment: Thanks for PR @pan3793. Please let me know once you're done with all the changes and I can verify it on Windows."}
